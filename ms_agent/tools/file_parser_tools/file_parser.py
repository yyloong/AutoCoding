import json
import uuid
import os
from docx import Document
import re
import time
import zipfile
import math
from pathlib import Path
from qwen_agent.settings import DEFAULT_WORKSPACE, DEFAULT_MAX_INPUT_TOKENS
from pptx.enum.shapes import MSO_SHAPE_TYPE
from qwen_agent.utils.tokenization_qwen import count_tokens, tokenizer
from typing import Any, Dict, List, Optional, Union
from collections import Counter
import xml.etree.ElementTree as ET
from pandas import Timestamp
from datetime import datetime
from pandas.api.types import is_datetime64_any_dtype
import pandas as pd
from tabulate import tabulate
from ms_agent.utils import get_logger
from .utils import (get_file_type, hash_sha256, is_http_url, get_basename_from_url, 
                                  sanitize_chrome_file_path, save_url_to_local_work_dir,json_loads)

logger = get_logger()

# Configuration constants
PARSER_SUPPORTED_FILE_TYPES = ['pdf', 'docx', 'pptx', 'txt', 'html', 'csv', 'tsv', 'xlsx', 'xls', 'doc', 'zip', '.mp4', '.mov', '.mkv', '.webm', '.mp3', '.wav']
def str_to_bool(value):
    """Convert string to boolean, handling common true/false representations"""
    if isinstance(value, bool):
        return value
    return str(value).lower() in ('true', '1', 'yes', 'on')
USE_IDP = str_to_bool(os.getenv("USE_IDP", "True"))
IDP_TIMEOUT = 150000
ENABLE_CSI = False
PARAGRAPH_SPLIT_SYMBOL = '\n'


class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (datetime, Timestamp)):
            return obj.isoformat()
        return super().default(obj)


class FileParserError(Exception):
    """Custom exception for document parsing errors"""

    def __init__(self, message: str, code: str = '400', exception: Optional[Exception] = None):
        super().__init__(message)
        self.code = code
        self.exception = exception

def clean_text(text: str) -> str:
    cleaners = [
        lambda x: re.sub(r'\n+', '\n', x),  
        lambda x: x.replace("Add to Qwen's Reading List", ''),
        lambda x: re.sub(r'-{6,}', '-----', x),  
        lambda x: x.strip()
    ]
    for cleaner in cleaners:
        text = cleaner(text)
    return text


def get_plain_doc(doc: list):
    paras = []
    for page in doc:
        for para in page['content']:
            for k, v in para.items():
                if k in ['text', 'table', 'image']:
                    paras.append(v)
    return PARAGRAPH_SPLIT_SYMBOL.join(paras)


def df_to_markdown(df: pd.DataFrame) -> str:
    df = df.dropna(how='all').fillna('')
    return tabulate(df, headers='keys', tablefmt='pipe', showindex=False)


def parse_word(docx_path: str, extract_image: bool = True):
    """
    ç»ˆæç‰ˆ Word è§£æï¼š
    1. ä½¿ç”¨ python-docx æå–æ–‡æœ¬å’Œè¡¨æ ¼ã€‚
    2. ä½¿ç”¨ zipfile ç›´æ¥è§£å‹ .docx æ–‡ä»¶æå–æ‰€æœ‰å›¾ç‰‡ï¼ˆ100% æˆåŠŸç‡ï¼‰ã€‚
    3. å°†å›¾ç‰‡åˆ†æç»“æœé™„åŠ åˆ°æ–‡æ¡£æœ«å°¾ã€‚
    """
    content = []
    
    # --- é˜¶æ®µ 1: æå–æ–‡æœ¬å’Œè¡¨æ ¼ (ä½¿ç”¨ python-docx) ---
    try:
        doc = Document(docx_path)
        
        # æå–æ®µè½æ–‡æœ¬
        for para in doc.paragraphs:
            text = para.text.strip()
            if text:
                content.append({'text': text})
        
        # æå–è¡¨æ ¼
        for table in doc.tables:
            tbl_text = []
            for row in table.rows:
                # ç®€å•å¤„ç†ï¼šå°†å•å…ƒæ ¼å†…å®¹ç”¨ | è¿æ¥
                row_cells = [cell.text.replace('\n', ' ').strip() for cell in row.cells]
                tbl_text.append('|' + '|'.join(row_cells) + '|')
            
            tbl_str = '\n'.join(tbl_text)
            if tbl_str.strip():
                content.append({'table': tbl_str})
                
    except Exception as e:
        logger.error(f"Text parsing failed: {e}")
        # å³ä½¿æ–‡æœ¬è§£æå¤±è´¥ï¼Œä¹Ÿå¯ä»¥å°è¯•æå–å›¾ç‰‡

    # --- é˜¶æ®µ 2: æå–æ‰€æœ‰å›¾ç‰‡ (ä½¿ç”¨ zipfile æš´åŠ›æå–) ---
    if extract_image:
        try:
            # å‡†å¤‡å›¾ç‰‡è¾“å‡ºç›®å½•
            img_output_dir = os.path.join(os.path.dirname(docx_path), "extracted_images")
            if not os.path.exists(img_output_dir):
                os.makedirs(img_output_dir, exist_ok=True)

            # æ‰“å¼€ docx ä½œä¸º zip æ–‡ä»¶
            with zipfile.ZipFile(docx_path, 'r') as z:
                # è·å– zip ä¸­æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨
                all_files = z.namelist()
                
                # è¿‡æ»¤å‡ºåª’ä½“æ–‡ä»¶å¤¹ä¸‹çš„å›¾ç‰‡ (word/media/image1.png ...)
                media_files = [f for f in all_files if f.startswith('word/media/') and f != 'word/media/']
                
                logger.info(f"Found {len(media_files)} images in docx.")

                for media_file in media_files:
                    # æ’é™¤éå›¾ç‰‡æ–‡ä»¶ (å¦‚ wmf, emf çŸ¢é‡å›¾ VLM å¾ˆéš¾è¯»ï¼Œbin æ˜¯ OLE å¯¹è±¡)
                    valid_exts = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')
                    if not media_file.lower().endswith(valid_exts):
                        continue

                    # è¯»å–å›¾ç‰‡æ•°æ®
                    img_data = z.read(media_file)
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    base_name = os.path.basename(media_file) # image1.png
                    # åŠ ä¸ª uuid é˜²æ­¢è¦†ç›–
                    save_name = f"docx_{uuid.uuid4().hex[:6]}_{base_name}"
                    save_path = os.path.join(img_output_dir, save_name)

                    with open(save_path, "wb") as f:
                        f.write(img_data)

                    # è°ƒç”¨ VLM åˆ†æ
                    desc = f"[Extracted Image: {base_name}]"
                    if '_analyze_image_with_vlm' in globals():
                        try:
                            # æç¤ºè¯å¢åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                            desc = globals()['_analyze_image_with_vlm'](save_path, context="Word Document Appendix")
                        except Exception as vlm_e:
                            logger.warning(f"VLM analysis failed for {media_file}: {vlm_e}")
                    
                    # å°†å›¾ç‰‡åˆ†æç»“æœåŠ å…¥å†…å®¹åˆ—è¡¨
                    # ç­–ç•¥ï¼šå¯ä»¥æ”¾åœ¨æœ€å‰é¢ï¼Œä¹Ÿå¯ä»¥æ”¾åœ¨æœ€åé¢
                    # è¿™é‡Œé€‰æ‹©è¿½åŠ åˆ° contentï¼Œå¹¶æ ‡è®°æ¥æº
                    content.append({
                        'image_analysis': desc,
                        'image_path': save_path,
                        'type': 'image_extraction_global'
                    })

        except Exception as e:
            logger.error(f"Image extraction via zip failed: {e}")

    # --- é˜¶æ®µ 3: ç»“æœæ•´åˆ ---
    # å¦‚æœå®Œå…¨æ²¡å†…å®¹
    if not content:
        return []

    # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šæ–‡æ¡£å†…å®¹ = [æ–‡æœ¬/è¡¨æ ¼æ®µè½...] + [æ‰€æœ‰å›¾ç‰‡çš„åˆ†æç»“æœ...]
    # è™½ç„¶ä¸¢å¤±äº†â€œå›¾ç‰‡åœ¨ç¬¬å‡ æ®µâ€çš„ä¿¡æ¯ï¼Œä½†ä¿è¯äº†å›¾ç‰‡å†…å®¹ç»å¯¹ä¸ä¼šä¸¢ã€‚
    return [{'page_num': 1, 'content': content}]

try:
    from pdf2image import convert_from_path
except ImportError:
    convert_from_path = None

from openai import OpenAI
import os
import base64

client = OpenAI(
    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key="sk-xxx",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

def _encode_image(image_path):
    """è¾…åŠ©å‡½æ•°ï¼šå°†æœ¬åœ°å›¾ç‰‡è½¬ä¸º Base64 å­—ç¬¦ä¸²"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def _analyze_image_with_vlm(image_path: str, context: str = "") -> str:
    """
    ä½¿ç”¨ OpenAI SDK è°ƒç”¨é˜¿é‡Œäº‘ Qwen-VL-Max æ¨¡å‹ã€‚
    """
    if not os.path.exists(image_path):
        return f"[Error: Image path not found: {image_path}]"

    # 1. åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ŒæŒ‡å‘é˜¿é‡Œäº‘ DashScope å…¼å®¹ç«¯ç‚¹
    # 2. å›¾ç‰‡è½¬ Base64
    try:
        base64_image = _encode_image(image_path)
        # æ ¹æ®æ–‡ä»¶åç¼€åˆ¤æ–­ mime type (ç®€å•å¤„ç†ï¼Œé»˜è®¤ jpeg/png)
        file_ext = os.path.splitext(image_path)[-1].lower().replace('.', '')
        if file_ext == 'jpg': file_ext = 'jpeg'
        mime_type = f"image/{file_ext}"
    except Exception as e:
        logger.error(f"Image encoding failed: {e}")
        return f"[å›¾ç‰‡è¯»å–å¤±è´¥: {str(e)}]"

    # 3. æ„é€  Prompt (é’ˆå¯¹ Coding ä»»åŠ¡ä¼˜åŒ–)
    prompt_text = (
        f"ä½ æ˜¯ä¸€ä¸ªé«˜çº§è½¯ä»¶æ¶æ„å¸ˆã€‚è¯·åˆ†æè¿™å¼ æ¥è‡ª{context}çš„å›¾ç‰‡ã€‚\n"
        "æ ¸å¿ƒä»»åŠ¡ï¼šæå–å›¾ç‰‡ä¸­çš„é€»è¾‘ç»“æ„ï¼Œç”¨äºè¾…åŠ©ä»£ç ç”Ÿæˆã€‚\n"
        "1. **æµç¨‹å›¾**ï¼šè¯·è¾“å‡º Mermaid graph TD ä»£ç ï¼Œå‡†ç¡®æè¿°èŠ‚ç‚¹åˆ¤å®šå’Œè·³è½¬ã€‚\n"
        "2. **æ¶æ„å›¾**ï¼šæè¿°æ¨¡å—åˆ’åˆ†å’Œæ•°æ®æµå‘ã€‚\n"
        "3. **çº¯æ–‡æœ¬/è¡¨æ ¼**ï¼šæå–å…³é”®æ•°æ®ã€‚\n"
        "å¦‚æœä¸åŒ…å«æŠ€æœ¯ä¿¡æ¯ï¼Œè¯·ç®€çŸ­è¯´æ˜ã€‚"
    )

    try:
        # 4. å‘èµ·è¯·æ±‚
        completion = client.chat.completions.create(
            model="qwen-vl-max",  # æŒ‡å®šé˜¿é‡Œè§†è§‰æ¨¡å‹
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text", 
                            "text": prompt_text
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                # å¿…é¡»ä½¿ç”¨ Data URI æ ¼å¼
                                "url": f"data:{mime_type};base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            # max_tokens=2000 # å¯é€‰ï¼šé™åˆ¶è¾“å‡ºé•¿åº¦
        )

        # 5. è·å–ç»“æœ
        content = completion.choices[0].message.content
        if not content:
            return "[VLM åˆ†æç»“æœä¸ºç©º]"
        
        return f"\n[å›¾ç‰‡åˆ†æ ({os.path.basename(image_path)})]:\n{content}\n"

    except Exception as e:
        logger.error(f"OpenAI-compatible API Error: {str(e)}")
        return f"[VLM è°ƒç”¨å¼‚å¸¸: {str(e)}]"

def parse_png(image_path: str, context: str = "Image") -> List[dict]:
    """
    è§£æ PNG/JPG å›¾ç‰‡ï¼Œè°ƒç”¨ VLM è¿›è¡Œåˆ†æã€‚
    """
    # å‡è®¾ _analyze_image_with_vlm å‡½æ•°åœ¨ä¸Šä¸‹æ–‡ä¸­å·²å®šä¹‰
    result = _analyze_image_with_vlm(image_path, context=context)
    
    # ä¿®æ”¹è¯´æ˜ï¼š
    # å°† key ç”± 'image_analysis' æ”¹ä¸º 'text'ï¼Œä»¥ä¾¿ _flatten_result èƒ½æ­£ç¡®æå–å†…å®¹ã€‚
    # å¯ä»¥ä¿ç•™ type å­—æ®µä½œä¸ºå…ƒæ•°æ®ã€‚
    return [{
        'page_num': 1, 
        'content': [{
            'text': result, 
            'type': 'image_analysis'
        }]
    }]

def parse_ppt(path: str, extract_image: bool = True):
    """
    è§£æ PPTï¼Œæ”¯æŒé€’å½’æå–ç»„åˆå›¾å½¢ä¸­çš„æ–‡æœ¬ï¼Œå¹¶æ”¯æŒæå–åµŒå…¥çš„å›¾ç‰‡ï¼ˆæµç¨‹å›¾æˆªå›¾ï¼‰ã€‚
    """
    from pptx import Presentation
    from pptx.exc import PackageNotFoundError
    try:
        ppt = Presentation(path)
    except PackageNotFoundError as ex:
        logger.warning(ex)
        return []

    # é€’å½’å¤„ç† Shape çš„å†…éƒ¨å‡½æ•°ï¼ˆè§£å†³ Group ç»„åˆå›¾è¯»ä¸åˆ°å­—çš„é—®é¢˜ï¼‰
    def process_shape(shape, slide_idx):
        shape_content = []
        
        # 1. æ–‡æœ¬æ¡†å¤„ç†
        if shape.has_text_frame:
            text_parts = []
            for paragraph in shape.text_frame.paragraphs:
                # ç®€å•çš„æ¸…æ´—
                p_text = ''.join(run.text for run in paragraph.runs).strip()
                if p_text:
                    text_parts.append(p_text)
            if text_parts:
                shape_content.append({'text': '\n'.join(text_parts)})

        # 2. è¡¨æ ¼å¤„ç†
        if shape.has_table:
            tbl = []
            for row in shape.table.rows:
                row_text = []
                for cell in row.cells:
                    # å•å…ƒæ ¼å†…å¯èƒ½ä¹Ÿæœ‰å¤æ‚ç»“æ„ï¼Œç®€å•è·å–æ–‡æœ¬
                    cell_txt = cell.text_frame.text if cell.text_frame else ""
                    row_text.append(cell_txt.replace('\n', ' ').strip())
                tbl.append('|' + '|'.join(row_text) + '|')
            if tbl:
                shape_content.append({'table': '\n'.join(tbl)})

        # 3. å›¾ç‰‡æå–å¤„ç† (å…³é”®ï¼šå¤„ç†æˆªå›¾ç±»æµç¨‹å›¾)
        if extract_image and shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
            try:
                # ç”Ÿæˆä¸´æ—¶è·¯å¾„
                image_ext = shape.image.ext
                image_name = f"ppt_slide_{slide_idx}_{uuid.uuid4().hex[:8]}.{image_ext}"
                # å‡è®¾æœ‰ä¸€ä¸ª temp ç›®å½•ï¼Œæˆ–è€…å­˜åˆ° path åŒçº§ç›®å½•
                save_dir = os.path.join(os.path.dirname(path), "extracted_images")
                os.makedirs(save_dir, exist_ok=True)
                image_path = os.path.join(save_dir, image_name)
                
                with open(image_path, 'wb') as f:
                    f.write(shape.image.blob)
                
                # è°ƒç”¨ VLM åˆ†æ
                analysis_text = _analyze_image_with_vlm(image_path, context="PPT Slide")
                shape_content.append({
                    'image_analysis': analysis_text, 
                    'image_path': image_path
                })
            except Exception as e:
                logger.warning(f"Failed to extract PPT image: {e}")

        # 4. é€’å½’å¤„ç†ç»„åˆå›¾å½¢ (å…³é”®ï¼šå¤„ç†åŸç”Ÿç»˜åˆ¶çš„æ¡†å›¾)
        if shape.shape_type == MSO_SHAPE_TYPE.GROUP:
            for sub_shape in shape.shapes:
                shape_content.extend(process_shape(sub_shape, slide_idx))
        
        return shape_content

    doc = []
    for slide_number, slide in enumerate(ppt.slides):
        page = {'page_num': slide_number + 1, 'content': []}
        for shape in slide.shapes:
            page['content'].extend(process_shape(shape, slide_number + 1))
        doc.append(page)
    
    return doc

import pdfplumber

# åªæœ‰åœ¨éœ€è¦å›¾ç‰‡OCR/VLMåˆ†ææ—¶æ‰éœ€è¦è¿™ä¸ªåº“
try:
    from pdf2image import convert_from_path
except ImportError:
    convert_from_path = None

def parse_pdf(pdf_path: str, extract_image: bool = False) -> List[dict]:
    """
    è§£æ PDFï¼šä¼˜å…ˆæå–æ–‡æœ¬/è¡¨æ ¼ï¼Œé’ˆå¯¹æ‰«æä»¶æˆ–å›¾è¡¨é¡µè°ƒç”¨ VLMã€‚
    """
    doc_content = []
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾æå–çš„å›¾ç‰‡
    temp_dir = os.path.join(os.path.dirname(pdf_path), "temp_images")
    if extract_image:
        os.makedirs(temp_dir, exist_ok=True)

    with pdfplumber.open(pdf_path) as pdf:
        total_pages = len(pdf.pages)
        
        for i, page in enumerate(pdf.pages):
            page_num = i + 1
            logger.info(f"Processing Page {page_num}/{total_pages}...")
            
            page_data = {'page_num': page_num, 'content': []}
            
            # --- A. æ–‡æœ¬æå– ---
            raw_text = page.extract_text(x_tolerance=1, y_tolerance=3) or ""
            if raw_text.strip():
                page_data['content'].append({
                    'type': 'text',
                    'text': raw_text,
                    'source': 'pdfplumber_text'
                })

            # --- B. è¡¨æ ¼æå– ---
            tables = page.extract_tables()
            if tables:
                for table in tables:
                    if not table: continue
                    # ç®€æ˜“ List è½¬ Markdown
                    clean_table = [[str(c).replace('\n', ' ') if c else '' for c in row] for row in table]
                    if len(clean_table) > 0:
                        headers = clean_table[0]
                        md_table = "| " + " | ".join(headers) + " |\n"
                        md_table += "| " + " | ".join(["---"] * len(headers)) + " |\n"
                        for row in clean_table[1:]:
                            md_table += "| " + " | ".join(row) + " |\n"
                        
                        page_data['content'].append({
                            'type': 'table',
                            'text': f"\n[Detected Table]\n{md_table}\n",
                            'source': 'pdfplumber_table'
                        })

            # --- C. æ™ºèƒ½ VLM è°ƒç”¨ç­–ç•¥ ---
            # åˆ¤æ–­é€»è¾‘ï¼š
            # 1. æ‰«æä»¶æ£€æµ‹ï¼šæœ¬é¡µæå–åˆ°çš„æ–‡å­—å°‘äº 50 ä¸ªå­—ç¬¦ã€‚
            # 2. æ˜¾å¼å¼€å…³ï¼šextract_image=True
            # 3. ä¾èµ–æ£€æŸ¥ï¼špdf2image å¯ç”¨
            is_scanned_page = len(raw_text) < 50
            
            if extract_image and convert_from_path:
                # åªæœ‰å½“ (æ˜¯æ‰«æä»¶) æˆ–è€… (è™½ç„¶æœ‰æ–‡å­—ä½†å¯èƒ½æœ‰é‡è¦å›¾è¡¨ - è¿™é‡Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´) æ—¶æ‰æ‰§è¡Œ
                # ä¸ºäº†èŠ‚çœ tokenï¼Œè¿™é‡Œç­–ç•¥è®¾ä¸ºï¼šå¦‚æœæ˜¯æ‰«æä»¶ï¼Œå¿…è·‘ï¼›å¦‚æœä¸æ˜¯æ‰«æä»¶ï¼Œä»…å½“ extract_image å¼ºå¼€å¯æ—¶è·‘
                
                try:
                    # å°† PDF å½“å‰é¡µè½¬ä¸ºå›¾ç‰‡
                    images = convert_from_path(
                        pdf_path, 
                        dpi=200, # 200 dpi å¯¹ VLM è¶³å¤Ÿäº†ï¼Œå¤ªé«˜è´¹æµé‡
                        first_page=page_num, 
                        last_page=page_num
                    )
                    
                    if images:
                        pil_image = images[0]
                        
                        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ä¾› VLM è¯»å–
                        temp_img_name = f"page_{page_num}_{int(time.time())}.jpg"
                        temp_img_path = os.path.join(temp_dir, temp_img_name)
                        pil_image.save(temp_img_path, "JPEG")
                        
                        # è°ƒç”¨ VLM
                        # å¦‚æœæ˜¯æ‰«æä»¶ï¼Œæç¤º VLM ä¾§é‡ OCRï¼›å¦åˆ™ä¾§é‡æ¶æ„åˆ†æ
                        context_hint = f"PDF Page {page_num}"
                        if is_scanned_page:
                            context_hint += " (Scanned Document - Please Extract Text)"
                        
                        logger.info(f"Invoking Qwen-VL-Max for page {page_num}...")
                        vlm_result = _analyze_image_with_vlm(temp_img_path, context=context_hint)
                        
                        page_data['content'].append({
                            'type': 'image_analysis',
                            'text': vlm_result,
                            'is_scanned_fallback': is_scanned_page
                        })
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ (å¯é€‰ï¼šå¦‚æœæƒ³ä¿ç•™å›¾ç‰‡è°ƒè¯•ï¼Œæ³¨é‡Šæ‰è¿™è¡Œ)
                        os.remove(temp_img_path)

                except Exception as e:
                    logger.error(f"Image processing failed for page {page_num}: {e}")

            doc_content.append(page_data)
            
    return doc_content


def parse_txt(path: str):
    with open(path, 'r', encoding='utf-8') as f:  
        text = f.read()
    paras = text.split(PARAGRAPH_SPLIT_SYMBOL)
    content = []
    for p in paras:
        content.append({'text': p})
    return [{'page_num': 1, 'content': content}]


def get_font(element):
    from pdfminer.layout import LTChar, LTTextContainer

    fonts_list = []
    for text_line in element:
        if isinstance(text_line, LTTextContainer):
            for character in text_line:
                if isinstance(character, LTChar):
                    fonts_list.append((character.fontname, character.size))

    fonts_list = list(set(fonts_list))
    if fonts_list:
        counter = Counter(fonts_list)
        most_common_fonts = counter.most_common(1)[0][0]
        return most_common_fonts
    else:
        return []


def extract_tables(pdf, page_num):
    table_page = pdf.pages[page_num]
    tables = table_page.extract_tables()
    return tables


def table_converter(table):
    table_string = ''
    for row_num in range(len(table)):
        row = table[row_num]
        cleaned_row = [
            item.replace('\n', ' ') if item is not None and '\n' in item else 'None' if item is None else item
            for item in row
        ]
        table_string += ('|' + '|'.join(cleaned_row) + '|' + '\n')
    table_string = table_string[:-1]
    return table_string


def extract_xls_schema(file_path: str) -> Dict[str, Any]:
    xls = pd.ExcelFile(file_path)
    schema = {
        "sheets": [],
        "n_sheets": len(xls.sheet_names)
    }

    for sheet_name in xls.sheet_names:
        df = xls.parse(sheet_name, nrows=3)  # è¯»å–å‰3è¡Œ

        dtype_mapping = {
            'object': 'string',
            'datetime64[ns]': 'datetime',
            'timedelta64[ns]': 'timedelta'
        }
        dtypes = df.dtypes.astype(str).replace(dtype_mapping).to_dict()

        sample_df = df.head(3).copy()
        for col in sample_df.columns:
            if is_datetime64_any_dtype(sample_df[col]):
                sample_df[col] = sample_df[col].dt.strftime('%Y-%m-%dT%H:%M:%S')

        sheet_info = {
            "name": sheet_name,
            "columns": df.columns.tolist(),
            "dtypes": dtypes,  
            "sample_data": sample_df.to_dict(orient='list') 
        }
        schema["sheets"].append(sheet_info)

    return schema


def extract_csv_schema(file_path: str) -> Dict[str, Any]:
    df_dtype = pd.read_csv(file_path, nrows=100)  
    df_sample = pd.read_csv(file_path, nrows=3) 

    return {
        "columns": df_dtype.columns.tolist(),
        "dtypes": df_dtype.dtypes.astype(str).to_dict(),
        "sample_data": df_sample.to_dict(orient='list'),
        "estimated_total_rows": _estimate_total_rows(file_path)
    }


def _estimate_total_rows(file_path) -> int:
    with open(file_path, 'rb') as f:
        line_count = 0
        chunk_size = 1024 * 1024  
        while chunk := f.read(chunk_size):
            line_count += chunk.count(b'\n')
    return line_count - 1  


def parse_tabular_file(file_path: str, **kwargs) -> List[dict]:
    try:
        df = pd.read_excel(file_path) if file_path.endswith(('.xlsx', '.xls')) else \
            pd.read_csv(file_path, **kwargs)
        if count_tokens(df_to_markdown(df)) > DEFAULT_MAX_INPUT_TOKENS:
            schema = extract_xls_schema(file_path) if file_path.endswith(('.xlsx', '.xls')) else \
                extract_csv_schema(file_path)
            return [{'page_num': 1, 'content': [{'schema': schema}]}]
        else:
            return [{'page_num': 1, 'content': [{'table': df_to_markdown(df)}]}]
    except Exception as e:
        logger.error(f"Table parsing failed: {str(e)}")
        return []  


def parse_zip(file_path: str, extract_dir: str) -> List[dict]:
    with zipfile.ZipFile(file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
        return [os.path.join(extract_dir, f) for f in zip_ref.namelist()]


def parse_html(file_path: str) -> List[dict]:
    from bs4 import BeautifulSoup  

    with open(file_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'lxml')

    content = [{'text': clean_text(p.get_text())}
               for p in soup.find_all(['p', 'div']) if p.get_text().strip()]

    return [{
        'page_num': 1,
        'content': content,
        'title': soup.title.string if soup.title else ''
    }]


def extract_xml_skeleton_markdown(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    markdown_lines = []

    def process_element(element, level=0, parent_path="", is_last=True, prefix=""):
        if level > 0:
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            markdown_lines.append(f"{prefix}{connector}**{element.tag}**")
        else:
            markdown_lines.append(f"## Root: {element.tag}")

        if element.attrib:
            attrs = [f"`{k}`" for k in element.attrib.keys()]
            attr_line = f"{prefix}{'    ' if level > 0 else ''}*Attributes:* {', '.join(attrs)}"
            markdown_lines.append(attr_line)

        if element.text and element.text.strip():
            text_line = f"{prefix}{'    ' if level > 0 else ''}*Has text content*"
            markdown_lines.append(text_line)
        seen_tags = set()
        unique_children = []
        for child in element:
            if child.tag not in seen_tags:
                seen_tags.add(child.tag)
                unique_children.append(child)

        for i, child in enumerate(unique_children):
            is_last_child = (i == len(unique_children) - 1)
            child_prefix = prefix + ("    " if is_last else "â”‚   ")
            process_element(child, level + 1,
                            f"{parent_path}/{element.tag}" if parent_path else element.tag,
                            is_last_child, child_prefix)

    process_element(root)
    markdown_content = "\n".join(markdown_lines)
    return markdown_content


def parse_xml(file_path: str) -> List[dict]:
    with open(file_path, 'r', encoding='utf-8') as f: 
        text = f.read()
    if count_tokens(text) > DEFAULT_MAX_INPUT_TOKENS:
        schema = extract_xml_skeleton_markdown(file_path)
        content = [{'schema': schema}]
    else:
        content = [{'text': text}]
    return [{'page_num': 1, 'content': content}]


def compress(results: list) -> list[str]:
    compress_results = []
    max_token = math.floor(DEFAULT_MAX_INPUT_TOKENS / len(results))
    for result in results:
        token_list = tokenizer.tokenize(result)
        token_list = token_list[:min(len(token_list), max_token)]
        compress_results.append(tokenizer.convert_tokens_to_string(token_list))
    return compress_results


# @register_tool('file_parser')
class SingleFileParser:
    name = "file_parser"
    description = f"File parsing tool, supports parsing data in  {'/'.join(PARSER_SUPPORTED_FILE_TYPES)} formats, and returns the parsed markdown format data."
    parameters = [{
        'name': 'url',
        'type': 'string',
        'description': 'The full path of the file to be parsed, which can be a local path or a downloadable http(s) link.',
        'required': True
    }]

    def __init__(self, cfg: Optional[Dict] = None):
        # è®¾ç½®æ•°æ®å­˜å‚¨æ ¹ç›®å½•
        self.cfg = cfg
        self.data_root = self.cfg.get('path', os.path.join(DEFAULT_WORKSPACE, 'tools', self.name))
        
        # ä¿®æ”¹ç‚¹ 1: ç§»é™¤ self.db = Storage(...)
        # æ”¹ä¸ºç›´æ¥ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œç”¨äºå­˜æ”¾ JSON ç¼“å­˜æ–‡ä»¶
        if not os.path.exists(self.data_root):
            os.makedirs(self.data_root, exist_ok=True)

        self.structured_doc = self.cfg.get('structured_doc', True)

        self.parsers = {
            'pdf': parse_pdf,
            'docx': parse_word,
            'doc': parse_word,
            'pptx': parse_ppt,
            'txt': parse_txt,
            'jsonl': parse_txt,
            'jsonld': parse_txt,
            'pdb': parse_txt,
            'py': parse_txt,
            "png": parse_png,
            'html': parse_html,
            'xml': parse_xml,
            'csv': lambda p: parse_tabular_file(p, sep=','),
            'tsv': lambda p: parse_tabular_file(p, sep='\t'),
            'xlsx': parse_tabular_file,
            'xls': parse_tabular_file,
            'zip': self.parse_zip
        }
    
    def _verify_json_format_args(self, params: Union[str, dict], strict_json: bool = False) -> dict:
        """Verify the parameters of the function call"""
        if isinstance(params, str):
            try:
                if strict_json:
                    params_json: dict = json.loads(params)
                else:
                    params_json: dict = json_loads(params)
            except json.decoder.JSONDecodeError:
                raise ValueError('Parameters must be formatted as a valid JSON!')
        else:
            params_json: dict = params
        if isinstance(self.parameters, list):
            for param in self.parameters:
                if 'required' in param and param['required']:
                    if param['name'] not in params_json:
                        raise ValueError('Parameters %s is required!' % param['name'])
        elif isinstance(self.parameters, dict):
            import jsonschema
            jsonschema.validate(instance=params_json, schema=self.parameters)
        else:
            raise ValueError
        return params_json

    def call(self, params: Union[str, dict], **kwargs) -> Union[str, list]:
        params = self._verify_json_format_args(params)
        file_path = self._prepare_file(params['url'])
        
        # ä¿®æ”¹ç‚¹ 2: ä½¿ç”¨æœ¬åœ°æ–‡ä»¶æ£€æŸ¥æ›¿ä»£ self.db.get()
        # æ„é€ ç¼“å­˜æ–‡ä»¶å: hash + _ori.json
        file_hash = hash_sha256(file_path)
        cache_file_path = os.path.join(self.data_root, f'{file_hash}_ori.json')

        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(cache_file_path):
            try:
                with open(cache_file_path, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                logger.info(f"Hit cache for {file_path}")
                return self._flatten_result(cached_data)
            except Exception as e:
                logger.warning(f"Cache file exists but read failed: {e}, reparsing...")
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–è¯»å–å¤±è´¥ï¼Œæ‰§è¡Œè§£æ
        return self._flatten_result(self._process_new_file(file_path))

    def _prepare_file(self, path: str) -> str:
        if is_http_url(path):
            download_dir = os.path.join(self.data_root, hash_sha256(path))
            os.makedirs(download_dir, exist_ok=True)
            return save_url_to_local_work_dir(path, download_dir)
        return sanitize_chrome_file_path(path)

    def _process_new_file(self, file_path: str) -> Union[str, list]:
        file_type = get_file_type(file_path)
        idp_types = ['pdf', 'docx', 'pptx', 'xlsx', 'jpg', 'png', 'mp3']
        logger.info(f'Start parsing {file_path}...')
        logger.info(f'File type {file_type}...')
        
        if file_type not in idp_types:
            try:
                # å°è¯•ä» url/path ä¸­è·å–åç¼€ï¼Œå¦‚æœæ²¡æœ‰å–åˆ°å¯èƒ½ä¼šæŠ¥é”™ï¼Œè¿™é‡ŒåŠ ä¸ªç®€å•çš„ä¿æŠ¤
                base_name = get_basename_from_url(file_path)
                if '.' in base_name:
                    file_type = base_name.split('.')[-1].lower()
            except:
                pass

        try:
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç¡®ä¿ parse_ppt/parse_pdf ç­‰å‡½æ•°åœ¨ä½œç”¨åŸŸå†…
            if file_type in self.parsers:
                results = self.parsers[file_type](file_path)
            else:
                # å¦‚æœç±»å‹ä¸æ”¯æŒï¼Œå°è¯•å½“åš txt å¤„ç†æˆ–è€…æŠ¥é”™
                logger.warning(f"Unsupported file type: {file_type}, trying txt parser.")
                results = parse_txt(file_path)

            tokens = 0
            for page in results:
                for para in page['content']:
                    if 'schema' in para:
                        para['token'] = count_tokens(json.dumps(para['schema']))
                    else:
                        para['token'] = count_tokens(para.get('text', para.get('table', '')))
                    tokens += para['token']

            if not results or not tokens:
                logger.error(f"Parsing failed: No information was parsed")
                # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œå»æ‰äº†è‡ªå®šä¹‰ Exceptionï¼Œå¦‚æœéœ€è¦å¯ä»¥ä¿ç•™
                raise ValueError("Document parsing failed: No content")
            else:
                self._cache_result(file_path, results)
                return results
        except Exception as e:
            logger.error(f"Parsing failed: {str(e)}")
            raise e

    def _cache_result(self, file_path: str, result: list):
        # ä¿®æ”¹ç‚¹ 3: ä½¿ç”¨æ ‡å‡†æ–‡ä»¶å†™å…¥æ›¿ä»£ self.db.put()
        try:
            file_hash = hash_sha256(file_path)
            cache_file_path = os.path.join(self.data_root, f'{file_hash}_ori.json')
            
            with open(cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False)
                
            logger.info(f'The parsing result of {file_path} has been cached to {cache_file_path}')
        except Exception as e:
            logger.error(f"Failed to cache result: {str(e)}")

    def _flatten_result(self, result: list) -> str:
        return PARAGRAPH_SPLIT_SYMBOL.join(
            para.get('text', para.get('table', ''))
            for page in result for para in page['content']
        )

    def parse_zip(self, file_path: str) -> List[dict]:
        extract_dir = os.path.join(self.data_root, f"zip_{hash_sha256(file_path)}")
        os.makedirs(extract_dir, exist_ok=True)

        results = []
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾å¤–éƒ¨æœ‰ parse_zip å‡½æ•°ç”¨äºè§£å‹è·å–æ–‡ä»¶åˆ—è¡¨
        # å¦‚æœæ²¡æœ‰å¤–éƒ¨å‡½æ•°ï¼Œéœ€è¦åœ¨è¿™é‡Œå®ç° unzip é€»è¾‘
        for extracted_file in parse_zip(file_path, extract_dir):
            if (ft := get_file_type(extracted_file)) in self.parsers:
                try:
                    results.extend(self.parsers[ft](extracted_file))
                except Exception as e:
                    logger.warning(f"Skip files {extracted_file}: {str(e)}")

        if not results:
            raise ValueError("No parseable content found in the ZIP file")
        return results

if __name__ == "__main__":
    import time
    import logging
    
    # 1. é…ç½®æ—¥å¿—è¾“å‡ºï¼Œç¡®ä¿èƒ½çœ‹åˆ° "Start parsing" å’Œ "Hit cache"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    print("\n" + "="*60)
    print(" ğŸš€ å¼€å§‹çœŸå®æ•°æ®æµ‹è¯• (Real Data Test) ")
    print("="*60)

    # =========================================================================
    # [é…ç½®åŒº] è¯·ä¿®æ”¹è¿™é‡Œ
    # =========================================================================
    
    # 1. è®¾ç½®å¾…æµ‹è¯•çš„çœŸå®æ–‡ä»¶è·¯å¾„ (æ”¯æŒ PDF, PPTX, DOCX ç­‰)
    # Windows ç”¨æˆ·è¯·æ³¨æ„è·¯å¾„è½¬ä¹‰ï¼Œä¾‹å¦‚ "D:\\Documents\\architecture_v1.pptx"
    TEST_FILE_PATH = "/home/u-longyy/ç”µè„‘ç»´ä¿®è¯æ˜.doc"  # <-- ä¿®æ”¹ä¸ºä½ çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„

    # 2. æ£€æŸ¥ API Key (å¦‚æœä½ å¯ç”¨äº† VLM è§†è§‰åˆ†æ)
    if not os.getenv("DASHSCOPE_API_KEY"):
        logger.warning("âš ï¸ æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ã€‚")
        logger.warning("   å¦‚æœä½ çš„æ–‡ä»¶åŒ…å«å›¾ç‰‡ä¸”éœ€è¦ VLM è§£ææµç¨‹å›¾ï¼Œè¯·å…ˆè®¾ç½® Keyï¼Œå¦åˆ™ VLM è°ƒç”¨å°†å¤±è´¥ã€‚")
        # os.environ["DASHSCOPE_API_KEY"] = "sk-ä½ çš„Key" # ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä¸´æ—¶ç¡¬ç¼–ç 

    # =========================================================================
    
    if not os.path.exists(TEST_FILE_PATH):
        logger.error(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {TEST_FILE_PATH}")
        logger.error("è¯·ä¿®æ”¹ä»£ç ä¸­çš„ TEST_FILE_PATH å˜é‡ä¸ºçœŸå®çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€‚")
        exit(1)

    # åˆå§‹åŒ– Parser
    # ç¼“å­˜å°†ç”Ÿæˆåœ¨å½“å‰ç›®å½•ä¸‹çš„ ./workspace/parser_cache ä¸­
    workspace_dir = "./workspace"
    parser = SingleFileParser(cfg={
        'path': os.path.join(workspace_dir, 'parser_cache'),
        'structured_doc': True 
    })

    # --- æµ‹è¯•é˜¶æ®µ 1: é¦–æ¬¡è§£æ (å†™å…¥æ–‡ä»¶ç¼“å­˜) ---
    print(f"\n[Phase 1] æ­£åœ¨è§£ææ–‡ä»¶: {os.path.basename(TEST_FILE_PATH)}")
    print("â³ è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ (å–å†³äºæ–‡ä»¶å¤§å°å’Œæ˜¯å¦è°ƒç”¨ VLM)...")
    
    t0 = time.time()
    # æ„é€ è¾“å…¥å‚æ•° (æ¨¡æ‹Ÿ Agent è°ƒç”¨)
    input_params = {"url": TEST_FILE_PATH}
    
    try:
        # è°ƒç”¨ call æ–¹æ³•
        result_text = parser.call(input_params)
        t1 = time.time()
        
        print(f"âœ… è§£æå®Œæˆ! è€—æ—¶: {t1 - t0:.2f}s")
        print("-" * 30)
        print("ğŸ“„ è§£æç»“æœé¢„è§ˆ (å‰ 500 å­—ç¬¦):")
        print(result_text)
        print("..." if len(result_text) > 500 else "")
        print("-" * 30)

    except Exception as e:
        logger.error(f"âŒ è§£æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        exit(1)

    # --- æµ‹è¯•é˜¶æ®µ 2: ç¼“å­˜å‘½ä¸­æµ‹è¯• (è¯»å–æœ¬åœ° JSON) ---
    print(f"\n[Phase 2] äºŒæ¬¡è¯»å–æµ‹è¯• (éªŒè¯æœ¬åœ°æ–‡ä»¶ç¼“å­˜æœºåˆ¶)")
    
    t2 = time.time()
    cached_result = parser.call(input_params)
    t3 = time.time()

    print(f"â±ï¸ äºŒæ¬¡è¯»å–è€—æ—¶: {t3 - t2:.4f}s")
    
    if (t3 - t2) < 1.0:
        print("âœ… é€Ÿåº¦æå¿«ï¼ŒæˆåŠŸå‘½ä¸­æœ¬åœ° JSON ç¼“å­˜ã€‚")
    else:
        print("âš ï¸ é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½æœªå‘½ä¸­ç¼“å­˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")

    # --- éªŒè¯å†…å®¹ä¸€è‡´æ€§ ---
    if result_text == cached_result:
        print("âœ… å†…å®¹ä¸€è‡´æ€§æ ¡éªŒé€šè¿‡ã€‚")
    else:
        print("âŒ å†…å®¹ä¸ä¸€è‡´!")

    # --- æ£€æŸ¥ VLM æ•ˆæœ (å¦‚æœæ˜¯ PPT/PDF) ---
    if "å›¾ç‰‡åˆ†æ" in result_text or "Mermaid" in result_text or "Visual Analysis" in result_text:
        print("\nğŸ‰ æ£€æµ‹åˆ° VLM åˆ†æå†…å®¹ï¼æµç¨‹å›¾/æ¶æ„å›¾å·²è¢«æˆåŠŸæå–ä¸ºæ–‡æœ¬æè¿°ã€‚")
    else:
        print("\nâ„¹ï¸ æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ VLM åˆ†ææ ‡è®°ã€‚")
        print("   (åŸå› å¯èƒ½æ˜¯ï¼šæ–‡ä»¶ä¸­æ— å›¾ç‰‡ã€parse_pdf/ppt ä¸­ extract_image é»˜è®¤ä¸º Falseã€æˆ– API è°ƒç”¨æœªç”Ÿæ•ˆ)")

    print("\næµ‹è¯•ç»“æŸã€‚ç¼“å­˜æ–‡ä»¶ä½äº:", parser.data_root)