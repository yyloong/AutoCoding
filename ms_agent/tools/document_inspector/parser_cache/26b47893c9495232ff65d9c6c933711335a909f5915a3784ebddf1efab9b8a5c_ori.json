[{"page_num": 1, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning\nwith Explanation\nZelei Cheng * 1 Xian Wu * 1 Jiahao Yu 1 Sabrina Yang 2 Gang Wang 3 Xinyu Xing 1\nAbstract However, training an optimal DRL agent for complex tasks,\nDeep reinforcement learning (DRL) is playing an particularly in environments with sparse rewards, presents\nincreasingly important role in real-world applica- a significant challenge. Often cases, the training of a DRL\ntions. However, obtaining an optimally perform- agent can hit a bottleneck without making further process:\ning DRL agent for complex tasks, especially with its sub-optimal performance becomes evident when it makes\nsparse rewards, remains a significant challenge. common mistakes or falls short of achieving the final goals.\nThe training of a DRL agent can be often trapped\nWhen the DRL agent hits its training bottleneck, a refine-\nin a bottleneck without further progress. In this\nment strategy can be considered, especially if the agent is\npaper, we propose RICE, an innovative refining\nalready locally optimal. To refine the locally optimal DRL\nscheme for reinforcement learning that incorpo-\nagent, one method is to analyze its behavior and patch the\nrates explanation methods to break through the\nerrors it made. A recent work (Cheng et al., 2023) proposes\ntraining bottlenecks. The high-level idea of RICE\nStateMask to identify critical states of the agent using an ex-\nis to construct a new initial state distribution that\nplanation method. One utility of StateMask is patching the\ncombines both the default initial states and criti-\nagent’s error, which fine-tunes the DRL agent starting from\ncal states identified through explanation methods,\nthe identified critical states (denoted as “StateMask-R”).\nthereby encouraging the agent to explore from\nHowever, such an approach suffers from two drawbacks.\nthe mixed initial states. Through careful design,\nOn the one hand, initializing solely from critical states will\nwe can theoretically guarantee that our refining\nhurt the diversity of initial states, which can cause overfitting\nscheme has a tighter sub-optimality bound. We\n(see Appendix D). On the other hand, fine-tuning alone can-\nevaluate RICE in various popular RL environ-\nnot help the DRL agent jump out of the local optima. These\nments and real-world applications. The results\nobservations drive us to rethink how to design a proper ini-\ndemonstrate that RICE significantly outperforms\ntial distribution and apply exploration-based techniques to\nexisting refining schemes in enhancing agent per-\npatch previous errors.\nformance.\nAnother reason behind the training bottleneck can be the\npoor choice of the training algorithm. Naturally, to improve\n1. Introduction performance, the developer needs to select another DRL\ntraining algorithm to re-train the DRL agent. However, for\nDeep reinforcement learning (DRL) has shown promising\ncomplex DRL tasks, re-training the agent from scratch is\nperformance in various applications ranging from playing\ntoo costly. For instance, for AlphaStar (Vinyals et al., 2019)\nsimulated games (Todorov et al., 2012; Mnih et al., 2013; Oh\nto attain grandmaster-level proficiency in StarCraft, its train-\net al., 2016; Cai et al., 2023) to completing real-world tasks\ning period exceeds one month with TPUs. Retraining an\nsuch as navigating autonomous vehicles and performing\nagent of this level can incur a cost amounting to millions of\ncybersecurity attacks and defenses (Bar-Zur et al., 2023;\ndollars (Agarwal et al., 2022). Therefore, existing research\nVyas et al., 2023; Anderson et al., 2018; Wang et al., 2023).\nhas investigated the reuse of previous DRL training (as prior\nknowledge) to facilitate re-training (Ho & Ermon, 2016;\n*Equal contribution 1Department of Computer Science,\nNorthwestern University, Evanston, Illinois, USA 2Presentation Fu et al., 2018; Cai et al., 2022). The most recent exam-\nHigh School, San Jose, California, USA 3Department of Com- ple is Jump-Start Reinforcement Learning (JSRL) proposed\nputer Science, University of Illinois at Urbana-Champaign, by Uchendu et al. (2023) which leverages a pre-trained\nUrbana, Illinois, USA. Correspondence to: Xinyu Xing\npolicy to design a curriculum to guide the training of a self-\n<xinyu.xing@northwestern.edu>.\nimproving exploration policy. However, their selection of\nProceedings of the 41 st International Conference on Machine exploration frontiers in the curriculum is random, which\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by cannot guarantee that the exploration frontiers have positive\nthe author(s).\n1", "source": "pdfplumber_text", "token": 1100}]}, {"page_num": 2, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nreturns. This motivates us to incorporate explanation meth- than those informed by random explanation.\nods to scrutinize the pre-trained policy and design more\n• We propose an alternative design of StateMask to ex-\neffective exploration frontiers.\nplain the agent’s policy in DRL-based applications.\nIn this work, we propose RICE1, a Refining scheme for Experiments show that our explanation has compara-\nReInforCement learning with Explanation. We first lever- ble fidelity with StateMask while improving efficiency.\nage a state-of-the-art explanation method to derive a step-\n• With extensive evaluations and case studies, we il-\nlevel explanation for the pre-trained DRL policy. The expla-\nlustrate the benefits of using RICE to improve a pre-\nnation method identifies the most critical states (i.e., steps\ntrained policy.\nthat contribute the most to the final reward of a trajectory),\nwhich will be used to construct the exploration frontiers.\nBased on the explanation results, we construct a mixed ini- 2. Related Work\ntial state distribution that combines the default initial states\n2.1. Explanation-based Refining\nand the identified critical states to prevent the overfitting\nproblem. By forcing the agent to revisit these exploration Recently, there has been some work that leverages the DRL\nfrontiers, we further incentivize the agent to explore starting explanation to improve the agent’s performance. These ex-\nfrom the frontiers. Through exploration, the agent is able to planations can be derived from either human feedback or\nexpand state coverage, and therefore more effectively break automated processes. Guan et al. (2021); Van Waveren et al.\nthrough the bottlenecks of reinforcement learning training. (2022) propose to utilize human feedback to correct the\nOur theoretical analysis shows that this method achieves a agent’s failures. More specifically, when the agent fails, hu-\ntighter sub-optimality bound by utilizing this mixed initial mans (can be non-experts) are involved to point out how to\ndistribution (see Section 3.4). avoid such a failure (i.e., what action should be done instead,\nand what action should be forbidden). Based on human feed-\nIn addition, we introduce key improvements to the state-of-\nback, the DRL agent gets refined by taking human-advised\nthe-art explanation method StateMask (Cheng et al., 2023)\naction in those important time steps and finally obtains the\nto better facilitate our refining scheme. We reformulate the\ncorrected policy. The downside is that it relies on humans to\nobjective function and add a new reward bonus for encour-\nidentify critical steps and craft rules for alternative actions.\naging blinding when training—this significantly simplifies\nThis can be challenging for a large action space, and the re-\nthe implementation without sacrificing the theoretical guar-\ntraining process is ad-hoc and time-consuming. Cheng et al.\nantee.\n(2023); Yu et al. (2023) propose to use step-level DRL expla-\nEvaluation and Findings. We evaluate the perfor- nation methods to automatically identify critical time steps\nmance of RICE using four MuJoCo games and four DRL- and refine the agent accordingly. It initiates the refining pro-\nbased real-world applications, including cryptocurrency cess by resetting the environment to the critical states and\nmining (Bar-Zur et al., 2023), autonomous cyber defense subsequently resumes training the DRL agents from these\n(Cage Challenge 2) (CAGE, 2022), autonomous driving (Li critical states. Empirically, we observe that this refining\net al., 2022), and malware mutation (Raff et al., 2017). strategy can easily lead to overfitting (see Appendix D). In-\nWe show that the explanation derived from our new de- stead, we propose a novel refining strategy with theoretical\nsign demonstrates similar fidelity to the state-of-the-art guarantees to improve the agent’s performance.\ntechnique StateMask (Cheng et al., 2023) with signifi-\ncantly improved training efficiency. With the explana- 2.2. Leveraging Existing Policy\ntion results, we show our refining method can produce\nhigher performance improvements for the pre-trained DRL The utilization of existing policies to initialize RL and en-\nagent, in comparison with existing approaches including hance exploration has been explored in previous literature.\nJSRL (Uchendu et al., 2023) and the original refining Some studies propose to “roll-in” with an existing policy\nmethod from StateMask (Cheng et al., 2023). for better exploration, as demonstrated in works (Agarwal\net al., 2020; Li et al., 2023). Similar to our approach, JSRL\nIn summary, our paper has the following contributions:\n(Uchendu et al., 2023) incorporates a guide policy for roll-in,\nfollowed by a self-improving exploration policy. Techni-\ncally, JSRL relies on a curriculum for the gradual update of\n• We develop a refining strategy to break through the\nthe exploration frontier. However, the curriculum may not\nbottlenecks of reinforcement learning training with an\nbe able to truly reflect the key reasons why the guide policy\nexplanation (which is backed up by a theoretical anal-\nsucceeds or fails. Therefore, we propose to leverage the\nysis). We show our refining method performs better\nexplanation method to automatically identify crucial states,\n1The source code of RICE can be found in https:// facilitating the rollout of the policy by integrating these iden-\ngithub.com/chengzelei/RICE tified states with the default initial states. In Section 4, we\n2", "source": "pdfplumber_text", "token": 1239}]}, {"page_num": 3, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nCritical state Initial state Original trajectory Explore trajectory\n(a) Original trajectories generated by a (b) Mix default initial states and identified (c) Rollout new trajectories with\npre-trained DRL policy. critical states based on explanation. RND-based exploration.\nFigure 1. Given a pre-trained DRL policy that is not fully optimal (a), we propose the RICE algorithm that resets the RL agent to specific\nvisited states (a mixture of default initial states and identified critical states) (b), followed by an exploration step initiated from these\nchosen states (c).\nempirically demonstrate that JSRL performs poorly in our which may be sub-optimal. Our objective is to break through\nselected games. Chang et al. (2023) propose PPO++ that the training bottlenecks of the pre-trained policy with an ex-\nreset the environment to a mixture of the default initial states planation. Rather than re-training from scratch, we propose\nand the visited states of a guide policy (i.e., a pre-trained to utilize explanation to take full advantage of the guidance\npolicy). It can be viewed as a special case in our framework, of the pre-trained policy π. Importantly, we do not assume\ni.e., constructing a mixed initial distribution with a random knowledge of the original training algorithm used for pol-\nexplanation. However, we claim that not all visited states icy π. And we make the following assumptions about the\nof a pre-trained policy are informative and our theoretical quality of π.\nanalysis and experiments both show that RICE based on our Assumption 3.1. Given a random policy πr, we have\nexplanation method outperforms the refining method based E [Aπ(s, a)] ≤ 0, ∀s.\na∼πr\non a random explanation.\nIntuitively, the above assumption implies that taking an\naction based on a random policy πr will provide a lower ad-\n3. Proposed Technique\nvantage than taking actions based on the policy π. This is a\nreasonable assumption since π is a pre-trained policy, thus it\n3.1. Problem Setup and Assumption\nwould perform much better than an untrained (i.e., random)\nWe model the problem as a Markov Decision Process policy.\n(MDP), which is defined as a tuple ⟨S, A, P, ρ, R,\nAssumption 3.2. The pre-trained policy π cover the states\nw\nγ⟩\nh\n.\ner\nI\ne\nn\ne\nt\na\nh\nc\ni\nh\ns t\ns\nup\na\nle\nn\n,\nd\nS\na\nan\nre\nd\npr\nA\nese\na\nn\nre\nts\nth\nth\ne\ne\ns\ns\nta\nta\nte\nte\na\na\nn\nn\nd\nd\na\na\nc\nc\nti\nt\no\nio\nn\nn\nse\no\nt\nf\n,\nvisited by the optimal policy π∗:\n(cid:13) (cid:13)\n(cid:13)\ndπ\nρ\n∗ (cid:13) (cid:13)\n(cid:13) ≤ C, where C is\nthe agent at t t ime t. t P : S × A → ∆(S) is the state a constant. (cid:13) dπ ρ (cid:13) ∞\ntransition function, R : S × A → R is the reward\nIn other words, Assumption 3.2 requires that the pre-trained\nfunction. γ ∈ (0, 1) is the discount factor. For a pol-\npolicy visits all good states in the state space. Note that it is\nicy π(a|s): S → A, the value function and Q-function\nis defined as V π(s) = E [ (cid:80)∞ γtR(s , a ) | s = s] a standard assumption in the online policy gradient learning\nπ t=0 t t 0\nand Qπ(s, a) = E [ (cid:80)∞ γtR(s , a ) | s = s, a = a]. (Agarwal et al., 2021; Uchendu et al., 2023; Li et al., 2023)\nπ t=0 t t 0 0 and is much weaker than the single policy concentrateabil-\nThe advantage function for the policy π is denoted as\nAπ(s, a) = Qπ(s, a) − V π(s). We assume the initial ity coefficient assumption (Rashidinejad et al., 2021; Xie\net al., 2021), which requires the pre-trained policy visits all\nstate distribution is given by ρ: s ∼ ρ. The goal of\n0\nRL is to find an optimal policy π∗ that maximizes its ex- good state-action pairs. The ratio in Assumption 3.2 is also\npected total reward : π∗ = arg max E [V π(s)]. Be- referred to as the distribution mismatch coefficient.\nπ s∼ρ\nsides, we also introduce the state occupancy distribution\nand the state-action occupancy measure for π, denoted 3.2. Technical Overview\nas dπ(s) = (1 − γ) (cid:80)∞ γt Prπ (s = s | s ∼ ρ) and\nρ t=0 t 0 Recall our goal is to refine the pre-trained DRL agent to\ndπ(s, a) = dπ(s)π(a|s).\nρ ρ break through the training bottlenecks. At a high level, the\nIn our setting, we have a pre-trained policy denoted as π, RICE algorithm integrates a roll-in step, where the RL agent\nis reset to specific visited states, followed by an exploration\n3", "source": "pdfplumber_text", "token": 1288}]}, {"page_num": 4, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nstep initiated from these chosen states. During the roll-in Algorithm 1 Training the Mask Network.\nstep, we draw inspiration from established RL-explanation Input: Target agent’s policy π\nmethods (Puri et al., 2019; Guo et al., 2021; Cheng et al., Output: Mask network π˜\nθ\n2023) to identify critical states, referred to as exploration Initialization: Initialize the weights θ for the mask net π˜ θ\nfrontiers, within the given policy π. As depicted in Figure 1, θ old ← θ\nfor iteration=1, 2, . . . do\nwhen presented with a trajectory sampled from the policy\nSet the initial state s ∼ ρ\n0\nπ, we employ a step-level explanation method – StateMask D ← ∅\n(Cheng et al., 2023) to identify the most crucial time steps for t=0 to T do\ninfluencing the final rewards in this trajectory. Subsequently, Sample a t ∼ π(a t |s t )\nwe guide the RL agent to revisit these selected states. The S C a o m m p p l u e te am t the ∼ ac π˜ t θ u o a l l d t ( a a k m t en |s a t c ) tion a ← a ⊙ am\nrationale behind revisiting these states lies in their ability to t t\n(s , R′ ) ← env.step(a) and record (s , s , am, R′ )\noffer an expanded initial state distribution compared to ρ, in t D +1 t t t+1 t t\nthereby enabling the agent to explore diverse and relevant end for\nstates it might otherwise neglect. Additionally, we intro- update θ old ← θ using D by PPO algorithm\nduce a mixing of these selected states with the initial states end for\nsampled from ρ. This mixing approach serves the purpose\nof preventing the agent from overfitting to specific states.\nwhere π denotes the policy of the target agent (i.e., our pre-\nIn Section 3.4, we theoretically show that RICE achieves\ntrained policy), π¯ denotes the policy of the perturbed agent\na tighter regret bound through the utilization of this mixed\n(i.e., integrating the random policy and the target agent π\ninitial distribution.\nvia the mask network π˜), η(·) is the expected total reward of\nThen, we propose an exploration-based method to further an agent by following a certain policy. To solve the Eqn. (2)\nenhance the DRL agent’s performance. The high-level idea with monotonicaly guarantee, StateMask carefully designs\nis to incentivize the agent to explore when initiating actions a surrogate function and utilize the prime-dual methods to\nfrom these frontiers. Intuitively, the pre-trained policy π optimize the π˜. However, we can optimize the learning\nmight converge to a local optimal, as shown in Figure 1. process of mask net within our setting to enhance simplicity.\nThrough exploration, we aim to expand state coverage by Specifically, we have the following theorem\nrewarding the agent for visiting novel states, thereby in-\nTheorem 3.3. Under Assumption 3.1, we have η(π¯) upper-\ncreasing the likelihood of successfully completing the task.\nbounded by η(π): η(π¯) ≤ η(π).\nSpecifically, we utilize the Proximal Policy Optimization\n(PPO) algorithm (Schulman et al., 2017) for refining the\nThe proof of the theorem can be found in Appendix A.\nDRL agent, leveraging the monotonicity of PPO.\nLeveraging this theorem, we can transform the objective\nfunction to J(θ) = max η(π¯). With this reformulation,\n3.3. Technique Detail we can utilize the vanilla PPO algorithm to train the state\nmask without sacrificing the theoretical guarantee. How-\nStep-level Explanation. We leverage a state-of-the-art\never, na¨ıvely maximizing the expected total reward may\nexplanation method StateMask (Cheng et al., 2023). At a\nintroduce a trivial solution to the problem which is to not\nhigh level, StateMask parameterizes the importance of the\nblind the target agent at all (always outputs “0”). To tackle\ntarget agent’s current time step as a neural network model\nthis problem, we add an additional reward by giving an extra\n(i.e., mask network). This neural network takes the current\nbonus when the mask net outputs “1”. The new reward can\nstate as input and then outputs this step’s importance score\nbe written as R′(s , a ) = R(s , a ) + αam where α is a\nwith respect to the agent’s final reward. To do so, StateMask t t t t t\nhyper-parameter. We present the learning process of the\nlearns a policy to “blind” the target agent at certain steps\nmask network in Algorithm 1. By applying this resolved\nwithout changing the agent’s final reward. Specifically, for\nmask to each state, we will be able to assess the state impor-\nan input state s , the mask net outputs a binary action am of\nt t tance (i.e., the probability of mask network outputting “0”)\neither “zero” or “one”, and the target agent will sample the\nat any time step.\naction a from its policy. The final action is determined by\nt\nthe following equation Constructing Mixed Initial State Distribution. With the\n(cid:40) a , if am = 0 , state mask π˜, we construct a mixed initial state distribution\na ⊙ am = t t (1) to expand the coverage of the state space. Initially, we\nt t a if am = 1 ,\nrandom t randomly sample a trajectory by executing the pre-trained\nThe mask net is then trained to minimize the following policy π. Subsequently, the state mask is applied to pinpoint\nobjective function: the most important state within the episode τ by assessing\nthe significance of each state. The resulting distribution of\nJ(θ) = min |η(π) − η(π¯)| , (2) these identified critical states is denoted as dπˆ(s). Indeed,\nρ\n4", "source": "pdfplumber_text", "token": 1368}]}, {"page_num": 5, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nAlgorithm 2 Refining the DRL Agent. 3.4. Theoretical Analysis\nInput: Pre-trained policy π, corresponding state mask π˜, default\nFinally, we provide theoretical analysis demonstrating that\ninitial state distribution ρ, reset probability threshold p\nOutput: The agent’s policy after refining π′ our refining algorithm can tighten the sub-optimality gap:\nfor iteration=1, 2, . . . do SubOpt := V π∗(ρ) − V π′(ρ), (i.e., the gap between the\nD ← ∅ long-term reward collected by the optimal policy π∗ and\nRAND NUM ← RAND(0,1) that obtained by the refined policy π′ when starting from\nif RAND NUM < p then\nthe default initial state distribution ρ).\nRun π to obtain a trajectory τ of length K\nIdentify the most critical state s t in τ via state mask π˜ In particular, we aim to answer the following two questions:\nSet the initial state s ← s\n0 t Q1:What are the benefits of incorporating StateMask to\nelse\ndetermine the exploration frontier?\nSet the initial state s ∼ ρ\n0\nend if\nQ2: what advantages does starting exploration from the\nfor t=0 to T do\nmixed initial distribution offer?\nSample a ∼ π(a |s )\nt t t\n(s , R ) ← env.step(a )\nt+1 t t To answer the questions, we first show that determining the\n(cid:13) (cid:13)2\nCalculate RND bonus R t RND = (cid:13) (cid:13) f (s t+1 ) − fˆ(s t+1 )(cid:13) (cid:13) exploration frontiers based on StateMask is equivalent to\nwith normalization sampling states from a better policy compared to π. Then,\nAdd (s t , s t+1 , a t , R t + λR t RND) to D we demonstrate that under the mixed initial distribution as\nend for\nintroduced in Section 3.3, we could provide a tighter upper\nOptimize π w.r.t PPO loss on D\nθ bound for the sub-optimality of trained policy π compared\nOptimize fˆ w.r.t. MSE loss on D using Adam\nθ with randomly selecting visited states to form the initial\nend for\nπ′ ← π distribution.\nθ\nIn order to answer Q1, we begin with Assumption 3.4 to\nassume the relationship between the policy value and the\nstate distribution mismatch coefficient.\nin Section 3.4, we demonstrate that this re-weighting-like Assumption 3.4. For two polices π and πˆ, if η(πˆ) ≥ η(π),\nsampling is equivalent to sampling the state from a better (cid:13) (cid:13) dπ∗ (cid:13) (cid:13) (cid:13) (cid:13) dπ∗ (cid:13) (cid:13)\np o o f l t i h c e y s πˆ e . le W ct e ed th i e m n p s o et rt t a h n e t i s n t i a t t i e a s l d d i i s s t t r r i i b b u u t t i i o o n n µ dπˆ a ( s s a ) m an ix d tu th r e e then we have (cid:13) (cid:13) d ρ π ρ ˆ (cid:13) (cid:13) ∞ ≤ (cid:13) (cid:13) d ρ π ρ (cid:13) (cid:13) ∞ .\nρ\noriginal initial distribution of interest ρ: µ(s) = βdπˆ(s) + Intuitively, this assumption posits that a superior policy\nρ\n(1 − β)ρ(s), where β is a hyper-parameter. would inherently possess a greater likelihood of visiting all\nfavorable states. We give validation of this assumption in a\nExploration with Random Network Distillation. Start- 2-state MDP in Appendix B.1.\ning from the new initial state distribution, we continue train-\nWe further present Lemma 3.5 to answer Q1, i.e., the bene-\ning the DRL agent while encouraging the agent to do ex-\nfits of incorporating StateMask to determine the exploration\nploration. In contrast to goal-conditional RL (Ren et al.,\nfrontier. The proof of Lemma 3.5 can be found in Appendix\n2019; Ecoffet et al., 2019), which typically involve ran-\nB.2.\ndom exploration from identified frontiers, we advocate for\nthe RL agent to explore novel states to increase the state Lemma 3.5. Given a pre-trained policy π, our MaskNet-\ncoverage. Motivated by this, we adopt Random Network based sampling approach in Section 3.3 is equivalent to\nDistillation (RND) (Burda et al., 2018) which is proved to sampling states from a state occupation distribution induced\nbe an effective exploration bonus, especially in large and by an improved policy πˆ.\ncontinuous state spaces where count-based bonuses (Belle-\nmare et al., 2016; Ostrovski et al., 2017) can be hard In order to answer Q2, we start with presenting Theorem\nto extend. Specifically, we directly utilize the PPO al- 3.6 to bound the sub-optimality via the state distribution\ngorithm to update the policy π, except that we add the mismatch coefficient.\nintrinsic reward to the task reward, that is, we optimize Theorem 3.6. Assume that for the refined policy π′,\nR′(s t , a t ) = R(s t , a t ) + λ|f (s t+1 ) − fˆ(s t+1 )|2, where λ E s∼dπ′ (cid:104) max a Aπ′(s, a) (cid:105) < ϵ. For two initial state dis-\ncontrols the trade-off between the task reward and explo- µ\ntributions µ and ρ, we have the following bound (Kakade &\nration bonus. Along with the policy parameters, the RND\nLangford, 2002)\npredictor network fˆ is updated to regress to the target net-\nwork f . Note that, as the state coverage increases, RND V π∗ (ρ) − V π′ (ρ) ≤ O( ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) ). (3)\nbonuses decay to zero and a performed policy is recovered. (1 − γ)2 (cid:13) (cid:13) dπ ρ ˆ (cid:13) (cid:13)\n∞\nWe present our proposed refining method in Algorithm 2.\n5", "source": "pdfplumber_text", "token": 1562}]}, {"page_num": 6, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nThe proof of Theorem 3.6 can be found in Appendix B.3. fine-tuning” (Schulman et al., 2017), i.e., lowering the learn-\nIt indicates that the upper bound on the difference between ing rate and continuing training with the PPO algorithm.\nthe performance of the optimal policy π∗ and that of the The second baseline is a refining method introduced by\n(cid:13) (cid:13)\npolicy π′ after refining is proportional to\n(cid:13)\n(cid:13)\ndπ\nρ\n∗ (cid:13)\n(cid:13) . With\nStateMask (Cheng et al., 2023), i.e., resetting to the crit-\n(cid:13) dπ ρ ˆ (cid:13) ∞ ical state and continuing training from the critical state.\nAssumption 3.4 and Lemma 3.5, we now claim that our re- The third baseline is Jump-Start Reinforcement Learning\nfining method with our explanation could further tighten the (referred to as “JSRL”) (Uchendu et al., 2023). JSRL in-\nsub-optimality gap via reducing the distribution mismatch troduces a guided policy π to set up a curriculum to train\ng\ncoefficient compared with forming an initial distribution by an exploration policy π . Through initializing π = π , we\ne e g\nrandom selecting visited states, i.e., with a random explana- can transform JSRL to be a refining method that can further\ntion. improve the performance of the guided policy.\nClaim 1. We can form a better (mixed) initial state dis- Evaluation Metrics. To evaluate the fidelity of the gen-\ntribution µ with our explanation method and tighten the erated explanation, we utilize an established fidelity score\nupper bound of V π∗(ρ) − V π′(ρ) compared with random metric defined in StateMask (Cheng et al., 2023). The idea\nexplanation. is to use a sliding window to step through all time steps and\nthen choose the window with the highest average impor-\nThe details of the analysis can be found in Appendix B.4. tance score (scored by the explanation method). The width\nBased on Assumption 3.2 and Claim 1, we can learn to of the sliding window is l while the whole trajectory length\nperform as well as the optimal policy as long as the visited is L. Then we randomize the action(s) at the selected critical\nstates of the optimal policy are covered by the (mixed) initial step(s) in the selected window (i.e., masking) and measure\ndistribution. the average reward change as d. Additionally, we denote\nthe maximum possible reward change as d . Therefore,\nmax\n4. Evaluation the fidelity score is calculated as log(d/d max ) − log(l/L).\nA higher fidelity score indicates higher fidelity.\nIn this section, we start with our experiment setup and de-\nFor the applications with dense rewards except the malware\nsign, followed by experiment results and analysis. We pro-\nmutation application, we measure the reward of the target\nvide additional evaluation details in Appendix C.\nagent before and after refining. In the case of the malware\nmutation application, we report the “final reward” as the\n4.1. Experiment Setup\nprobability of evading the malware detector, both before\nEnvironment Selection. We select eight representative and after refining. For the applications with sparse rewards,\nenvironments to demonstrate the effectiveness of RICE we report the performance during the refining process.\nacross two categories: simulated games (Hopper, Walker2d,\nReacher, and HalfCheetah of the MuJoCo games) and real- 4.2. Experiment Design\nworld applications (selfish mining, network defense, au-\nWe use the following experiments to evaluate the fidelity\ntonomous driving, and malware mutation) 2. We addition-\nand efficiency of the explanation method, the effectiveness\nally run the experiments in the three sparse MuJoCo games\nof the refining method and other factors that influenced the\nintroduced by Mazoure et al. (2019). The details of these\nsystem performance (e.g., alternative design choices, hyper-\napplications can be found in Appendix C.2.\nparameters).\nBaseline Explanation Methods. Since our explanation\nExperiment I. To show the equivalence of our explanation\nmethod proposes an alternative design of StateMask, the first\nmethod with StateMask, we compare the fidelity of our\nbaseline is StateMask. We compare our explanation method\nmethod with StateMask. Given a trajectory, the explanation\nwith StateMask to show the equivalence and efficiency of\nmethod first identifies and ranks top-K important time steps.\nour method. Additionally, we introduce “Random” as a\nAn accurate explanation means the important time steps\nbaseline explanation method. “Random” identifies critical\nhave significant contributions to the final reward. To validate\nsteps by randomly selecting a visited state as the critical\nthis, we let the agent fast-forward to the critical step and\nstate.\nforce the target agent to take random actions. Then we\nBaseline Refining Methods. We compare our refining follow the target agent’s policy to complete the rest of the\nmethod with three baselines. The first baseline is “PPO time steps. If the explanation is accurate, we expect a major\nchange to the final reward by randomizing the actions at\n2These are representative security applications that have a sig-\nthe important steps. We compute the fidelity score of each\nnificant impact on the security community (Anderson et al., 2018)\nexplanation method as mentioned in StateMask across 500\nand they represent RL tasks with sparse rewards, which are com-\nmon in security applications. trajectories. We set K = 10%, 20%, 30%, 40% and report\n6", "source": "pdfplumber_text", "token": 1261}]}, {"page_num": 7, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nthe fidelity of the selected methods under each setup. We tions and provide the full results in Figure 5 of Appendix\nrepeat each experiment 3 times with various random seeds C.3. We observe that the fidelity scores of StateMask and\nand report the mean and standard deviation. Additionally, our method are comparable. Furthermore, We evaluate\nto show the efficiency of our design, we report the training the efficiency of our explanation method compared with\ntime of the mask network using StateMask and our method StateMask. We report the cost time and the number of sam-\nwhen given a fixed number of training samples. ples when training our explanation method and StateMask\nin Table 4 of Appendix C.3. We observe an average of\nExperiment II. To show the effectiveness of the refin-\n16.8% drop in the training time compared with StateMask.\ning method, we compare the agent’s performance after re-\nThe reason is that the training algorithm of the mask net-\nfining using our method and three aforementioned base-\nwork in StateMask involves an estimation of the discounted\nline methods, i.e., PPO fine-tuning (Schulman et al., 2017),\naccumulated reward with respect to the current policy of\nStateMask’s fine-tuning from critical steps (Cheng et al.,\nthe perturbed agent and the policy of the target agent which\n2023), and Jump-Start Reinforcement Learning (Uchendu\nrequires additional computation cost. In contrast, our design\net al., 2023). For this experiment, all the refining meth-\nonly adds an additional term to the reward which is simple\nods use the same explanation generated by our explanation\nbut effective.\nmethod if needed, to ensure a fair comparison. Addition-\nally, we conduct a qualitative study to understand how our Effectiveness of Refining. We compare the agent’s per-\nrefining method influences agent behavior and performance. formance after refining using different retaining methods\nacross all applications with dense rewards in Table 1. The\nExperiment III To investigate how the quality of expla-\nperformance is measured by the final reward of the refined\nnation affects the downstream refining process, we run our\nagent. In most applications, rewards are typically assigned\nproposed refining method based on the critical steps identi-\npositive values. However, in Cage Challenge 2, the reward is\nfied by different explanation methods (Random, StateMask,\ndesigned to incorporate negative values (see Appendix C.2).\nand our method) and compare the agent’s performance after\nWe have three main observations. First, we observe that\nrefining.\nour refining method can bring the largest improvement for\nExperiment IV. To show the versatility of our method, the target agent in all applications. Second, we find that the\nwe examine the refining performance when the pre-trained PPO fine-tuning method only has marginal improvements\nagent was trained by other algorithms such as Soft Actor- for the agents due to its incapability of jumping out of local\nCritic (SAC) (Haarnoja et al., 2018). First, we obtain a optima. Third, the refining method proposed in StateMask\npre-trained SAC agent and then use Generative Adversarial (which is to start fine-tuning only from critical steps) cannot\nImitation Learning (GAIL) (Ho & Ermon, 2016) to learn always improve the agent’s performance. The reason is that\nan approximated policy network. We compare the refining this refining strategy can cause overfitting and thus harm the\nperformance using our method against baseline methods, agent’s performance. We illustrate this problem in greater\ni.e., PPO fine-tuning (Schulman et al., 2017), StateMask’s detail in a case study of Malware Mutation in Appendix D.\nfine-tuning from critical steps (Cheng et al., 2023), and It is also worth mentioning that we discover design flows of\nJump-Start Reinforcement Learning (Uchendu et al., 2023). Malware Mutation and present the details in Appendix D.\nIn addition, we also include fine-tuning the pre-trained SAC\nWe also run our experiments of varying refining methods\nagent with the SAC algorithm as a baseline.\non selected MuJoCo games with sparse rewards. Figure 2\nExperiment V. We test the impact of hyper-parameter shows the results of our method against other baselines in\nchoices for two primary hyper-parameters for refining: p SparseHopper and SparseHalfCheetah games. We observe\n(used to control the mixed initial state distribution) and λ that our refining method has significant advantages over\n(used to control the exploration bonus). For our refining other baselines with respect to final performance and refin-\nmethod, we vary p from {0, 0.25, 0.5, 0.75, 1} and vary λ ing efficiency. Through varying explanation methods, we\nfrom {0, 0.1, 0.01, 0.001}. By examining the agent’s per- confirm that the contribution should owe to our explana-\nformance with various λ values, we can further investigate tion method. We leave the refining results of the Sparse-\nthe necessity of the exploration bonus. Additionally, we Walker2d game and the hyper-parameter sensitivity results\nevaluate the choice of α for our explanation method (used to of all sparse MuJoCo games in Appendix C.4.\ncontrol the mask ratio for the mask network). Specifically, In addition to numerical results, we also provide a qualita-\nwe vary α from {0.01, 0.001, 0.0001}. tive analysis of the autonomous driving case to understand\nhow RICE influences agent behavior and performance, par-\n4.3. Experiment Results ticularly in a critical state, in Appendix C.5. We visualize\nthe agent’s behavior before and after refining the agent to\nFidelity and Efficiency of Explanation. We compare the\nshow that RICE is able to help the agent break through\nfidelity scores of our method with StateMask in all applica-\n7", "source": "pdfplumber_text", "token": 1316}]}, {"page_num": 8, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nTable 1. Agent Refining Performance—“No Refine” indicates the target agent’s performance before refining. For the first group of\nexperiments (left), we fixed the explanation method to our method (mask network) and varied the refining methods. For the second\ngroup of experiments (right), we fixed the refining method to our method and varied the explanation methods. We report the mean value\n(standard deviations) of the final reward after refining. A higher value is better.\nFix Explanation; Vary Refine Methods Fix Refine; Vary Explanation Methods\nTask No Refine\nPPO JSRL StateMask-R Ours Random StateMask Ours\nHopper 3559.44 (19.15) 3638.75 (16.67) 3635.08 (9.82) 3652.06 (8.63) 3663.91 (20.98) 3648.98 (39.06) 3661.86 (19.95) 3663.91 (20.98)\nWalker2d 3768.79 (18.68) 3965.63 (9.46) 3963.57 (6.73) 3966.96 (3.39) 3982.79 (3.15) 3969.64(6.38) 3982.67 (5.55) 3982.79 (3.15)\nReacher -5.79 (0.73) -3.04 (0.04) -3.23 (0.26) -3.45 (0.32) -2.66 (0.03) -3.11 (0.42) -2.69 (0.28) -2.66 (0.03)\nHalfCheetah 2024.09 (28.34) 2133.31 (4.11) 2128.04 (0.91) 2085.28 (1.92) 2138.89 (3.22) 2132.01 (0.76) 2136.23 (0.49) 2138.89 (3.22)\nSelfish Mining 14.36 (0.24) 14.93 (0.45) 14.88 (0.51) 14.53 (0.33) 16.56 (0.63) 15.09 (0.28) 16.49 (0.46) 16.56 (0.63)\nCage Challenge 2 -23.64 (0.27) -23.58 (0.37) -22.97 (0.57) -26.98 (0.84) -20.02 (0.32) -25.94 (2.34) -20.07 (1.33) -20.02 (0.32)\nAuto Driving 10.30 (2.25) 13.37 (3.10) 11.26 (3.66) 7.62 (1.77) 17.03 (1.65) 11.72 (1.77) 16.28 (2.33) 17.03 (1.65)\nMalware Mutation 42.20 (6.86) 49.33 (8.59) 43.10 (7.24) 50.13 (8.14) 57.53 (8.71) 48.60 (7.60) 57.16 (8.51) 57.53 (8.71)\nSparseHopper SparseHalfCheetah SparseHopper SparseHalfCheetah\n(a) Fix Explanation; Vary Refine (b) Fix Refine; Vary Explanation\nFigure 2. Agent Refining Performance in two Sparse MuJoCo Games—For Group (a), we fix the explanation method to our method\n(mask network) if needed while varying refining methods. For Group (b), we fix the refining method to our method while varying the\nexplanation methods.\nthe bottleneck based on the identified critical states of the\nfailure.\nRefining based on Different Explanations. To examine\nhow the quality of explanation affects the downstream re-\nfining process, we present Table 1. We run our proposed\nrefining method based on the critical steps identified by\nours and Random. We have two main observations. First, Pre-train the policy(1M steps) Refine the policy(1M steps)\nusing the explanation generated by our mask network, the\nFigure 3. SAC Agent Refining Performance in Hopper Game\nrefining achieves the best outcome across all applications.\n—In the left part, we show the training curve of obtaining a pre-\nSecond, using the explanation generated by our explanation\ntrained policy through the SAC algorithm. In the right part, we\nsignificantly outperforms the random baseline. This aligns\nshow the refining curves of different methods.\nwith our theoretical analysis that our refining framework\nprovides a tighter bound for the sub-optimality.\nAppendix C.3. We have three main observations.\nRefining a Pre-trained Agent of Other Algorithms. To\nFirst, p controls the mixing ratio of critical states (identified\nshow that our framework is general to refine pre-trained\nby the explanation method) and the initial state distribution\nagents that were not trained by PPO algorithms, we do ex-\nfor refining. The performance is low when p = 0 (all\nperiments on refining a SAC agent in the Hopper game.\nstarting from the default initial distribution) or p = 1 (all\nFigure 3 demonstrates the advantage of our refining method\nstarting from the identified critical states). The performance\nagainst other baselines when refining a SAC agent. Addi-\nhas significant improvements when 0 < p < 1, i.e., using a\ntionally, we observe that fine-tuning the DRL agent with\nmixed initial state distribution. Across all applications, we\nthe SAC algorithm still suffers from the training bottleneck\nobserve that setting p to 0.25 or 0.5 is most beneficial. A\nwhile switching to the PPO algorithm provides an opportu-\nmixed initial distribution can help eliminate the problem of\nnity to break through the bottleneck. We provide the refining\ncurves when varying hyper-parameters p and λ in Appendix overfitting.\nC.3. Second, as long as λ > 0 (thereby enabling exploration),\nthere is a noticeable improvement in performance, highlight-\nImpact of Hyper-parameters. Due to space limit, we\ning the importance of exploration in refining the pre-trained\nprovide the sensitivity of hyper-parameters p, λ, and α in\nagent. The result is less sensitive to the specific value of\n8", "source": "pdfplumber_text", "token": 1656}, {"type": "table", "text": "\n[Detected Table]\n| NoRefine | FixExplanation;VaryRefineMethods |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- |\n|  | PPO | JSRL | StateMask-R | Ours | Random | StateMask |\n| 3559.44(19.15) | 3638.75(16.67) | 3635.08(9.82) | 3652.06(8.63) | 3663.91(20.98) | 3648.98(39.06) | 3661.86(19.95) |\n| 3768.79(18.68) | 3965.63(9.46) | 3963.57(6.73) | 3966.96(3.39) | 3982.79(3.15) | 3969.64(6.38) | 3982.67(5.55) |\n| -5.79(0.73) | -3.04(0.04) | -3.23(0.26) | -3.45(0.32) | -2.66(0.03) | -3.11(0.42) | -2.69(0.28) |\n| 2024.09(28.34) | 2133.31(4.11) | 2128.04(0.91) | 2085.28(1.92) | 2138.89(3.22) | 2132.01(0.76) | 2136.23(0.49) |\n| 14.36(0.24) | 14.93(0.45) | 14.88(0.51) | 14.53(0.33) | 16.56(0.63) | 15.09(0.28) | 16.49(0.46) |\n| -23.64(0.27) | -23.58(0.37) | -22.97(0.57) | -26.98(0.84) | -20.02(0.32) | -25.94(2.34) | -20.07(1.33) |\n| 10.30(2.25) | 13.37(3.10) | 11.26(3.66) | 7.62(1.77) | 17.03(1.65) | 11.72(1.77) | 16.28(2.33) |\n| 42.20(6.86) | 49.33(8.59) | 43.10(7.24) | 50.13(8.14) | 57.53(8.71) | 48.60(7.60) | 57.16(8.51) |\n\n", "source": "pdfplumber_table", "token": 843}, {"type": "table", "text": "\n[Detected Table]\n| SparseHopper |  |  | SparseHopper |  |  |\n| --- | --- | --- | --- | --- | --- |\n\n", "source": "pdfplumber_table", "token": 36}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  |  |  |  |  |\n\n", "source": "pdfplumber_table", "token": 69}]}, {"page_num": 9, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nλ. In general, a λ value of 0.01 yields good performance 6. Conclusion\nacross all four applications.\nIn this paper, we present RICE to break through bottle-\nThird, recall that the hyper-parameter α is to control the necks of reinforcement learning training with explanation.\nbonus of blinding the target agent when training the mask We propose an alternative design of StateMask to provide\nnetwork. We vary α from {0.01, 0.001, 0.0001} and find high-fidelity explanations for DRL agents’ behaviors, by\nthat our explanation method is not that sensitive to α. identifying critical time steps that contribute the most to\nthe agent’s success/failure. We encourage the agent to ex-\n5. Discussion plore starting from a mixture of default initial states and the\nidentified critical states. Compared with existing refining\nApplicability. RICE is suitable for DRL applica- strategies, we empirically show that our method brings the\ntions that are trained within controllable environment largest improvement after refining with theoretical guaran-\n(e.g., simulators), in order to generate explanations. In fact, tees.\nmost of today’s DRL applications rely on some form of\nsimulator for their training. For example, for safety-critical\nAcknowledgements\napplications such as autonomous driving, DRL agents are\nusually designed, trained, and tested in a simulated environ- This project was supported in part by Northwestern Univer-\nment first before moving them to real-world testing. Simu- sity TGS Fellowship and NSF Grants 2225234, 2225225,\nlation platforms broadly include Carla (Dosovitskiy et al., 2229876, 1955719, and 2055233.\n2017) and MetaDrive (Li et al., 2022) which have been used\nto facilitate the training of DRL agents (Zhang et al., 2021;\nImpact Statement\nWang et al., 2023; Peng et al., 2022). Therefore, RICE\nshould be applicable to such DRL systems (especially dur- This paper presents work whose goal is to advance the field\ning their development phase) for refining a pre-trained DRL of reinforcement learning with explanation. There are many\nagent. potential social impacts of our work. Our approach provides\na feasible solution to break through the training bottlenecks\nWarm Start vs Cold Start. As is mentioned in Section 3,\nof reinforcement learning with explanation, which is an\nour method requires a “warm start” setting, i.e., the agent\nautomatic process and saves manual effort.\nhas good coverage of the state distribution of the optimal\npolicy. Even if the agent has good coverage of the state However, it is also worth noting the potential negative soci-\ndistribution, it does not necessarily mean that the agent has etal impacts of our work. Some of the real-world applica-\nalready learned a good policy due to the potential of choos- tions we select such as malware mutation can create attack\ning wrong actions (Uchendu et al., 2023). Therefore, the examples that may bring additional ethical concerns. In the\ntraining bottleneck can still exist under a good coverage of realm of security research, the ultimate goal of these tasks\nthe state distribution. In contrast, Our method does not work is to generate stronger testing cases to enhance the defense,\nwell in a “cold start” setting, i.e., when the state coverage and it is standard practice. Take malware mutation as an\nof the pre-trained policy is extremely poor. In that case, example, the produced samples can be used to proactively\nstep-level explanation methods cannot give useful help and improve the robustness and effectiveness of malware de-\nour method is actually equivalent to the RND method 3. tection systems (e.g., through adversarial training), thereby\nbenefiting cybersecurity defense (Yang et al., 2017).\nCritical State Filtering. Though RICE identifies critical\nstates based on their necessity for achieving good outcomes,\nReferences\nit does not fully consider their importance for further agent\nlearning. For instance, a state might be deemed critical,\nGitHub - bfilar/malware rl: Malware Bypass Research us-\nyet the trained agent could have already converged to the ing Reinforcement Learning — github.com. https:\noptimal action for that state. In such cases, resetting the //github.com/bfilar/malware_rl, a.\nenvironment to this state doesn’t significantly benefit the\nlearning process. Future work could explore additional\nfiltering of critical states using metrics such as policy con- GitHub - cage-challenge/cage-challenge-2: TTCP CAGE\nvergence or temporal difference (TD) errors, which may Challenge 2 — github.com. https://github.com/\nhelp concentrate efforts and accelerate refinement. cage-challenge/cage-challenge-2, b.\n3We provide an example of Mountain Car game in Appendix E\nto illustrate this limitation. GitHub - john-cardiff/-cyborg-cage-2 — github.com.\nhttps://github.com/john-cardiff/\n-cyborg-cage-2, c.\n9", "source": "pdfplumber_text", "token": 1128}]}, {"page_num": 10, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nGitHub - roibarzur/pto-selfish-mining: Code repository Cheng, Z., Wu, X., Yu, J., Sun, W., Guo, W., and Xing,\nfor technical papers about selfish mining analysis. — X. Statemask: Explaining deep reinforcement learning\ngithub.com. https://github.com/roibarzur/ through state mask. In Proc. of NeurIPS, 2023.\npto-selfish-mining, d.\nDosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and\nMountain car continuous. https://mgoulao. Koltun, V. CARLA: An open urban driving simulator. In\ngithub.io/gym-docs/environments/ Proc. of CoRL, pp. 1–16, 2017.\nclassic_control/mountain_car_\ncontinuous/. Accessed: 2024-05-24. drive Contributors, D. DI-drive: OpenDILab deci-\nsion intelligence platform for autonomous driving\nAgarwal, A., Henaff, M., Kakade, S., and Sun, W. Pc-\nsimulation. https://github.com/opendilab/\npg: Policy cover directed exploration for provable policy\nDI-drive, 2021.\ngradient learning. Proc. of NeurIPS, 2020.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O.,\nAgarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G.\nand Clune, J. Go-explore: a new approach for hard-\nOn the theory of policy gradient methods: Optimality,\nexploration problems. arXiv preprint arXiv:1901.10995,\napproximation, and distribution shift. Journal of Machine\n2019.\nLearning Research, 2021.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C.,\nClune, J. First return, then explore. Nature, 2021.\nand Bellemare, M. Reincarnating reinforcement learning:\nReusing prior computation to accelerate progress. In Proc.\nErez, T., Tassa, Y., and Todorov, E. Infinite-horizon\nof NeurIPS, 2022.\nmodel predictive control for periodic tasks with contacts.\nAnderson, H. S., Kharkar, A., Filar, B., Evans, D., and Roth, Robotics: Science and systems VII, pp. 73, 2012.\nP. Learning to evade static pe machine learning mal-\nware models via reinforcement learning. arXiv preprint Eyal, I. and Sirer, E. G. Majority is not enough: Bitcoin\narXiv:1801.08917, 2018. mining is vulnerable. Communications of the ACM, 61\n(7):95–102, 2018.\nBar-Zur, R., Abu-Hanna, A., Eyal, I., and Tamar, A. Werl-\nman: To tackle whale (transactions), go deep (rl). In Proc. Eysenbach, B., Salakhutdinov, R., and Levine, S. The infor-\nof IEEE S&P, 2023. mation geometry of unsupervised reinforcement learning.\nIn Proc. of ICLR, 2021.\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T.,\nSaxton, D., and Munos, R. Unifying count-based ex- Fu, J., Luo, K., and Levine, S. Learning robust rewards with\nploration and intrinsic motivation. In Proc. of NeurIPS, adverserial inverse reinforcement learning. In Proc. of\n2016. ICLR, 2018.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Explo-\nGuan, L., Verma, M., Guo, S. S., Zhang, R., and Kambham-\nration by random network distillation. In Proc. of ICLR,\npati, S. Widening the pipeline in human-guided reinforce-\n2018.\nment learning with explanation and context-aware data\naugmentation. In Proc.of NeurIPS, 2021.\nCAGE. Ttcp cage challenge 2. In Proc. of AAAI-22 Work-\nshop on Artificial Intelligence for Cyber Security (AICS),\nGuo, W., Wu, X., Khan, U., and Xing, X. Edge: Explaining\n2022.\ndeep reinforcement learning policies. In Proc. of NeurIPS,\nCai, X.-Q., Ding, Y.-X., Chen, Z., Jiang, Y., Sugiyama, 2021.\nM., and Zhou, Z.-H. Seeing differently, acting similarly:\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-\nHeterogeneously observable imitation learning. In Proc.\ncritic: Off-policy maximum entropy deep reinforcement\nof ICLR, 2022.\nlearning with a stochastic actor. In Proc. of ICML, pp.\nCai, X.-Q., Zhang, Y.-J., Chiang, C.-K., and Sugiyama, 1861–1870, 2018.\nM. Imitation learning from vague feedback. In Proc. of\nNeurIPS, 2023. Ho, J. and Ermon, S. Generative adversarial imitation learn-\ning. In Proc. of NeurIPS, 2016.\nChang, J. D., Brantley, K., Ramamurthy, R., Misra, D., and\nSun, W. Learning to generate better than your llm. arXiv Kakade, S. and Langford, J. Approximately optimal approx-\npreprint arXiv:2306.11816, 2023. imate reinforcement learning. In Proc. of ICML, 2002.\n10", "source": "pdfplumber_text", "token": 1410}]}, {"page_num": 11, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nLi, Q., Peng, Z., Feng, L., Zhang, Q., Xue, Z., and Zhou, Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nB. Metadrive: Composing diverse driving scenarios for Klimov, O. Proximal policy optimization algorithms.\ngeneralizable reinforcement learning. IEEE Transactions arXiv preprint arXiv:1707.06347, 2017.\non Pattern Analysis and Machine Intelligence, 2022.\nSundararajan, M., Taly, A., and Yan, Q. Axiomatic attribu-\nLi, Q., Zhai, Y., Ma, Y., and Levine, S. Understanding the tion for deep networks. In Proc. of ICML, 2017.\ncomplexity gains of single-task rl with a curriculum. In\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics\nProc. of ICML, pp. 20412–20451, 2023.\nengine for model-based control. In Proc. of IROS, 2012.\nMazoure, B., Doan, T., Durand, A., Hjelm, R. D., and\nUchendu, I., Xiao, T., Lu, Y., Zhu, B., Yan, M., Simon, J.,\nPineau, J. Leveraging exploration in off-policy algorithms\nBennice, M., Fu, C., Ma, C., Jiao, J., et al. Jump-start\nvia normalizing flows. In Proc. of CoRL, 2019.\nreinforcement learning. In Proc. of ICML, 2023.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nVan Waveren, S., Pek, C., Tumova, J., and Leite, I. Correct\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint me if i’m wrong: Using non-experts to repair reinforce-\narXiv:1312.5602, 2013. ment learning policies. In Proc. of HRI, 2022.\nOh, J., Chockalingam, V., Lee, H., et al. Control of memory, Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,\nactive perception, and action in minecraft. In Proc. of Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,\nICML, 2016. T., Georgiev, P., et al. Grandmaster level in starcraft ii\nusing multi-agent reinforcement learning. Nature, 2019.\nOh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learn-\ning. In Proc. of ICML, pp. 3878–3887, 2018. Vyas, S., Hannay, J., Bolton, A., and Burnap, P. P. Au-\ntomated cyber defence: A review. arXiv preprint\nOstrovski, G., Bellemare, M. G., Oord, A., and Munos, R. arXiv:2303.04926, 2023.\nCount-based exploration with neural density models. In\nProc. of ICML, 2017. Wang, X., Zhang, J., Hou, D., and Cheng, Y. Autonomous\ndriving based on approximate safe action. IEEE Transac-\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., tions on Intelligent Transportation Systems, 2023.\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance Weng, J., Chen, H., Yan, D., You, K., Duburcq, A., Zhang,\ndeep learning library. In Proc. of NeurIPS, 2019. M., Su, Y., Su, H., and Zhu, J. Tianshou: A highly\nmodularized deep reinforcement learning library. Journal\nPeng, Z., Li, Q., Liu, C., and Zhou, B. Safe driving via of Machine Learning Research, 2022.\nexpert guided policy optimization. In Proc. of CoRL,\n2022. Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. Policy\nfinetuning: Bridging sample-efficient offline and online\nPuri, N., Verma, S., Gupta, P., Kayastha, D., Deshmukh, S., reinforcement learning. In Proc. of NeurIPS, 2021.\nKrishnamurthy, B., and Singh, S. Explain your move:\nUnderstanding agent actions using specific and relevant Yang, W., Kong, D., Xie, T., and Gunter, C. A. Malware\nfeature attribution. In Proc. of ICLR, 2019. detection in adversarial settings: Exploiting feature evolu-\ntions and confusions in android apps. In Proc. of ACSAC,\nRaff, E., Barker, J., Sylvester, J., Brandon, R., Catanzaro, 2017.\nB., and Nicholas, C. Malware detection by eating a whole\nexe. arXiv preprint arXiv:1710.09435, 2017. Yu, J., Guo, W., Qin, Q., Wang, G., Wang, T., and Xing, X.\nAirs: Explanation for deep reinforcement learning based\nRaffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, security applications. In Proc. of USENIX Security, 2023.\nM., and Dormann, N. Stable-baselines3: Reliable rein-\nforcement learning implementations. Journal of Machine Zhang, Z., Liniger, A., Dai, D., Yu, F., and Van Gool, L.\nLearning Research, 2021. End-to-end urban driving by imitating a reinforcement\nlearning coach. In Proc. of ICCV, 2021.\nRashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell,\nS. Bridging offline reinforcement learning and imitation\nlearning: A tale of pessimism. In Proc. of NeurIPS, 2021.\nRen, Z., Dong, K., Zhou, Y., Liu, Q., and Peng, J. Explo-\nration via hindsight goal generation. In Proc. of NeurIPS,\n2019.\n11", "source": "pdfplumber_text", "token": 1566}]}, {"page_num": 12, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nA. Proof of Theorem 3.3\nBased on the Performance Difference Lemma (Kakade & Langford, 2002), we have\n1\nη(π¯) − η(π) = E E Aπ (s, a) , (4)\n1 − γ s∼dπ ρ ¯ a∼π¯(·|s)\nwhere π is the policy of the target agent, π¯ is the perturbed policy, ρ is the initial distribution, and γ is the discount rate.\nNote that the perturbed policy π¯ is a mixture of the target agent’s policy π and a random policy πr (i.e., π¯(·|s) = π˜(ae =\n0|s)π(·|s) + π˜(ae = 1|s)πr(·|s)). Denote the probability of the mask network outputting 0 at state s as π˜(ae = 0|s) =\nξ(s) and the probability of the mask network outputting 1 at state s as π˜(ae = 1|s) = 1 − ξ(s) Given the fact that\nAπ (s, π(·|s)) = E Aπ(s, a) = 0, we have\na∼π(s)\n1\nη(π¯) − η(π) = E E Aπ (s, a)\n1 − γ s∼dπ ρ ¯ a∼π¯(·|s)\n1 (cid:88)\n= E π¯(a|s)Aπ (s, a)\n1 − γ\ns∼dπ\nρ\n¯\na\n1 (cid:88) 1 (cid:88)\n= E ξ(s)π(a|s)Aπ (s, a) + E (1 − ξ(s))πr(a|s)Aπ (s, a) (5)\n1 − γ\ns∼dπ\nρ\n¯\n1 − γ\ns∼dπ\nρ\n¯\na a\n1 1\n= E ξ(s)E Aπ (s, a) + E (1 − ξ(s))E Aπ (s, a)\n1 − γ s∼dπ ρ ¯ a∼π(·|s) 1 − γ s∼dπ ρ ¯ a∼πr(·|s)\n1\n= E (1 − ξ(s))E Aπ (s, a) ≤ 0.\n1 − γ\ns∼dπ\nρ\n¯ a∼πr(·|s)\nTherefore, we show that η(π¯) is upper bounded by η(π) given Assumption 3.1.\nB. Theoretical Guarantee\nB.1. Validation of Assumption 3.4 in a 2-state MDP\nIn a 2-state MDP, we have two different states, namely, s and s . The state distribution of any policy π follows\nA B\ndπ(s ) + dπ(s ) = 1. As such, the set of feasible state marginal distribution can be described by a line [(0, 1), (1, 0)]\nρ A ρ B\nin R2. Let’s denote vector s = [s , s ]. The expected total reward of a policy π can be represented as η(π) =<\nA B\ndπ(s), R(s) > (Eysenbach et al., 2021), where R(s) = [R(s ), R(s )]. Figure 4 shows the area of achievable state\nρ A B\ndistribution via the initial state distribution ρ (highlighted in orange).\nIt should be noted that not all the points in the line [(0, 1), (1, 0)] corresponded to a valid Markovian policy. However, for\nany convex combination of valid state occupancy measures, there exists a Markovian policy that has this state occupancy\nmeasure. As such, the policy search occurs within a convex polytope, essentially a segment (i.e., , marked in orange) along\nthis line. In Figure 4, we visualize R(s) as vectors starting at the origin. Since V πˆ(ρ) ≥ V π(ρ), We mark dπˆ(s) closer to\nρ\nR(s) (i.e., the inner product between dπˆ(s) and R(s) and is larger than dπ(s) and R(s)). The following theorem explains\nρ ρ\nhow we determine the location of the location of dπ∗(s) in Figure 4.\nρ\nTheorem B.1 (Fact 1 (Eysenbach et al., 2021)). For every state-dependent reward function, among the set of policies that\nmaximize that reward function is one that lies at a vertex of the state marginal polytope.\nAccording to Theorem B.1, dπ∗(s) located at either vertex in the orange segment. Since π∗ is the optimal policy, it lies\nρ\nat the vertex that has the larger inner product within R(s). Once the position of dπ∗(s) is determined, we can easily find\nρ\n(cid:13) (cid:13) (cid:13) (cid:13)\n(cid:13) dπ∗ (s) (cid:13) (cid:13) dπ∗ (s) (cid:13)\n(cid:13) ρ (cid:13) ≤ (cid:13) ρ (cid:13) based on Figure 4.\n(cid:13) dπ ρ ˆ(s) (cid:13) ∞ (cid:13) dπ ρ (s) (cid:13) ∞\nB.2. Proof of Lemma 3.5\nProof. Since our explanation method provides the importance of each state, we could view the sampling based on the state’s\nimportance as a reweighting of the state occupancy measure. Mathematically, it can be expressed as dπˆ(s) = dπ(s)w(s),\nρ ρ\n12", "source": "pdfplumber_text", "token": 1354}]}, {"page_num": 13, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n𝑑𝑑(𝑠𝑠𝐵𝐵)\n1\n𝜋𝜋\n𝑑𝑑𝜌𝜌 (𝒔𝒔)\n𝜋𝜋�\n𝑑𝑑𝜌𝜌 (𝒔𝒔)∗\n𝜋𝜋\n𝑑𝑑𝜌𝜌 (𝒔𝒔)\n𝑅𝑅(𝒔𝒔)\n0 1 𝑑𝑑(𝑠𝑠𝐴𝐴)\nFigure 4. Visualization of state occupancy measures with respect to different policies and the reward function in a 2-state MDP.\nwhere πˆ is the equivalent policy of reweighting the original policy π and w(s) is the weight provided by the mask network.\nAlthough the mask network takes the current input state as input, it implicitly considers the current action as well, as detailed\nby StateMask (Cheng et al., 2023). Consequently, a more accurate formulation is dπˆ(s, a) = dπ(s, a)w(s, a), where w(s, a)\nρ ρ\nrepresent the weight assigned by mask network.\nRecall that our proposed explanation method is to randomize actions at non-critical steps, which essentially considers the\nvalue of Q = Qπ(s, a) − E [Qπ(s, a′)]. In fact, a larger Q indicates current time step is more critical to the\ndiff a′∈A diff\nagent’s final reward. Our mask network approximates the value of Q via the deep neural network to determine the\ndiff\nimportance of each step, which implies w(s, a) ∝ Q ∝ Qπ(s, a).\ndiff\nNext, we aim to prove that our MaskNet-based sampling approach is equivalent to sampling from a better policy πˆ.\nFirst, the equivalent policy πˆ after reweighting can be expressed as\ndπˆ(s, a) dπ(s, a)w(s, a) dπ(s)\nπˆ(a|s) = ρ = ρ = w(s, a)π(a|s) ρ . (6)\ndπˆ(s) dπˆ(s) dπˆ(s)\nρ ρ ρ\nFurther , we would like to show that if w(s, a) = f (Qπ(s, a)) where f (·) is a monotonic increasing function, πˆ is uniformly\nas good as,or better than π, i.e., V πˆ(s) ≥ V π(s).\nProposition B.2. Suppose two policies πˆ and π satisfy g (πˆ(a|s)) = g(π(a|s)) + h (s, Qπ(s, a)) where g(·) is a monotoni-\ncally increasing function, and h(s, ·) is monotonically increasing for any fixed s . Then we have V πˆ(s) ≥ V π(s), ∀s ∈ S.\nProof. For a given s, we partition the action set A into two subsets A and A .\n1 2\nA ≜ {a ∈ A|πˆ(a|s) ⩾ π(a|s)} .\n1\nA ≜ {a ∈ A|πˆ(a|s) < π(a|s)} .\n2\nThus, ∀a ∈ A , ∀a ∈ A , we have\n1 1 2 2\nh (s, Qπ(s, a )) = g (πˆ(a |s)) − g(π(a |s))\n1 1 1\n≥ 0\n(7)\n≥ g (πˆ(a |s)) − g(π(a |s))\n2 2\n= h (s, Qπ(s, a )) .\n2\nLet h (s, Qπ(s, a)) = Qπ(s, a). We can get Qπ(s, a ) ≥ Qπ(s, a ) which means we can always find q(s) ∈ R such that\n1 2\n13", "source": "pdfplumber_text", "token": 830}]}, {"page_num": 14, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nQπ(s, a ) ≥ q(s) ≥ Qπ(s, a ), ∀a ∈ A , ∀a ∈ A . Thus,\n1 2 1 1 2 2\n(cid:88) (cid:88)\nπˆ(a|s)Qπ(s, a) − π(a|s)Qπ(s, a)\na∈A a∈A\n(cid:88) (cid:88)\n= (πˆ (a |s) − π(a |s)) Qπ (s, a ) + (πˆ (a |s) − π(a |s)) Qπ(s, a )\n1 1 1 2 2 2\na1∈A1 a2∈A2\n(cid:88) (cid:88)\n≥ (πˆ (a |s) − π (a |s)) q(s) + (πˆ (a |s) − π (a |s)) q(s) (8)\n1 1 2 2\na1∈A1 a2∈A2\n(cid:88)\n= q(s) (π′(a|s) − π(a|s))\na∈A\n= 0.\nLet V (s) = V π(s). And we denote the value function of following πˆ for l steps then following π as V (s) =\n0 l\nE (cid:2) E (r + γV (s′)) (cid:3) if l ≥ 1.\na∼πˆ(.|s) s′,r|s,a l−1\nFirst, we observe that\nV (s) = E (cid:2) E (r + γV π(s′))]\n1 a∼πˆ(.|s) s′,r|s,a\n(cid:88)\n= πˆ(a|s)Qπ(s, a)\na∈A\n(9)\n(cid:88)\n⩾ π(a|s)Qπ(s, a)\na∈A\n= V (s).\n0\nBy induction, we assume V (s) ≥ V (s). Given that\nl l−1\nV (s) = E (cid:2) E (r + V (s′)) (cid:3) ,\nl+1 a∼πˆ s′,r|s,a l\nV (s) = E (cid:2) E (r + V (s′)) (cid:3) ,\nl a∼πˆ s′,r|s,a l−1\nwe have V (s) ≥ V (s).\nl+1 l\nTherefore, we can conclude that V (s) ≥ V (s), ∀l ≥ 0. We have V (s) ≥ V (s) which is V πˆ(s) ≥ V π(s).\nl+1 l ∞ 0\nBased on the Proposition B.2, if we choose g as a logarithmic function and h = log(w(s, a)) + log(dπ(s)) − log(dπˆ(s)),\nρ ρ\nwe can easily verify that our MaskNet-based sampling approach is equivalent to sampling from a better policy πˆ.\nB.3. Proof of Theorem 3.6\nProof. Given the fact that the refined policy π′ is converged, (i.e., the local one-step improvement is small\n(cid:104) (cid:105)\nE max Aπ′(s, a) < ϵ), we have\ns∼dπ′ a\nµ\nϵ >\n(cid:88) dπ′\n(s)\n(cid:104)\nmax\nAπ′\n(s, a)\n(cid:105)\nµ\na\ns∈S\n≥ min\n(cid:32) dπ\nµ\n′(s) (cid:33)\n(cid:88) dπ∗ (s) max Aπ′ (s, a)\ns dπ∗(s) ρ a (10)\nρ s\n≥ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) −1 (cid:88) dπ∗ (s)π∗(a|s)Aπ′ (s, a).\n(cid:13) dπ′ (cid:13) ρ\n(cid:13) µ (cid:13) ∞ s,a\nBased on the Performance Difference Lemma (Kakade & Langford, 2002), for two policies π∗, π′ and a state distribution ρ,\nthe performance difference is bounded by\n1 (cid:104) (cid:105)\nV\nπ∗\n(ρ) − V\nπ′\n(ρ) = E E\nAπ′\n(s, a) . (11)\n1 − γ\ns∼dπ\nρ\n∗ a∼π∗(.|s)\n14", "source": "pdfplumber_text", "token": 1099}]}, {"page_num": 15, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nThen we have\n(cid:13) (cid:13) dπ∗ (cid:13) (cid:13) −1 (cid:16) (cid:17)\nϵ > (1 − γ) (cid:13) ρ (cid:13) V π (ρ) − V π′ (ρ) . (12)\n(cid:13) dπ′ (cid:13)\n(cid:13) µ (cid:13)\n∞\nTherefore, we have\nV π∗ (ρ) − V π′ (ρ) ≤ ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) . (13)\n1 − γ (cid:13) dπ′ (cid:13)\n(cid:13) µ (cid:13)\n∞\nDue to dπ′(s) ≥ (1 − γ)µ(s), we further obtain\nµ\nV π∗ (ρ) − V π′ (ρ) ≤ ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) . (14)\n(1 − γ)2 (cid:13) µ (cid:13)\n(cid:13) (cid:13)\n∞\nSince µ(s) = βdπˆ(s) + (1 − β)ρ(s) ≥ βdπˆ(s), we have\nρ ρ\nV π∗ (ρ) − V π′ (ρ) ≤ ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) . (15)\n(1 − γ)2 (cid:13) βdπˆ (cid:13)\n(cid:13) ρ (cid:13)\n∞\nIn our case, β is a constant (i.e., a hyper-parameter), thus we could derive that\nV π∗ (ρ) − V π′ (ρ) ≤ O( ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) ), (16)\n(1 − γ)2 (cid:13) dπˆ (cid:13)\n(cid:13) ρ (cid:13)\n∞\nwhich completes the proof.\nB.4. Analysis of Claim 1\nRecall that Lemma 3.5 indicates that our MaskNet-based sampling approach is equivalent to sampling states from a better\npolicy πˆ compared with a random explanation sampling from the policy π, i.e., η(πˆ) ≥ η(π). Let us denote the new initial\n(cid:13) (cid:13) (cid:13) (cid:13)\n(cid:13) dπ∗ (cid:13) (cid:13) dπ∗ (cid:13)\ndistribution using our MaskNet-based sampling approach as µ. By Assumption 3.4, we have (cid:13) ρ (cid:13) ≤ (cid:13) ρ (cid:13) . Using\n(cid:13) dπ ρ ˆ (cid:13) ∞ (cid:13) dπ ρ (cid:13) ∞\nour explanation method introduces a smaller distribution mismatch coefficient than using a random explanation method.\nTherefore, we claim that using our explanation method, we are able to form a better initial distribution µ and tighten the\nupper bound in Theorem 3.6, i.e., enhancing the agent’s performance after refining.\nC. Details of Evaluation\nC.1. Implementation Details\nImplementation of Our Method. We implement the proposed method using PyTorch (Paszke et al., 2019). We implement\nour method in four selected MuJoCo games based on Stable-Baselines3 (Raffin et al., 2021). We train the agents on a server\nwith 8 NVIDIA A100 GPUs for all the learning algorithms. For all our experiments, if not otherwise stated, we use a set of\ndefault hyper-parameters for p, λ, and α (listed in Appendix C.3).\nWe implement the environment reset function similar to Ecoffet et al. (2019) to restore the environment to selected critical\nstates. This method is feasible in our case, as we operate within simulator-based environments. However, in the real world,\nit may not be always possible to return to a certain state with the same sequences of actions due to the stochastic nature\nof state transition. It’s important to note that our framework is designed to be versatile and is indeed compatible with a\ngoal/state-conditioned policy approach such as Ecoffet et al. (2021). Given a trajectory with an identified most important\nstate, we can select the most important state as the final goal and select the en-route intermediate states as sub-goals. Then\nwe can train an agent to reach the final goal by augmenting each state with the next goal and giving a goal-conditioned\nreward once the next goal is reached until all goals are achieved.\nImplementation of Baseline Methods. Regarding baseline approaches, we use the code released by the authors or\nimplement our own version if the authors don’t release the code. Specifically, as for StateMask, we use their released open-\nsourced code from https://github.com/nuwuxian/RL-state_mask. Regarding Jump-Start Reinforcement\nLearning, we use the implementation from https://github.com/steventango/jumpstart-rl.\n15", "source": "pdfplumber_text", "token": 1301}]}, {"page_num": 16, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nC.2. Extra Introduction to Applications\nHopper. Hopper game is a captivating two-dimensional challenge featuring a one-legged figure comprising a torso, thigh,\nleg, and a single supporting foot (Erez et al., 2012). The objective is to propel the Hopper forward through strategic hops by\napplying torques to the three hinges connecting its body parts. Observations include positional values followed by velocities\nof each body part, and the action space involves applying torques within a three-dimensional action space. Under the dense\nreward setting, the reward system combines healthy reward, forward reward, and control cost. Under the sparse reward\nsetting (Mazoure et al., 2019), the reward informs the x position of the hopper only if x > 0.6 in our experiments. The\nepisode concludes if the Hopper becomes unhealthy. We use “Hopper-v3” in our experiments.\nWalker2d. Walker2d is a dynamic two-dimensional challenge featuring a two-legged figure with a torso, thighs, legs, and\nfeet. The goal is to coordinate both sets of lower limbs to move in the forward direction by applying torques to the six hinges\nconnecting these body parts. The action space involves six dimensions, allowing exert torques at the hinge joints for precise\ncontrol. Observations encompass positional values and velocities of body parts, with the former preceding the latter. Under\nthe dense reward setting, the reward system combines a healthy reward bonus, forward reward, and control cost. Under\nthe sparse reward setting (Mazoure et al., 2019), the reward informs the x position of the hopper only if x > 0.6 in our\nexperiments. The episode concludes if the walker is deemed unhealthy. We use “Walker2d-v3” in our experiments and\nnormalize the observation when training the DRL agent.\nReacher. Reacher is an engaging two-jointed robot arm game where the objective is to skillfully maneuver the robot’s\nend effector, known as the fingertip, towards a randomly spawned target. The action space involves applying torques at\nthe hinge joints. Observations include the cosine and sine of the angles of the two arms, the coordinates of the target,\nangular velocities of the arms, and the vector between the target and the fingertip. It is worth noting that there is no sparse\nreward implementation of Reacher-v2 in Mazoure et al. (2019). The reward system comprises two components: “reward\ndistance” indicating the proximity of the fingertip to the target, and “reward control” penalizing excessive actions with a\nnegative squared Euclidean norm. The total reward is the sum of these components, and an episode concludes either after 50\ntimesteps with a new random target or if any state space value becomes non-finite. We use “Reacher-v2” in our experiments.\nHalfCheetah. HalfCheetah is an exhilarating 2D robot game where players control a 9-link cheetah with 8 joints, aiming to\npropel it forward with applied torques for maximum speed. The action space contains six dimensions, that enable strategic\nmovement. Observations include positional values and velocities of body parts. Under the dense reward setting, the\nreward system balances positive “forward reward” for forward motion with “control cost” penalties for excessive actions.\nUnder the sparse reward setting (Mazoure et al., 2019), the reward informs the x position of the hopper only if x > 5\nin our experiments. Episodes conclude after 1000 timesteps, offering a finite yet thrilling gameplay experience. We use\n“HalfCheetah-v3” in our experiments and normalize the observation when training the DRL agent.\nSelfish Mining. Selfish mining is a security vulnerability in blockchain protocols, identified by Eyal & Sirer (2018).\nWhen a miner holds a certain amount of computing power, they can withhold their freshly minted blocks from the public\nblockchain, thereby initiating a fork that is subsequently mined ahead of the official public blockchain. With this advantage,\nthe miner can introduce this fork into the network, overwriting the original blockchain and obtaining more revenue.\nTo find the optimal selfish mining strategies, Bar-Zur et al. (2023) proposed a deep reinforcement learning model to generate\na mining policy. The policy takes the current chain state as the input and chooses from the three pre-determined actions,\ni.e., adopting, revealing, and mining. With this policy network, the miner can obtain more mining rewards compared to\nusing heuristics-based strategies.\nWe train a PPO agent in the blockchain model developed by Bar-Zur et al. (git, d). The network architecture of the PPO\nagent is a 4-layer Multi-Layer Perceptron (MLP) with a hidden size of 128, 128, 128, and 128 in each layer. We adopt a\nsimilar network structure for training our mask network. The whale transaction has a fee of 10 with the occurring probability\nof 0.01 while other normal transactions have a fee of 1. The agent will receive a positive reward if his block is accepted and\nwill be penalized if his action is determined to be unsuccessful, e.g., revealing a private chain.\nIn our selfish mining task (Bar-Zur et al., 2023), three distinct actions are defined as follows:\nAdopt l: The miner chooses to adopt the first l blocks in the public chain while disregarding their private chain. Following\nthis, the miner will continue their mining efforts, commencing from the last adopted block.\nReveal l: This action becomes legal when the miner’s private chain attains a length of at least l. The consequence of this\n16", "source": "pdfplumber_text", "token": 1239}]}, {"page_num": 17, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\naction may result in either the creation of an active fork in the public chain or the overriding of the public chain.\nMine: This action simply involves continuing with the mining process. Once executed, a new block is mined and subsequently\nadded to either the private chain of the rational miner or to the public chain, contingent on which entity successfully mined\nthe block.\nCAGE Challenge 2. To inspire new methods for automating cyber defense, the Technical Cooperation Program (TTCP)\nlaunched the Autonomous Cyber Defence Challenge (CAGE Challenge) to produce AI-based blue teams for instantaneous\nresponse against cyber attacks (CAGE, 2022). The goal is to create a DRL blue agent to protect a network against a red\nagent. The action space of the blue agent includes monitoring, analyzing, decoyApache, decoyFemitter, decoyHarakaSMPT,\ndecoySmss, decoySSHD, decoySvchost, decoyTomcat, removing, and restoring. Note that the blue agent can receive a\nnegative reward when the red agent gets admin access to the system (and continues to receive negative rewards as the red\nagent maintains the admin access). We use CAGE challenge 2 for our evaluation.\nWe choose the champion scheme proposed by Cardiff University (git, c) in CAGE challenge 2 (git, b). The target agent is a\nPPO-based blue agent to defend a network against the red agent “B-line”. The trail has three different lengths, i.e., 30, 50,\nand 100. The final reward is the sum of the average rewards of these three different lengths.\nThe action set of the blue agent is defined as follows.\nMonitor: The blue agent automatically collects the information about flagged malicious activity on the system and reports\nnetwork connections and associated processes that are identified as malicious.\nAnalyze: The blue agent analyzes the information on files associated with recent alerts including signature and entropy.\nDecoyApache, DecoyFemitter, DecoyHarakaSMPT, DecoySmss, DecoySSHD, DecoySvchost, DecoyTomcat: The blue agent\nsets up the corresponding decoy service on a specified host. An alert will be raised if the red agent accesses the decoy\nservice.\nRemove: The blue agent attempts to remove red from a host by destroying malicious processes, files, and services.\nRestore: The blue agent restores a system to a known good state. Since it significantly impacts the system’s availability, a\nreward penalty of -1 will be added when executing this action.\nAutonomous Driving. Deep reinforcement learning has been applied in autonomous driving to enhance driving safety.\nOne representative driving simulator is MetaDrive (Li et al., 2022). A DRL agent is trained to guide a vehicle safely and\nefficiently to travel to its destination. MetaDrive converts the Birds Eye View (BEV) of the road conditions and the sensor\ninformation such as the vehicle’s steering, direction, velocity, and relative distance to traffic lanes into a vector representation\nof the current state. The policy network takes this state vector as input and yields driving actions, including accelerating,\nbraking, and steering commands. MetaDrive employs a set of reward functions to shape the learning process. For instance,\npenalties are assigned when the agent collides with other vehicles or drives out of the road boundary. To promote smooth and\nefficient driving, MetaDrive also incorporates rewards to encourage forward motion and the maintenance of an appropriate\nspeed.\nWe select the “Macro-v1” environment powered by the MetaDrive simulator (Li et al., 2022). The goal of the agent is to\nlearn a deep policy to successfully cross the car flow and reach the destination. We train the target agent and our mask\nnetwork by the PPO algorithm following the implementation of DI-drive (drive Contributors, 2021). The environment\nreceives normalized action to control the target agent a = [a , a ] ∈ [−1, 1]2. The action vector a will then be converted to\n1 2\nthe steering (degree), acceleration (hp), and brake signal (hp).\nMalware Mutation. DRL has been used to assess the robustness of ML-based malware detectors. For example, Anderson\net al. (2018) propose a DRL-based approach to attack malware detectors for portable executable (PE) files. We use the\n“Malconv” gym environment Raff et al. (2017) implemented in (git, a) for our experiments. We train a DRL agent based on\nTianshou framework (Weng et al., 2022). The input of the DRL agent is a feature vector of the target malware and outputs\nthe corresponding action to guide the malware manipulation. We present the action set of the MalConv gym environment in\nTable 2 for ease of comprehension in the case study section. A big reward of 10 is provided when evading detection.\nThe reward mechanism of the “Malconv” environment is as follows. Initially, the malware detection model will provide\na score sc of the current malware. If sc is lower than some threshold, the malware has already evaded the detection.\n0 0\nOtherwise, the DRL agent will take some mutation actions to bypass the detection. At step t, after executing the agent’s\n17", "source": "pdfplumber_text", "token": 1140}]}, {"page_num": 18, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nTable 2. Action set of the MalConv gym environment.\nAction index Action meaning\n0 “modify machine type”\n1 “pad overlay”\n2 “append benign data overlay”\n3 “append benign binary overlay”\n4 “add bytes to section cave”\n5 “add section strings”\n6 “add section benign data”\n7 “add strings to overlay”\n8 “add imports”\n9 “rename section”\n10 “remove debug”\n11 “modify optional header”\n12 “modify timestamp”\n13 “break optional header checksum”\n14 “upx unpack”\n15 “upx pack”\nTable 3. Hyper-parameter choices in Experiment I-V. “Selfish” represents Selfish Mining. “Cage” represents Cage Challenge 2. “Auto”\nrepresents Autonomous Driving. “Malware” represents Malware Mutation.\nHyper-parameter Hopper Walker2d Reacher HalfCheetah Selfish Cage Auto Malware\np 0.25 0.25 0.50 0.50 0.25 0.50 0.25 0.50\nλ 0.001 0.01 0.001 0.01 0.001 0.01 0.01 0.01\nα 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\naction, the malware detection model will provide a new score sc . If sc is lower than some threshold, the mutation is\nt t\nsuccessful and a big reward of 10 will be given. Otherwise, the reward will be sc − sc . The maximum allowed number of\n0 t\nsteps is 10.\nC.3. Additional Experiment Results\nHyper-parameter Choices in Experiment I-V. Table 3 summarizes our hyper-parameter choices in Experiment I-V. For all\napplications, we choose the coefficient of the intrinsic reward for training the mask network α as 0.01. The hyper-parameters\np and λ for our refining method vary by application.\nFidelity Scores in Experiment I. Figure 5 shows the fidelity score comparison across all explanation methods. We have\nthree key observations. First, We observe that our explanation method has similar fidelity scores with StateMask across all\napplications, empirically indicating the equivalence of our explanation method with StateMask. Second, we observe that our\nexplanation method and StateMask have higher fidelity scores than random explanation across all applications, indicating\nthat the mask network provides more faithful explanations for the target agents.\nEfficiency Comparison in Experiment II. Table 4 reports the efficiency evaluation results when training a mask network\nusing StateMask and our method. We observe that it takes 16.8% less time on average to train a mask network using our\nmethod than using StateMask, which shows the advantage of our method with respect to efficiency.\nComparison with Self-Imitation Learning. We compare RICE against the self-imitation learning (SIL) approach (Oh\net al., 2018) across four MuJoCo games. We present the results presented in Table 5. These experiment results demonstrate\nthat RICE consistently outperforms the self-imitation learning method. While self-imitation learning has the advantage\nof encouraging the agent to imitate past successful experiences by prioritizing them in the replay buffer, it cannot address\nscenarios where the agent (and its past experience) has errors or sub-optimal actions. In contrast, RICE constructs a mixed\ninitial distribution based on the identified critical states (using explanation methods) and encourages the agent to explore the\nnew initial states. This helps the agent escape from local minima and break through the training bottlenecks.\nImpact of Other Explanation Methods. We investigate the impact of other explanation methods (i.e., AIRS (Yu et al.,\n2023) and Integrated Gradients (Sundararajan et al., 2017)) on four Mujoco games. we fix the refining method and use\n18", "source": "pdfplumber_text", "token": 899}, {"type": "table", "text": "\n[Detected Table]\n| Hopper | Walker2d | Reacher | HalfCheetah | Selfish | Cage | Auto |\n| --- | --- | --- | --- | --- | --- | --- |\n| 0.25 | 0.25 | 0.50 | 0.50 | 0.25 | 0.50 | 0.25 |\n| 0.001 | 0.01 | 0.001 | 0.01 | 0.001 | 0.01 | 0.01 |\n| 0.0001 | 0.0001 | 0.0001 | 0.0001 | 0.0001 | 0.0001 | 0.0001 |\n\n", "source": "pdfplumber_table", "token": 190}]}, {"page_num": 19, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n3\n2\n1\n0\nTop10 Top20 Top30 Top40\nHopper\nytilediF\n3 1.0 2.0 Random\nStateMask\n0.8\n2 1.5 OURS\n0.6\n1.0\n0.4\n1\n0.2 0.5\n0 0.0 0.0\nTop10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40\nWalker2d Reacher HalfCheetah\n2.5 1.5 1.0 Random\n1.5 StateMask\n2.0 1.0 0.5 OURS\n1.0\n1.5\n1.0 0.5 0.5 0.0\n0.5 0.0 0.0 0.5\n0.0\nTop10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40\nSelfish Mining Cage Challenge 2 Autonomous Driving Malware Mutation\nFigure 5. Fidelity scores for explanation generated by baseline methods and our proposed explanation method. Note that a higher score\nimplies higher fidelity.\nTable 4. Efficiency comparison when training the mask network. We report the number of seconds when training the mask using a fixed\nnumber of samples. “Selfish” represents Selfish Mining. “Cage” represents Cage Challenge 2. “Auto” represents Autonomous Driving.\n“Malware” represents Malware Mutation.\nApplications Hopper Walker2d Reacher HalfCheetah Selfish Cage Auto Malware\nNum. of samples 3 × 105 3 × 105 3 × 105 3 × 105 1.5 × 106 1 × 107 2443260 32349\nStateMask 15393 2240 8571 1579 9520 79382 109802 50775\nOurs 12426 1899 7033 1317 8360 65400 88761 41340\ndifferent explanation methods to identify critical steps for refinement. The results are reported in Table 6. We observe that\nusing the explanation generated by our mask network, the refining achieves the best outcome across all four applications.\nUsing other explanation methods (Integrated Gradients and AIRS), our framework still achieves better results than the\nrandom baseline, suggesting that our framework can work with different explanation method choices.\nSensitivty of p and λ in Hopper game with an imitated PPO agent. We report the sensitivity of hyper-parameters p and\nλ in Hopper game with an imitated PPO agent in Figure 6. We observe that in general, a mixture probability of p = 0.25 or\np = 0.5 is a better choice. An RND bonus can facilitate the agent with faster refinement.\nSensitivity of Hyper-parameters p and λ. We provide the sensitivity results of p in all applications in Figure 7. We\nobserve that generally a mixture probability of p = 0.25 or p = 0.5 is a good choice. Additionally, recall that we need to\nuse the hyper-parameter λ to balance the scale of the “true” environment reward and the exploration bonus. We test the\nsensitivity of λ from the space {0.1, 0.01, 0.001}. Figure 8 reports the agent’s performance after refining under different\nsettings of λ. We observe that our retaining method is insensitive to the choice of λ. The agent’s performance does not vary\na lot with different settings of λ. But λ = 0.01 gives the best performance in all applications except selfish mining.\nSensitivity of α. Recall that under certain assumptions, we are able to simplify the design of StateMask. We propose\nan intrinsic reward mechanism to encourage the mask network to blind more states without sacrificing performance. The\nhyper-parameter α is then introduced to balance the performance of the perturbed agent and the need for encouraging\nblinding. We test the sensitivity of α from the space {0.01, 0.001, 0.0001} and report the fidelity scores under different\nsettings of α in Figure 9. We observe that though the value of α varies, the fidelity score does not change much.\nC.4. Evaluation Results of MuJoCo Games with Sparse Rewards\nResults of SparseWalker2d. First, we compare our refining method with other baseline methods (i.e., PPO fine-tuning,\nStateMask-R, and JSRL) in the SparseWalker2d game. Figure 10 shows that our refining method is able to help the DRL\nagent break through the bottleneck with the highest efficiency compared with other baseline refining methods. Additionally,\nby replacing our explanation method with a random explanation, we observe that the refining performance is getting worse.\n19", "source": "pdfplumber_text", "token": 1157}, {"type": "table", "text": "\n[Detected Table]\n| 3 1.0 2.0 Random 3 StateMask 0.8 ytilediF 2 1.5 OURS 2 0.6 1.0 0.4 1 1 0.2 0.5 0 0 0.0 0.0 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Hopper Walker2d Reacher HalfCheetah 2.5 1.5 1.0 Random 1.5 StateMask 2.0 1.0 0.5 OURS 1.0 1.5 1.0 0.5 0.5 0.0 0.5 0.0 0.0 0.5 0.0 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Selfish Mining Cage Challenge 2 Autonomous Driving Malware Mutation | Random |\n| --- | --- |\n|  | StateMask OURS |\n\n", "source": "pdfplumber_table", "token": 288}, {"type": "table", "text": "\n[Detected Table]\n| Random StateMask OURS |\n| --- |\n|  |\n\n", "source": "pdfplumber_table", "token": 19}, {"type": "table", "text": "\n[Detected Table]\n| Hopper | Walker2d | Reacher | HalfCheetah | Selfish | Cage | Auto |\n| --- | --- | --- | --- | --- | --- | --- |\n| 3×105 | 3×105 | 3×105 | 3×105 | 1.5×106 | 1×107 | 2443260 |\n| 15393 | 2240 | 8571 | 1579 | 9520 | 79382 | 109802 |\n| 12426 | 1899 | 7033 | 1317 | 8360 | 65400 | 88761 |\n\n", "source": "pdfplumber_table", "token": 191}]}, {"page_num": 20, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n4000.0\n3500.0\n3000.0\n2500.0\n2000.0\n1500.0\n1000.0\n500.0\n0.0\n0.0 200000.0 400000.0 600000.0 800000.0 1000000.0\n= 0.0\ndraweR\n4000.0\n3500.0\n3000.0\n2500.0\n2000.0\n1500.0\n1000.0\n500.0\n0.0\n0.0 200000.0 400000.0 600000.0 800000.0 1000000.0\n= 0.1\ndraweR\n4000.0\n3500.0\n3000.0\n2500.0\n2000.0\n1500.0\n1000.0\n500.0\n0.0\n0.0 200000.0 400000.0 600000.0 800000.0 1000000.0\n= 0.01\ndraweR\n4000.0\n3500.0\n3000.0\n2500.0\n2000.0\n1500.0\n1000.0\n500.0\n0.0\n0.0 200000.0 400000.0 600000.0 800000.0 1000000.0\n= 0.001\ndraweR\np=0.0\np=0.25\np=0.5\np=0.75\np=1.0\nFigure 6. Sensitivity results of hyper-parameters p and λ in Hopper game with an imitated PPO agent. We vary the hyper-parameter λ\nfrom {0, 0.1, 0.01, 0.001} and record the performance of the agent after refining. A smaller choice of λ means a smaller reward bonus\nfor exploration.\n3665\n3660\n3655\n3650\n3645\n0.00 0.25 0.50 0.75 1.00\nHopper\necnamrofreP\n3980\n3.0\n3975\n3.5\n3970\n4.0\n3965\n0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00\nWalker2d Reacher HalfCheetah\n16.5\n16.0\n15.5\n15.0\n14.5\n0.00 0.25 0.50 0.75 1.00\nSelfish Mining\necnamrofreP\n=0 =0.1 =0.01 =0.001\n20 57.5\n16\n22 55.0\n14\n24 12 52.5\n26 10 50.0\n28 8\n47.5\n0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00\nCage Challenge 2 Safe Driving Malware Mutation\nFigure 7. Sensitivity results of hyper-parameter p in all applications. We vary the hyper-parameter p from {0, 0.25, 0.5, 0.75, 1} under\ndifferent λ, and record the performance of the agent after refining. When p = 0, refining starts from the default initial states of the\nenvironment. When p = 1, refining starts exclusively from critical states. We show that the “mixed” initial state distribution helps to\nachieve a better performance.\n20", "source": "pdfplumber_text", "token": 1113}]}, {"page_num": 21, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n3660\n3655\n3650\n10 3 10 2 10 1\nHopper\necnamrofreP\n2140\n3980\n3.0 2130\n3975\n3.5 2120\n3970\n2110\n4.0\n3965\n10 3 10 2 10 1 10 3 10 2 10 1 10 3 10 2 10 1\nWalker2d Reacher HalfCheetah\n16.5\n16.0\n15.5\n15.0 10 3 10 2 10 1\nSelfish Mining\necnamrofreP\np=0 p=0.25 p=0.5 p=0.75 p=1\n58\n20\n16 56\n22\n14 54\n24\n12 52\n26\n10 50\n28 48\n10 3 10 2 10 1 10 3 10 2 10 1 10 3 10 2 10 1\nCage Chllenge 2 Safe Driving Malware Mutation\nFigure 8. Sensitivity results of hyper-parameter λ. We vary the hyper-parameter λ from {0.1, 0.01, 0.001} and record the performance of\nthe agent after refining. A smaller choice of λ means a smaller reward bonus for exploration.\n3\n2\n1\n0\nTop10 Top20 Top30 Top40\nHopper\nytilediF\n3 1.0 2.0 =0.01\n=0.001\n0.8\n2 1.5 =0.0001\n0.6\n1.0\n0.4\n1\n0.2 0.5\n0 0.0 0.0\nTop10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40\nWalker2d Reacher HalfCheetah\n2.5 1.5 1.0 =0.01\n1.5 =0.001\n2.0 =0.0001\n1.0 0.5\n1.5 1.0\n1.0\n0.5 0.5 0.0\n0.5\n0.0 0.0 0.0\nTop10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40\nSelfish Mining Cage Challenge 2 Autonomous Driving Malware Mutation\nFigure 9. Sensitivity results of hyper-parameter α. We vary the hyper-parameter α from {0.01, 0.001, 0.0001} and record the fidelity\nscores of the mask network trained under different settings of α. A higher fidelity score means a higher fidelity.\n21", "source": "pdfplumber_text", "token": 759}, {"type": "table", "text": "\n[Detected Table]\n| 3 1.0 2.0 =0.01 3 =0.001 0.8 ytilediF 2 1.5 =0.0001 2 0.6 1.0 0.4 1 1 0.2 0.5 0 0 0.0 0.0 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Hopper Walker2d Reacher HalfCheetah 2.5 1.5 1.0 =0.01 1.5 =0.001 2.0 =0.0001 1.0 0.5 1.5 1.0 1.0 0.5 0.5 0.0 0.5 0.0 0.0 0.0 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Selfish Mining Cage Challenge 2 Autonomous Driving Malware Mutation |  | =0.01 |\n| --- | --- | --- |\n|  |  | =0.001 =0.0001 |\n\n", "source": "pdfplumber_table", "token": 329}]}, {"page_num": 22, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nTable 5. Performance comparison between Self-Imitation Learning (SIL) and RICE on four MuJoCo tasks.\nMethod Hopper Walker2d Reacher HalfCheetah\nSIL 3646.46 (23.12) 3967.66 (1.53) -2.87 (0.09) 2069.80 (3.44)\nOurs 3663.91 (20.98) 3982.79 (3.15) -2.66 (0.03) 2138.89 (3.22)\nTable 6. Performance comparison when using different explanation methods across four MuJoCo tasks.\nTask Random Explanation Integrated Gradients AIRS Ours\nHopper 3648.98 (39.06) 3653.24 (14.23) 3654.49 (8.12) 3663.91 (20.98)\nWalker2d 3969.64 (6.38) 3972.15 (4.77) 3976.35 (2.40) 3982.79 (3.15)\nReacher -3.11 (0.42) -2.99 (0.31) -2.89 (0.19) -2.66 (0.03)\nHalfCheetah 2132.01 (0.76) 2132.81 (0.83) 2133.98 (2.52) 2138.89 (3.22)\nSensitivity of p and λ. We report the sensitivity of hyper-parameters p and λ in the three MuJoCo games with sparse\nrewards in Figure 11, Figure 12, and Figure 13. We have the following observations: First, generally, a mixed probability p\nwithin the range of 0.25 and 0.5 would be a good choice. Second, the refining benefits from the exploration bonus in the\nsparse MuJoCo games. Third, PPO fine-tuning cannot guarantee that the refined agent can achieve a good performance.\nEspecially in SparseWalker2d game, we observe that ppo fine-tuning cannot break through the training bottleneck of the\nDRL agent.\nC.5. Qualitative Analysis\nWe do a qualitative analysis of the autonomous driving case to understand how RICE impacts agent behavior and performance.\nWe visualize the agent’s behavior before and after refining the agent. Figure 14(a) shows a trajectory wherein the target\nagent (depicted by the green car) fails to reach its destination due to a collision with a pink car on the road. Given the\nundesired outcome, we use our method to identify the critical steps that contribute to the final (undesired) outcome. The\nimportant steps are highlighted in red color. Our method identifies the important step as the one when the green car switches\nacross two lanes into the lane of the pink car. The critical state is reasonable because this early step allows the green car to\nswitch lanes to avoid the collision. Based on the provided explanation, we apply our refining method to improve the target\nagent. The trajectory after refining is shown in Figure 14(b). It shows that after refining, the refined agent (the green car)\nsuccessfully identifies an alternative path to reach the destination while avoiding collision.\nD. Case Study: Malware Mutation\nD.1. Design Intuitions\nFirst, we use malware mutation as a case study to confirm our design intuitions before the proposed refining method. Recall\nthat the refining method contains three important ideas. First, we integrate the explanation result (identified critical step)\ninto the refining process. Second, we design a mixed initial state distribution to guide the refining of the target agent. Third,\nwe encourage the agent to perform exploration for diverse states during the refining phase. In the following, we create\nmultiple baselines by gradually adding these ideas to a naive baseline to show the contribution of each idea. We also provide\nevidence to support our stance against overfitting. Table 7 summarizes the results.\nTo start, the original agent is trained for 100 epochs until convergence. We test the target agent for 500 runs, resulting in\nan average evasion probability of 33.8%. To extract behavioral patterns, we perform a frequency analysis on the mutation\nactions taken by the agent across all 500 runs. As shown in the first row of Table 7, there is a clear preference for A\n4\n(i.e., “add bytes to section cave”). A complete list of the possible actions (16 in total) is shown in Table 2 (Appendix).\nContinue Learning w/o Explanation. The most common refining method is to lower the learning rate and continue\ntraining. We continue to train this target agent using the PPO algorithm for an additional 30 epochs and evaluate its\nperformance over 500 runs. This yields an average evasion probability of 38.8% (second row in Table 7). It is worth noting\nthat A (i.e., “add bytes to section cave”) remains the most frequently selected action.\n4\nLeverage Explanation Results for Refining. Subsequently, we assess the refining outcome by incorporating our\nexplanation result into the refining process. Specifically, we initiate the refining exclusively from the critical steps identified\nby the explanation method. For this setting, we do not perform exploration.\n22", "source": "pdfplumber_text", "token": 1236}, {"type": "table", "text": "\n[Detected Table]\n| Hopper | Walker2d | Reacher |\n| --- | --- | --- |\n| 3646.46(23.12) | 3967.66(1.53) | -2.87(0.09) |\n| 3663.91(20.98) | 3982.79(3.15) | -2.66(0.03) |\n\n", "source": "pdfplumber_table", "token": 112}, {"type": "table", "text": "\n[Detected Table]\n| RandomExplanation | IntegratedGradients | AIRS |\n| --- | --- | --- |\n| 3648.98(39.06) | 3653.24(14.23) | 3654.49(8.12) |\n| 3969.64(6.38) | 3972.15(4.77) | 3976.35(2.40) |\n| -3.11(0.42) | -2.99(0.31) | -2.89(0.19) |\n| 2132.01(0.76) | 2132.81(0.83) | 2133.98(2.52) |\n\n", "source": "pdfplumber_table", "token": 201}]}, {"page_num": 23, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nFigure 10. Agent Refining Performance in the SparseWalker2d Games. For the left figure, we fix the explanation method to our method\n(mask network) if needed while varying refining methods. For the right figure, we fix the refining method to our method while varying the\nexplanation methods.\nTable 7. Malware Mutation Case Study—We evaluate the evasion probability of the agent under different settings and count the\ncorresponding action frequencies.\nRefine Setting Test Setting Action Frequency Evasion\nOriginal agent w/o refinement From default initial S {A : 4,914, A : 5} 33.8%\n4 9\nContinue training From default initial S {A : 2,590, A : 55, A : 99, A : 95} 38.8%\n4 7 1 9\nFrom critical states {A : 2,546, A : 138, A : 32, A : 8} 50.8%\nRefine from critical states 12 5 4 9\nFrom default initial S {A : 4,728, A : 62} 36.2%\n12 5\nRefine from mixed initial state dist. From default initial S {A : 1,563, A : 1,135, A : 332, A : 12} 58.4%\n4 12 5 6\nRefine from mixed initial state dist. + exploration From default initial S {A : 2,448, A : 165, A : 138, A : 6} 68.2%\n5 7 12 4\nDuring the test phase, we explore two testing settings. First, we artificially reset the test environment to start from these\ncritical steps. We find that evasion probability surges to 50.8%. A (i.e., “modify timestamp”) becomes as the most\n12\nfrequently chosen action. This indicates the refined agent learns a policy when encountering the critical state again. However,\nfor more realistic testing, we need to set the test environment to the default initial state (i.e., the correct testing condition).\nUnder this setting, we find the evasion probability diminishes to 36.2%. This stark contrast in results shows evidence of\noverfitting. The refined agent excels at solving the problem when starting from critical steps but falters when encountering\nthe task from default initial states.\nImpact of Mixed Initial State Distribution. Given the above result, we further build a baseline by refining from the\nproposed mixed initial state distribution (i.e., blending the default initial state distribution with the critical states). For this\nsetting, we also do not perform exploration. Through 500 runs of testing, we observe a notable improvement, with the\naverage evasion probability reaching 58.4% (from the previous baseline’s 36.2%). Furthermore, the action frequency pattern\nhas also undergone a shift. It combines the preferred actions from the two previous refining strategies, highlighting the\nfrequent selection of both A and A .\n4 12\nImpact of Exploration. Finally, we explore the impact of exploration. This baseline represents the complete version\nof our proposed system by adding the exploration step and using the mixed initial distribution. We notice that the\naverage evasion probability across 500 runs has a major increase, reaching 68.2%. The most frequent action now is A\n5\n(i.e., “add section strings”). A and A are still among the top actions but their frequencies are lowered. This shows the\n4 12\nbenefits of exploring previously unseen states and diverse mutation paths. In return, the refined agent is able to get out of the\nlocal minima to identify more optimal policies.\nD.2. Discovery of Design Flaws\nAdditionally, our explanation results have led to the discovery of design flaws in the malware mutation application (Raff\net al., 2017). We will further explain how we use RICE to identify these problems.\nQuestions and Intuitions. When using RICE to explain the malware mutation process, we observe a scenario where the\nagent constantly chooses the same action “upx pack” in multiple consecutive steps. According to the agent, these actions\n23", "source": "pdfplumber_text", "token": 948}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  |\n\n", "source": "pdfplumber_table", "token": 45}, {"type": "table", "text": "\n[Detected Table]\n| TestSetting | ActionFrequency |\n| --- | --- |\n| FromdefaultinitialS | {A :4,914,A :5} 4 9 |\n| FromdefaultinitialS | {A :2,590,A :55,A :99,A :95} 4 7 1 9 |\n| Fromcriticalstates | {A :2,546,A :138,A :32,A :8} 12 5 4 9 |\n| FromdefaultinitialS | {A :4,728,A :62} 12 5 |\n| FromdefaultinitialS | {A :1,563,A :1,135,A :332,A :12} 4 12 5 6 |\n| FromdefaultinitialS | {A :2,448,A :165,A :138,A :6} 5 7 12 4 |\n\n", "source": "pdfplumber_table", "token": 217}]}, {"page_num": 24, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.0\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.1\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.01\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.001\ndraweR\np=0.0 p=0.25 p=0.5 p=0.75 p=1.0\nFigure 11. Sensitivity results of hyper-parameter λ in SparseHopper game. We vary the hyper-parameter λ from {0, 0.1, 0.01, 0.001} and\nrecord the performance of the agent after refining. A smaller choice of λ means a smaller reward bonus for exploration.\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.0\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.1\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.01\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.001\ndraweR\np=0.0 p=0.25 p=0.5 p=0.75 p=1.0\nFigure 12. Sensitivity results of hyper-parameter λ in SparseWalker2d game. We vary the hyper-parameter λ from {0, 0.1, 0.01, 0.001}\nand record the performance of the agent after refining. A smaller choice of λ means a smaller reward bonus for exploration.\n24", "source": "pdfplumber_text", "token": 973}]}, {"page_num": 25, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 50000.0 100000.0 150000.0 200000.0 250000.0\n= 0.0\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 50000.0 100000.0 150000.0 200000.0 250000.0\n= 0.1\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 50000.0 100000.0 150000.0 200000.0 250000.0\n= 0.01\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 50000.0 100000.0 150000.0 200000.0 250000.0\n= 0.001\ndraweR\np=0.0 p=0.25 p=0.5 p=0.75 p=1.0\nFigure 13. Sensitivity results of hyper-parameter λ in SparseHalfCheetah game. We vary the hyper-parameter λ from {0, 0.1, 0.01, 0.001}\nand record the performance of the agent after refining. A smaller choice of λ means a smaller reward bonus for exploration.\nreceive a similar reward. However, RICE (our mask network) returns different “explanations” for these steps (i.e., they have\nhighly different importance scores). According to RICE, only the first action holds a high importance score, while the other\nconsecutive actions barely have an impact on the final reward (i.e., they appear redundant). This raises the question: why\ndoes the agent assign a similar reward to these consecutive steps in the first place?\nAnother interesting observation is from refining experiments. We find that PPO-based refining cannot yield substantial\nimprovements. While we have expected that these methods do not perform as well as ours (given our exploration step),\nthe difference is still bigger than we initially expected. This motivates us to further examine the reward function design to\nexplore whether it has inadvertently discouraged the DRL agent from finding good evasion paths.\nProblems of Reward Design. Driven by the intuitions above, we examined the reward design and identified two problems.\nFirstly, the reward mechanism is inherently non-Markovian which deviates from the expectation of a typical reinforcement\nlearning (RL) framework. In typical RL settings, rewards are contingent on the current state s and the next state s′. However,\nthe current design computes the reward based on the initial state s and the subsequent state s′. Consequently, this may\n0\nassign an identical reward for the same action (e.g., “upx pack”) in consecutive steps. This non-Markovian nature of the\nreward mechanism can mislead the DRL agent and hurt its performance.\nSecond, we find that the intermediate rewards exhibit unusually high sparsity, i.e., many intermediate rewards tend to have\na value close to zero, which poses a significant challenge for the PPO algorithm to learn a good policy based on such\nintermediate rewards. Agents refined with these methods can be easily trapped in local minima.\nFixing the Problematic Reward Design. Based on these insights, we fix the bugs in the reward design with two simple\nsteps: (1) We make the reward function Markovian, which depends only on the current state and the next state. (2) We\nperform scaling on the intermediate reward with a coefficient of 3. After that, we re-run an experiment to evaluate the\ncorrectness of our modifications. We train a DRL agent for 100 epochs with the same parameters under the new reward\ndesign and test its performance over 3 trials of 500 runs. The experiment shows that the evasion probability of the agent\nunder the new reward design jumps from 42.2% (using the old reward function, see Table 1) to 72.0%, which further\nconfirms our intuitions. This case study illustrates how developers can use RICE to debug their system and improve their\ndesigns.\n25", "source": "pdfplumber_text", "token": 1087}]}, {"page_num": 26, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nTarget agent Collision\nhigh\nlow\n(a) Original trajectory with importance score\n(b) Trajectory after refining with RICE Avoid collision\nFigure 14. (a): In the original trajectory, the target agent (the green car) eventually collides with the pink car, which is an undesired\noutcome. Each time step is marked with a different color: “yellow” indicates the least important step and “red” represents the most\nimportant step. (b): We highlight the critical states identified by our explanation method and the corresponding outcome after refining.\nUsing our explanation method, the target agent (the green car) successfully avoids collision.\n100\n50\n0\n0 1 2 3 4\nTraining Step (1e6)\ndraweR\nOurs\nRND\nFigure 15. Refining performance with our method and RND method in MountainCarContinuous-v0 game. The state coverage of the\npre-trained policy is limited to a small range around the initial point.\nE. Limitation\nWe use the continuous “Mountain Car” environment (mou) as a negative control task to illustrate a scenario where RICE\ndoes not work well. In this “extreme” case, Assumption 3.2 is completely broken since the state coverage of the pre-trained\nagent is limited to a small range around the initial point. In this experiment, we train a target agent using Proximal Policy\nOptimization (PPO) for 1 million steps. The results show that the policy performance remained poor, with the agent\nfrequently getting trapped at the starting point of the environment. In such cases where the original policy fails to learn\nan effective strategy, the role of explanations becomes highly limited. Since RICE relies on the identified critical states to\nenhance the policy, if the policy itself is extremely weak (i.e., not satisfying Assumption 3.2), then the explanations will not\nbe meaningful, which further huts the refinement. In the case of the Mountain Car experiment, RICE essentially reduces to\nbeing equivalent to Random Network Distillation (RND) due to the lack of meaningful explanation. We show the result\nwhen refining the pre-trained agent using our method and RND in Figure 15.\n26", "source": "pdfplumber_text", "token": 488}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n", "source": "pdfplumber_table", "token": 24}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n", "source": "pdfplumber_table", "token": 24}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |\n| --- | --- | --- | --- |\n|  |  |  |  |\n|  |  |  | Ours RND |\n|  |  |  |  |\n\n", "source": "pdfplumber_table", "token": 54}]}]