[{"page_num": 1, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning\nwith Explanation\nZelei Cheng * 1 Xian Wu * 1 Jiahao Yu 1 Sabrina Yang 2 Gang Wang 3 Xinyu Xing 1\nAbstract However, training an optimal DRL agent for complex tasks,\nDeep reinforcement learning (DRL) is playing an particularly in environments with sparse rewards, presents\nincreasingly important role in real-world applica- a significant challenge. Often cases, the training of a DRL\ntions. However, obtaining an optimally perform- agent can hit a bottleneck without making further process:\ning DRL agent for complex tasks, especially with its sub-optimal performance becomes evident when it makes\nsparse rewards, remains a significant challenge. common mistakes or falls short of achieving the final goals.\nThe training of a DRL agent can be often trapped\nWhen the DRL agent hits its training bottleneck, a refine-\nin a bottleneck without further progress. In this\nment strategy can be considered, especially if the agent is\npaper, we propose RICE, an innovative refining\nalready locally optimal. To refine the locally optimal DRL\nscheme for reinforcement learning that incorpo-\nagent, one method is to analyze its behavior and patch the\nrates explanation methods to break through the\nerrors it made. A recent work (Cheng et al., 2023) proposes\ntraining bottlenecks. The high-level idea of RICE\nStateMask to identify critical states of the agent using an ex-\nis to construct a new initial state distribution that\nplanation method. One utility of StateMask is patching the\ncombines both the default initial states and criti-\nagent’s error, which fine-tunes the DRL agent starting from\ncal states identified through explanation methods,\nthe identified critical states (denoted as “StateMask-R”).\nthereby encouraging the agent to explore from\nHowever, such an approach suffers from two drawbacks.\nthe mixed initial states. Through careful design,\nOn the one hand, initializing solely from critical states will\nwe can theoretically guarantee that our refining\nhurt the diversity of initial states, which can cause overfitting\nscheme has a tighter sub-optimality bound. We\n(see Appendix D). On the other hand, fine-tuning alone can-\nevaluate RICE in various popular RL environ-\nnot help the DRL agent jump out of the local optima. These\nments and real-world applications. The results\nobservations drive us to rethink how to design a proper ini-\ndemonstrate that RICE significantly outperforms\ntial distribution and apply exploration-based techniques to\nexisting refining schemes in enhancing agent per-\npatch previous errors.\nformance.\nAnother reason behind the training bottleneck can be the\npoor choice of the training algorithm. Naturally, to improve\n1. Introduction performance, the developer needs to select another DRL\ntraining algorithm to re-train the DRL agent. However, for\nDeep reinforcement learning (DRL) has shown promising\ncomplex DRL tasks, re-training the agent from scratch is\nperformance in various applications ranging from playing\ntoo costly. For instance, for AlphaStar (Vinyals et al., 2019)\nsimulated games (Todorov et al., 2012; Mnih et al., 2013; Oh\nto attain grandmaster-level proficiency in StarCraft, its train-\net al., 2016; Cai et al., 2023) to completing real-world tasks\ning period exceeds one month with TPUs. Retraining an\nsuch as navigating autonomous vehicles and performing\nagent of this level can incur a cost amounting to millions of\ncybersecurity attacks and defenses (Bar-Zur et al., 2023;\ndollars (Agarwal et al., 2022). Therefore, existing research\nVyas et al., 2023; Anderson et al., 2018; Wang et al., 2023).\nhas investigated the reuse of previous DRL training (as prior\nknowledge) to facilitate re-training (Ho & Ermon, 2016;\n*Equal contribution 1Department of Computer Science,\nNorthwestern University, Evanston, Illinois, USA 2Presentation Fu et al., 2018; Cai et al., 2022). The most recent exam-\nHigh School, San Jose, California, USA 3Department of Com- ple is Jump-Start Reinforcement Learning (JSRL) proposed\nputer Science, University of Illinois at Urbana-Champaign, by Uchendu et al. (2023) which leverages a pre-trained\nUrbana, Illinois, USA. Correspondence to: Xinyu Xing\npolicy to design a curriculum to guide the training of a self-\n<xinyu.xing@northwestern.edu>.\nimproving exploration policy. However, their selection of\nProceedings of the 41 st International Conference on Machine exploration frontiers in the curriculum is random, which\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by cannot guarantee that the exploration frontiers have positive\nthe author(s).\n1", "source": "pdfplumber_text", "token": 1100}, {"type": "image_analysis", "text": "\n[图片分析 (page_1_1765334882.jpg)]:\n图片:<{这是一张来自学术论文第一页的文本图像，内容为一篇题为《RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation》的论文。该页包含以下关键信息：\n\n- **标题**：RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n- **作者**：Zelei Cheng, Xian Wu, Jiahao Yu, Sabrina Yang, Gang Wang, Xinyu Xing，分别来自西北大学、圣何塞高中和伊利诺伊大学厄巴纳-香槟分校。\n- **摘要（Abstract）**：介绍了深度强化学习（DRL）在现实世界应用中的重要性，以及在稀疏奖励环境中训练最优DRL智能体所面临的挑战。提出了一种名为RICE的新颖精炼方案，通过结合默认初始状态和关键状态（由解释方法识别），构建新的初始状态分布，以帮助智能体突破训练瓶颈。该方法理论上可保证更紧的次优性边界，并在多个流行RL环境和实际应用中显著优于现有精炼方案。\n- **引言（Introduction）**：进一步阐述了DRL在游戏、自动驾驶、网络安全等领域的应用前景，以及训练瓶颈问题（如局部最优、算法选择不当等）。提到了StateMask、Jump-Start Reinforcement Learning (JSRL) 等相关工作及其局限性。\n- **出版信息**：Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s)。\n\n该页面不含流程图或数据图表，主要为论文的标题、作者、摘要和引言部分的纯文本内容。没有可视化元素或数据点可供提取。}>\n", "is_scanned_fallback": false, "token": 405}]}, {"page_num": 2, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nreturns. This motivates us to incorporate explanation meth- than those informed by random explanation.\nods to scrutinize the pre-trained policy and design more\n• We propose an alternative design of StateMask to ex-\neffective exploration frontiers.\nplain the agent’s policy in DRL-based applications.\nIn this work, we propose RICE1, a Refining scheme for Experiments show that our explanation has compara-\nReInforCement learning with Explanation. We first lever- ble fidelity with StateMask while improving efficiency.\nage a state-of-the-art explanation method to derive a step-\n• With extensive evaluations and case studies, we il-\nlevel explanation for the pre-trained DRL policy. The expla-\nlustrate the benefits of using RICE to improve a pre-\nnation method identifies the most critical states (i.e., steps\ntrained policy.\nthat contribute the most to the final reward of a trajectory),\nwhich will be used to construct the exploration frontiers.\nBased on the explanation results, we construct a mixed ini- 2. Related Work\ntial state distribution that combines the default initial states\n2.1. Explanation-based Refining\nand the identified critical states to prevent the overfitting\nproblem. By forcing the agent to revisit these exploration Recently, there has been some work that leverages the DRL\nfrontiers, we further incentivize the agent to explore starting explanation to improve the agent’s performance. These ex-\nfrom the frontiers. Through exploration, the agent is able to planations can be derived from either human feedback or\nexpand state coverage, and therefore more effectively break automated processes. Guan et al. (2021); Van Waveren et al.\nthrough the bottlenecks of reinforcement learning training. (2022) propose to utilize human feedback to correct the\nOur theoretical analysis shows that this method achieves a agent’s failures. More specifically, when the agent fails, hu-\ntighter sub-optimality bound by utilizing this mixed initial mans (can be non-experts) are involved to point out how to\ndistribution (see Section 3.4). avoid such a failure (i.e., what action should be done instead,\nand what action should be forbidden). Based on human feed-\nIn addition, we introduce key improvements to the state-of-\nback, the DRL agent gets refined by taking human-advised\nthe-art explanation method StateMask (Cheng et al., 2023)\naction in those important time steps and finally obtains the\nto better facilitate our refining scheme. We reformulate the\ncorrected policy. The downside is that it relies on humans to\nobjective function and add a new reward bonus for encour-\nidentify critical steps and craft rules for alternative actions.\naging blinding when training—this significantly simplifies\nThis can be challenging for a large action space, and the re-\nthe implementation without sacrificing the theoretical guar-\ntraining process is ad-hoc and time-consuming. Cheng et al.\nantee.\n(2023); Yu et al. (2023) propose to use step-level DRL expla-\nEvaluation and Findings. We evaluate the perfor- nation methods to automatically identify critical time steps\nmance of RICE using four MuJoCo games and four DRL- and refine the agent accordingly. It initiates the refining pro-\nbased real-world applications, including cryptocurrency cess by resetting the environment to the critical states and\nmining (Bar-Zur et al., 2023), autonomous cyber defense subsequently resumes training the DRL agents from these\n(Cage Challenge 2) (CAGE, 2022), autonomous driving (Li critical states. Empirically, we observe that this refining\net al., 2022), and malware mutation (Raff et al., 2017). strategy can easily lead to overfitting (see Appendix D). In-\nWe show that the explanation derived from our new de- stead, we propose a novel refining strategy with theoretical\nsign demonstrates similar fidelity to the state-of-the-art guarantees to improve the agent’s performance.\ntechnique StateMask (Cheng et al., 2023) with signifi-\ncantly improved training efficiency. With the explana- 2.2. Leveraging Existing Policy\ntion results, we show our refining method can produce\nhigher performance improvements for the pre-trained DRL The utilization of existing policies to initialize RL and en-\nagent, in comparison with existing approaches including hance exploration has been explored in previous literature.\nJSRL (Uchendu et al., 2023) and the original refining Some studies propose to “roll-in” with an existing policy\nmethod from StateMask (Cheng et al., 2023). for better exploration, as demonstrated in works (Agarwal\net al., 2020; Li et al., 2023). Similar to our approach, JSRL\nIn summary, our paper has the following contributions:\n(Uchendu et al., 2023) incorporates a guide policy for roll-in,\nfollowed by a self-improving exploration policy. Techni-\ncally, JSRL relies on a curriculum for the gradual update of\n• We develop a refining strategy to break through the\nthe exploration frontier. However, the curriculum may not\nbottlenecks of reinforcement learning training with an\nbe able to truly reflect the key reasons why the guide policy\nexplanation (which is backed up by a theoretical anal-\nsucceeds or fails. Therefore, we propose to leverage the\nysis). We show our refining method performs better\nexplanation method to automatically identify crucial states,\n1The source code of RICE can be found in https:// facilitating the rollout of the policy by integrating these iden-\ngithub.com/chengzelei/RICE tified states with the default initial states. In Section 4, we\n2", "source": "pdfplumber_text", "token": 1239}, {"type": "image_analysis", "text": "\n[图片分析 (page_2_1765334897.jpg)]:\n图片:<{这是一张来自学术论文PDF的第2页，内容主要为英文文本，包含论文的引言、相关工作（Related Work）部分以及一些贡献点的总结。页面顶部标题为“RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation”，表明该论文提出了一种名为RICE（Reinforcing learning with Explanation）的方法，旨在通过解释机制突破强化学习训练中的瓶颈。\n\n主要内容包括：\n\n1. **研究动机与方法**：\n   - 提出RICE框架，利用最先进的解释方法对预训练的深度强化学习（DRL）策略进行细化。\n   - 使用解释方法识别轨迹中对最终奖励贡献最大的关键状态（即关键步骤），用于构建探索边界。\n   - 构建混合初始状态分布，结合默认初始状态和识别出的关键状态，以防止过拟合，并激励智能体从边界开始探索，从而扩展状态覆盖范围，有效打破强化学习训练的瓶颈。\n\n2. **理论分析**：\n   - 理论分析表明，使用该混合初始分布可以实现更紧的次最优性界（tighter sub-optimality bound）。\n\n3. **方法改进**：\n   - 对现有解释方法StateMask（Cheng et al., 2023）进行改进，重新制定目标函数，并引入新的奖励项以鼓励训练过程中的“遮蔽”行为，简化实现同时保持理论保障。\n\n4. **实验评估**：\n   - 在四个MuJoCo游戏和四个基于DRL的真实世界应用中评估RICE性能，包括加密货币挖掘、自主网络防御（Cage Challenge 2）、自动驾驶和恶意软件变异等任务。\n   - 实验结果表明，新提出的解释方法在保真度上与StateMask相当，但显著提升了训练效率。\n\n5. **贡献总结**：\n   - 提出一种基于解释的精炼策略，突破强化学习训练瓶颈，并提供理论支持。\n   - 展示该方法优于随机解释引导的策略。\n   - 提出StateMask的替代设计，提升效率且保持解释保真度。\n   - 通过大量评估和案例研究展示RICE在提升预训练策略方面的优势。\n\n6. **相关工作（Related Work）**：\n   - 2.1节讨论了基于解释的精炼方法，如Guan et al. (2021) 和 Van Waveren et al. (2022) 利用人类反馈纠正智能体失败；Cheng et al. (2023) 和 Yu et al. (2023) 使用分步级DRL解释方法识别关键时间步并重置环境进行再训练。\n   - 2.2节讨论了利用现有策略初始化强化学习并增强探索的研究，如JSRL（Uchendu et al., 2023）采用引导策略进行roll-in，随后使用自改进探索策略，但其课程可能无法真正反映引导策略成功或失败的关键原因。\n\n7. **脚注信息**：\n   - 注释①说明RICE的源代码可在 https://github.com/chengzelei/RICE 获取。\n\n8. **整体特征**：\n   - 文本结构清晰，逻辑严谨，属于典型的计算机科学领域学术论文格式。\n   - 包含多个引用文献，如Cheng et al. (2023), Guan et al. (2021), Van Waveren et al. (2022), Uchendu et al. (2023) 等。\n   - 页面底部显示页码为“2”。\n\n该图片不含流程图、数据图表或图像，仅为纯文本内容，重点在于阐述RICE方法的设计思想、理论基础、实验验证及与现有工作的对比分析。}>\n", "is_scanned_fallback": false, "token": 826}]}, {"page_num": 3, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nCritical state Initial state Original trajectory Explore trajectory\n(a) Original trajectories generated by a (b) Mix default initial states and identified (c) Rollout new trajectories with\npre-trained DRL policy. critical states based on explanation. RND-based exploration.\nFigure 1. Given a pre-trained DRL policy that is not fully optimal (a), we propose the RICE algorithm that resets the RL agent to specific\nvisited states (a mixture of default initial states and identified critical states) (b), followed by an exploration step initiated from these\nchosen states (c).\nempirically demonstrate that JSRL performs poorly in our which may be sub-optimal. Our objective is to break through\nselected games. Chang et al. (2023) propose PPO++ that the training bottlenecks of the pre-trained policy with an ex-\nreset the environment to a mixture of the default initial states planation. Rather than re-training from scratch, we propose\nand the visited states of a guide policy (i.e., a pre-trained to utilize explanation to take full advantage of the guidance\npolicy). It can be viewed as a special case in our framework, of the pre-trained policy π. Importantly, we do not assume\ni.e., constructing a mixed initial distribution with a random knowledge of the original training algorithm used for pol-\nexplanation. However, we claim that not all visited states icy π. And we make the following assumptions about the\nof a pre-trained policy are informative and our theoretical quality of π.\nanalysis and experiments both show that RICE based on our Assumption 3.1. Given a random policy πr, we have\nexplanation method outperforms the refining method based E [Aπ(s, a)] ≤ 0, ∀s.\na∼πr\non a random explanation.\nIntuitively, the above assumption implies that taking an\naction based on a random policy πr will provide a lower ad-\n3. Proposed Technique\nvantage than taking actions based on the policy π. This is a\nreasonable assumption since π is a pre-trained policy, thus it\n3.1. Problem Setup and Assumption\nwould perform much better than an untrained (i.e., random)\nWe model the problem as a Markov Decision Process policy.\n(MDP), which is defined as a tuple ⟨S, A, P, ρ, R,\nAssumption 3.2. The pre-trained policy π cover the states\nw\nγ⟩\nh\n.\ner\nI\ne\nn\ne\nt\na\nh\nc\ni\nh\ns t\ns\nup\na\nle\nn\n,\nd\nS\na\nan\nre\nd\npr\nA\nese\na\nn\nre\nts\nth\nth\ne\ne\ns\ns\nta\nta\nte\nte\na\na\nn\nn\nd\nd\na\na\nc\nc\nti\nt\no\nio\nn\nn\nse\no\nt\nf\n,\nvisited by the optimal policy π∗:\n(cid:13) (cid:13)\n(cid:13)\ndπ\nρ\n∗ (cid:13) (cid:13)\n(cid:13) ≤ C, where C is\nthe agent at t t ime t. t P : S × A → ∆(S) is the state a constant. (cid:13) dπ ρ (cid:13) ∞\ntransition function, R : S × A → R is the reward\nIn other words, Assumption 3.2 requires that the pre-trained\nfunction. γ ∈ (0, 1) is the discount factor. For a pol-\npolicy visits all good states in the state space. Note that it is\nicy π(a|s): S → A, the value function and Q-function\nis defined as V π(s) = E [ (cid:80)∞ γtR(s , a ) | s = s] a standard assumption in the online policy gradient learning\nπ t=0 t t 0\nand Qπ(s, a) = E [ (cid:80)∞ γtR(s , a ) | s = s, a = a]. (Agarwal et al., 2021; Uchendu et al., 2023; Li et al., 2023)\nπ t=0 t t 0 0 and is much weaker than the single policy concentrateabil-\nThe advantage function for the policy π is denoted as\nAπ(s, a) = Qπ(s, a) − V π(s). We assume the initial ity coefficient assumption (Rashidinejad et al., 2021; Xie\net al., 2021), which requires the pre-trained policy visits all\nstate distribution is given by ρ: s ∼ ρ. The goal of\n0\nRL is to find an optimal policy π∗ that maximizes its ex- good state-action pairs. The ratio in Assumption 3.2 is also\npected total reward : π∗ = arg max E [V π(s)]. Be- referred to as the distribution mismatch coefficient.\nπ s∼ρ\nsides, we also introduce the state occupancy distribution\nand the state-action occupancy measure for π, denoted 3.2. Technical Overview\nas dπ(s) = (1 − γ) (cid:80)∞ γt Prπ (s = s | s ∼ ρ) and\nρ t=0 t 0 Recall our goal is to refine the pre-trained DRL agent to\ndπ(s, a) = dπ(s)π(a|s).\nρ ρ break through the training bottlenecks. At a high level, the\nIn our setting, we have a pre-trained policy denoted as π, RICE algorithm integrates a roll-in step, where the RL agent\nis reset to specific visited states, followed by an exploration\n3", "source": "pdfplumber_text", "token": 1288}, {"type": "image_analysis", "text": "\n[图片分析 (page_3_1765334919.jpg)]:\n图片:<{\n  类型: 流程图\n  描述: 图1展示了一个强化学习（RL）算法RICE的执行流程，旨在通过解释性方法突破预训练深度强化学习（DRL）策略的训练瓶颈。该图分为三个子图，分别表示不同阶段的轨迹演化过程。\n\n  (a) Original trajectories generated by a pre-trained DRL policy:\n    - 展示了由预训练DRL策略生成的原始轨迹。\n    - 轨迹以橙色曲线表示，从初始状态（红色圆点）出发，经过一系列状态，最终到达目标区域（蓝色星形标记）。\n    - 存在一个关键状态（绿色圆点），在路径中被识别为可能影响性能的关键点。\n\n  (b) Mix default initial states and identified critical states based on explanation:\n    - 在此阶段，系统结合默认初始状态和通过解释机制识别出的关键状态。\n    - 新的起始点包括多个红色圆点（默认初始状态）和绿色圆点（关键状态）。\n    - 橙色轨迹从这些新起点出发，探索不同的路径。\n\n  (c) Rollout new trajectories with RND-based exploration:\n    - 引入基于随机网络差异（RND）的探索机制，生成新的探索轨迹（蓝色曲线）。\n    - 探索轨迹从(b)中的选定状态开始，尝试更广泛地覆盖状态空间。\n    - 目标是打破预训练策略的局限性，提高策略的泛化能力。\n\n  执行流程:\n    (a) -> (b) -> (c)\n    即：原始轨迹 → 混合初始与关键状态 → 基于RND的探索新轨迹\n\n  关键元素:\n    - 初始状态（Initial state）: 红色圆点\n    - 关键状态（Critical state）: 绿色圆点\n    - 原始轨迹（Original trajectory）: 橙色曲线\n    - 探索轨迹（Explore trajectory）: 蓝色曲线\n    - 目标位置（Goal）: 蓝色星形标记\n\n  图片标号: Figure 1\n  图片标题: Given a pre-trained DRL policy that is not fully optimal (a), we propose the RICE algorithm that resets the RL agent to specific visited states (a mixture of default initial states and identified critical states) (b), followed by an exploration step initiated from these chosen states (c).\n}>\n", "is_scanned_fallback": false, "token": 554}]}, {"page_num": 4, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nstep initiated from these chosen states. During the roll-in Algorithm 1 Training the Mask Network.\nstep, we draw inspiration from established RL-explanation Input: Target agent’s policy π\nmethods (Puri et al., 2019; Guo et al., 2021; Cheng et al., Output: Mask network π˜\nθ\n2023) to identify critical states, referred to as exploration Initialization: Initialize the weights θ for the mask net π˜ θ\nfrontiers, within the given policy π. As depicted in Figure 1, θ old ← θ\nfor iteration=1, 2, . . . do\nwhen presented with a trajectory sampled from the policy\nSet the initial state s ∼ ρ\n0\nπ, we employ a step-level explanation method – StateMask D ← ∅\n(Cheng et al., 2023) to identify the most crucial time steps for t=0 to T do\ninfluencing the final rewards in this trajectory. Subsequently, Sample a t ∼ π(a t |s t )\nwe guide the RL agent to revisit these selected states. The S C a o m m p p l u e te am t the ∼ ac π˜ t θ u o a l l d t ( a a k m t en |s a t c ) tion a ← a ⊙ am\nrationale behind revisiting these states lies in their ability to t t\n(s , R′ ) ← env.step(a) and record (s , s , am, R′ )\noffer an expanded initial state distribution compared to ρ, in t D +1 t t t+1 t t\nthereby enabling the agent to explore diverse and relevant end for\nstates it might otherwise neglect. Additionally, we intro- update θ old ← θ using D by PPO algorithm\nduce a mixing of these selected states with the initial states end for\nsampled from ρ. This mixing approach serves the purpose\nof preventing the agent from overfitting to specific states.\nwhere π denotes the policy of the target agent (i.e., our pre-\nIn Section 3.4, we theoretically show that RICE achieves\ntrained policy), π¯ denotes the policy of the perturbed agent\na tighter regret bound through the utilization of this mixed\n(i.e., integrating the random policy and the target agent π\ninitial distribution.\nvia the mask network π˜), η(·) is the expected total reward of\nThen, we propose an exploration-based method to further an agent by following a certain policy. To solve the Eqn. (2)\nenhance the DRL agent’s performance. The high-level idea with monotonicaly guarantee, StateMask carefully designs\nis to incentivize the agent to explore when initiating actions a surrogate function and utilize the prime-dual methods to\nfrom these frontiers. Intuitively, the pre-trained policy π optimize the π˜. However, we can optimize the learning\nmight converge to a local optimal, as shown in Figure 1. process of mask net within our setting to enhance simplicity.\nThrough exploration, we aim to expand state coverage by Specifically, we have the following theorem\nrewarding the agent for visiting novel states, thereby in-\nTheorem 3.3. Under Assumption 3.1, we have η(π¯) upper-\ncreasing the likelihood of successfully completing the task.\nbounded by η(π): η(π¯) ≤ η(π).\nSpecifically, we utilize the Proximal Policy Optimization\n(PPO) algorithm (Schulman et al., 2017) for refining the\nThe proof of the theorem can be found in Appendix A.\nDRL agent, leveraging the monotonicity of PPO.\nLeveraging this theorem, we can transform the objective\nfunction to J(θ) = max η(π¯). With this reformulation,\n3.3. Technique Detail we can utilize the vanilla PPO algorithm to train the state\nmask without sacrificing the theoretical guarantee. How-\nStep-level Explanation. We leverage a state-of-the-art\never, na¨ıvely maximizing the expected total reward may\nexplanation method StateMask (Cheng et al., 2023). At a\nintroduce a trivial solution to the problem which is to not\nhigh level, StateMask parameterizes the importance of the\nblind the target agent at all (always outputs “0”). To tackle\ntarget agent’s current time step as a neural network model\nthis problem, we add an additional reward by giving an extra\n(i.e., mask network). This neural network takes the current\nbonus when the mask net outputs “1”. The new reward can\nstate as input and then outputs this step’s importance score\nbe written as R′(s , a ) = R(s , a ) + αam where α is a\nwith respect to the agent’s final reward. To do so, StateMask t t t t t\nhyper-parameter. We present the learning process of the\nlearns a policy to “blind” the target agent at certain steps\nmask network in Algorithm 1. By applying this resolved\nwithout changing the agent’s final reward. Specifically, for\nmask to each state, we will be able to assess the state impor-\nan input state s , the mask net outputs a binary action am of\nt t tance (i.e., the probability of mask network outputting “0”)\neither “zero” or “one”, and the target agent will sample the\nat any time step.\naction a from its policy. The final action is determined by\nt\nthe following equation Constructing Mixed Initial State Distribution. With the\n(cid:40) a , if am = 0 , state mask π˜, we construct a mixed initial state distribution\na ⊙ am = t t (1) to expand the coverage of the state space. Initially, we\nt t a if am = 1 ,\nrandom t randomly sample a trajectory by executing the pre-trained\nThe mask net is then trained to minimize the following policy π. Subsequently, the state mask is applied to pinpoint\nobjective function: the most important state within the episode τ by assessing\nthe significance of each state. The resulting distribution of\nJ(θ) = min |η(π) − η(π¯)| , (2) these identified critical states is denoted as dπˆ(s). Indeed,\nρ\n4", "source": "pdfplumber_text", "token": 1368}, {"type": "image_analysis", "text": "\n[图片分析 (page_4_1765334934.jpg)]:\n图片:<{\n  这是一张来自学术论文的PDF页面（第4页），内容主要涉及强化学习（Reinforcement Learning, RL）中的一种新方法——RICE（Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation）。该页面包含以下关键元素：\n\n  1. **算法描述**：\n     - **Algorithm 1**: 描述了“Training the Mask Network”的过程。\n       - 输入：目标智能体策略 π\n       - 输出：掩码网络 $\\bar{\\pi}_\\theta$\n       - 初始化：初始化掩码网络权重 θ\n       - 主循环：对每个迭代，从初始状态分布 ρ 中采样轨迹，使用掩码网络生成动作掩码 $a_t^m$，并计算实际执行的动作 a。通过环境交互获得下一个状态和奖励，并记录相关数据。最后利用PPO算法更新掩码网络参数。\n       - 执行流程表示为：\n         ```\n         初始化 → for iteration in 1..∞:\n             采样初始状态 s₀ ~ ρ\n             D ← ∅\n             for t=0 to T:\n                 采样 aₜ ~ π(aₜ|sₜ)\n                 采样 aₜ^m ~ π̄_θ_old(aₜ^m|sₜ)\n                 计算实际动作 a ← aₜ ⊙ aₜ^m\n                 (sₜ₊₁, R'ₜ) ← env.step(a)\n                 记录 (sₜ, sₜ₊₁, aₜ^m, R'ₜ) 到 D\n             使用 PPO 更新 θ_old ← θ 利用 D\n         ```\n\n  2. **数学公式**：\n     - 公式(1)：定义了动作选择机制：\n       $$\n       a_t \\odot a_t^m = \n       \\begin{cases}\n       a_t, & \\text{if } a_t^m = 0 \\\\\n       a_{\\text{random}}, & \\text{if } a_t^m = 1\n       \\end{cases}\n       $$\n       表示当掩码输出为0时，采用原策略动作；否则随机选择动作。\n\n     - 公式(2)：目标函数定义：\n       $$\n       J(\\theta) = \\min |\\eta(\\pi) - \\eta(\\bar{\\pi})|\n       $$\n       目标是最小化目标策略 π 和扰动策略 $\\bar{\\pi}$ 的期望总奖励差异。\n\n  3. **文本内容摘要**：\n     - 提出了一种基于解释的强化学习方法，结合StateMask技术识别关键状态（exploration frontiers）。\n     - 使用step-level explanation方法（StateMask）评估每一步的重要性，指导智能体重新访问重要状态。\n     - 引入混合初始状态分布以防止过拟合特定状态。\n     - 利用PPO算法优化掩码网络，确保单调性保证。\n     - 定理3.3指出在一定假设下，$\\eta(\\bar{\\pi}) \\leq \\eta(\\pi)$，即扰动策略不会劣于原始策略。\n\n  4. **关键术语与概念**：\n     - StateMask：一种状态重要性评分模型，输出二进制掩码。\n     - 掩码网络 $\\bar{\\pi}$：用于决定是否“屏蔽”当前动作。\n     - 混合初始状态分布：由预训练策略采样轨迹后，通过掩码筛选出关键状态构成的新分布。\n     - PPO（Proximal Policy Optimization）：用于训练掩码网络的优化算法。\n\n  5. **结构特征**：\n     - 页面布局清晰，左侧为正文描述，右侧为算法伪代码及理论分析。\n     - 包含定理、推导、算法流程和数学表达式，属于典型的机器学习/人工智能研究论文风格。\n     - 图片中标注有“Figure 1”、“Section 3.4”、“Appendix A”等引用信息，但图本身未显示。\n\n  6. **无图像或图表**：\n     - 此页不含可视化的图形（如折线图、热力图、流程图等），仅包含文字、公式和算法伪代码。\n     - 虽然提到“Figure 1”，但该图未出现在此页。\n\n  总结：该页面是关于RICE方法的技术细节部分，重点在于如何通过掩码网络（mask network）进行状态重要性评估，并结合PPO优化实现更有效的探索与训练。核心思想是通过解释性机制引导智能体关注关键状态，从而突破训练瓶颈。\n}>\n", "is_scanned_fallback": false, "token": 1028}]}, {"page_num": 5, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nAlgorithm 2 Refining the DRL Agent. 3.4. Theoretical Analysis\nInput: Pre-trained policy π, corresponding state mask π˜, default\nFinally, we provide theoretical analysis demonstrating that\ninitial state distribution ρ, reset probability threshold p\nOutput: The agent’s policy after refining π′ our refining algorithm can tighten the sub-optimality gap:\nfor iteration=1, 2, . . . do SubOpt := V π∗(ρ) − V π′(ρ), (i.e., the gap between the\nD ← ∅ long-term reward collected by the optimal policy π∗ and\nRAND NUM ← RAND(0,1) that obtained by the refined policy π′ when starting from\nif RAND NUM < p then\nthe default initial state distribution ρ).\nRun π to obtain a trajectory τ of length K\nIdentify the most critical state s t in τ via state mask π˜ In particular, we aim to answer the following two questions:\nSet the initial state s ← s\n0 t Q1:What are the benefits of incorporating StateMask to\nelse\ndetermine the exploration frontier?\nSet the initial state s ∼ ρ\n0\nend if\nQ2: what advantages does starting exploration from the\nfor t=0 to T do\nmixed initial distribution offer?\nSample a ∼ π(a |s )\nt t t\n(s , R ) ← env.step(a )\nt+1 t t To answer the questions, we first show that determining the\n(cid:13) (cid:13)2\nCalculate RND bonus R t RND = (cid:13) (cid:13) f (s t+1 ) − fˆ(s t+1 )(cid:13) (cid:13) exploration frontiers based on StateMask is equivalent to\nwith normalization sampling states from a better policy compared to π. Then,\nAdd (s t , s t+1 , a t , R t + λR t RND) to D we demonstrate that under the mixed initial distribution as\nend for\nintroduced in Section 3.3, we could provide a tighter upper\nOptimize π w.r.t PPO loss on D\nθ bound for the sub-optimality of trained policy π compared\nOptimize fˆ w.r.t. MSE loss on D using Adam\nθ with randomly selecting visited states to form the initial\nend for\nπ′ ← π distribution.\nθ\nIn order to answer Q1, we begin with Assumption 3.4 to\nassume the relationship between the policy value and the\nstate distribution mismatch coefficient.\nin Section 3.4, we demonstrate that this re-weighting-like Assumption 3.4. For two polices π and πˆ, if η(πˆ) ≥ η(π),\nsampling is equivalent to sampling the state from a better (cid:13) (cid:13) dπ∗ (cid:13) (cid:13) (cid:13) (cid:13) dπ∗ (cid:13) (cid:13)\np o o f l t i h c e y s πˆ e . le W ct e ed th i e m n p s o et rt t a h n e t i s n t i a t t i e a s l d d i i s s t t r r i i b b u u t t i i o o n n µ dπˆ a ( s s a ) m an ix d tu th r e e then we have (cid:13) (cid:13) d ρ π ρ ˆ (cid:13) (cid:13) ∞ ≤ (cid:13) (cid:13) d ρ π ρ (cid:13) (cid:13) ∞ .\nρ\noriginal initial distribution of interest ρ: µ(s) = βdπˆ(s) + Intuitively, this assumption posits that a superior policy\nρ\n(1 − β)ρ(s), where β is a hyper-parameter. would inherently possess a greater likelihood of visiting all\nfavorable states. We give validation of this assumption in a\nExploration with Random Network Distillation. Start- 2-state MDP in Appendix B.1.\ning from the new initial state distribution, we continue train-\nWe further present Lemma 3.5 to answer Q1, i.e., the bene-\ning the DRL agent while encouraging the agent to do ex-\nfits of incorporating StateMask to determine the exploration\nploration. In contrast to goal-conditional RL (Ren et al.,\nfrontier. The proof of Lemma 3.5 can be found in Appendix\n2019; Ecoffet et al., 2019), which typically involve ran-\nB.2.\ndom exploration from identified frontiers, we advocate for\nthe RL agent to explore novel states to increase the state Lemma 3.5. Given a pre-trained policy π, our MaskNet-\ncoverage. Motivated by this, we adopt Random Network based sampling approach in Section 3.3 is equivalent to\nDistillation (RND) (Burda et al., 2018) which is proved to sampling states from a state occupation distribution induced\nbe an effective exploration bonus, especially in large and by an improved policy πˆ.\ncontinuous state spaces where count-based bonuses (Belle-\nmare et al., 2016; Ostrovski et al., 2017) can be hard In order to answer Q2, we start with presenting Theorem\nto extend. Specifically, we directly utilize the PPO al- 3.6 to bound the sub-optimality via the state distribution\ngorithm to update the policy π, except that we add the mismatch coefficient.\nintrinsic reward to the task reward, that is, we optimize Theorem 3.6. Assume that for the refined policy π′,\nR′(s t , a t ) = R(s t , a t ) + λ|f (s t+1 ) − fˆ(s t+1 )|2, where λ E s∼dπ′ (cid:104) max a Aπ′(s, a) (cid:105) < ϵ. For two initial state dis-\ncontrols the trade-off between the task reward and explo- µ\ntributions µ and ρ, we have the following bound (Kakade &\nration bonus. Along with the policy parameters, the RND\nLangford, 2002)\npredictor network fˆ is updated to regress to the target net-\nwork f . Note that, as the state coverage increases, RND V π∗ (ρ) − V π′ (ρ) ≤ O( ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) ). (3)\nbonuses decay to zero and a performed policy is recovered. (1 − γ)2 (cid:13) (cid:13) dπ ρ ˆ (cid:13) (cid:13)\n∞\nWe present our proposed refining method in Algorithm 2.\n5", "source": "pdfplumber_text", "token": 1562}, {"type": "image_analysis", "text": "\n[图片分析 (page_5_1765334962.jpg)]:\n图片:<{\n  这是一张来自学术论文的PDF页面（第5页），内容主要包含算法描述、理论分析和相关数学推导，具体如下：\n\n  1. **算法部分（Algorithm 2）**：\n     - 名称：Refining the DRL Agent（精炼深度强化学习代理）\n     - 输入：预训练策略 π、对应的状态掩码 ṡ、默认初始状态分布 ρ、重置概率阈值 p\n     - 输出：精炼后的策略 π'\n     - 流程描述：\n       - 初始化数据集 D 和随机数 RAND_NUM\n       - 若 RAND_NUM < p，则执行策略 π 生成轨迹 τ，通过状态掩码 ṡ 找到最临界状态 s_t，并将其设为初始状态 s₀\n       - 否则，从分布 ρ 中采样初始状态 s₀\n       - 对于 t=0 到 T 的每一步：\n         - 根据策略 π 采样动作 a_t\n         - 执行环境 step 得到 (s_{t+1}, R_t)\n         - 计算 RND 奖励 R^{RND}_t = ||f(s_{t+1}) - f̂(s_{t+1})||²（归一化后）\n         - 将 (s_t, s_{t+1}, a_t, R_t + λ·R^{RND}_t) 添加至数据集 D\n       - 使用 PPO 损失优化策略 π_θ\n       - 使用 MSE 损失优化预测网络 f̂\n       - 更新策略 π' ← π\n\n     - 算法流程图示：  \n       Input → Random Sampling → Trajectory Generation → State Masking → Initial State Selection → Loop: Action Sampling → Environment Step → RND Bonus Calculation → Data Collection → Policy & Network Optimization → Output π'\n\n     - 关键依赖关系：\n       - 策略 π 和状态掩码 ṡ 共同决定初始状态选择\n       - RND 奖励依赖于目标网络 f 和预测网络 f̂ 的差异\n       - 数据集 D 用于 PPO 和 MSE 优化\n       - 最终输出 π' 是经过精炼的策略\n\n  2. **理论分析部分（3.4 Theoretical Analysis）**：\n     - 目标：证明精炼算法能缩小次最优性差距 SubOpt = V^{π*}(ρ) - V^{π'}(ρ)\n     - 提出两个核心问题：\n       - Q1: 引入 StateMask 确定探索边界有何优势？\n       - Q2: 从混合初始分布开始探索有何好处？\n\n     - 主要假设与引理：\n       - Assumption 3.4：若策略 ṡ 比 π 更优（η(ṡ) ≥ η(π)），则其状态访问分布更接近最优策略的分布。\n       - Lemma 3.5：基于 MaskNet 的采样等价于从一个改进策略诱导的状态占用分布中采样。\n       - Theorem 3.6：在特定条件下，可对次最优性进行上界估计，公式为：\n         V^{π*}(ρ) - V^{π'}(ρ) ≤ O(ε / (1−γ)² · ||d̄_ρ^{π*} / d̄_ρ^{π'}||_∞)\n\n     - 数学符号说明：\n       - V^{π*}: 最优策略下的长期回报\n       - V^{π'}: 精炼策略下的长期回报\n       - d̄_ρ^{π*}: 从初始分布 ρ 出发，由最优策略 π* 诱导的状态分布\n       - d̄_ρ^{π'}: 由精炼策略 π' 诱导的状态分布\n       - γ: 折扣因子\n       - ε: 误差项，表示最大状态价值偏差\n\n  3. **其他关键信息**：\n     - 文中提到的 RND（Random Network Distillation）方法用于增强探索，奖励机制为状态表征差异的平方。\n     - 探索策略采用混合初始分布 μ(s) = β·d̄_ρ^{π}(s) + (1−β)·ρ(s)，其中 β 是超参数。\n     - 使用 PPO 和 Adam 优化器分别更新策略和预测网络。\n\n  4. **整体特征**：\n     - 该页属于一篇关于强化学习中训练瓶颈突破的研究论文，聚焦于通过“解释”机制提升训练效率。\n     - 结合了算法设计、理论分析和数学证明，强调了状态掩码（StateMask）和混合初始分布对提升探索效率的作用。\n     - 图片中标注为 “Page 5”，无图像或图表，仅为文本和公式。\n\n  总结：这是一张不含图像但包含重要算法流程和理论分析的学术页面，核心内容是提出并验证一种利用状态掩码和随机网络蒸馏来精炼DRL代理的方法。该方法通过理论推导保证了性能提升，并提供了可复现的算法步骤。\n}>\n", "is_scanned_fallback": false, "token": 1131}]}, {"page_num": 6, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nThe proof of Theorem 3.6 can be found in Appendix B.3. fine-tuning” (Schulman et al., 2017), i.e., lowering the learn-\nIt indicates that the upper bound on the difference between ing rate and continuing training with the PPO algorithm.\nthe performance of the optimal policy π∗ and that of the The second baseline is a refining method introduced by\n(cid:13) (cid:13)\npolicy π′ after refining is proportional to\n(cid:13)\n(cid:13)\ndπ\nρ\n∗ (cid:13)\n(cid:13) . With\nStateMask (Cheng et al., 2023), i.e., resetting to the crit-\n(cid:13) dπ ρ ˆ (cid:13) ∞ ical state and continuing training from the critical state.\nAssumption 3.4 and Lemma 3.5, we now claim that our re- The third baseline is Jump-Start Reinforcement Learning\nfining method with our explanation could further tighten the (referred to as “JSRL”) (Uchendu et al., 2023). JSRL in-\nsub-optimality gap via reducing the distribution mismatch troduces a guided policy π to set up a curriculum to train\ng\ncoefficient compared with forming an initial distribution by an exploration policy π . Through initializing π = π , we\ne e g\nrandom selecting visited states, i.e., with a random explana- can transform JSRL to be a refining method that can further\ntion. improve the performance of the guided policy.\nClaim 1. We can form a better (mixed) initial state dis- Evaluation Metrics. To evaluate the fidelity of the gen-\ntribution µ with our explanation method and tighten the erated explanation, we utilize an established fidelity score\nupper bound of V π∗(ρ) − V π′(ρ) compared with random metric defined in StateMask (Cheng et al., 2023). The idea\nexplanation. is to use a sliding window to step through all time steps and\nthen choose the window with the highest average impor-\nThe details of the analysis can be found in Appendix B.4. tance score (scored by the explanation method). The width\nBased on Assumption 3.2 and Claim 1, we can learn to of the sliding window is l while the whole trajectory length\nperform as well as the optimal policy as long as the visited is L. Then we randomize the action(s) at the selected critical\nstates of the optimal policy are covered by the (mixed) initial step(s) in the selected window (i.e., masking) and measure\ndistribution. the average reward change as d. Additionally, we denote\nthe maximum possible reward change as d . Therefore,\nmax\n4. Evaluation the fidelity score is calculated as log(d/d max ) − log(l/L).\nA higher fidelity score indicates higher fidelity.\nIn this section, we start with our experiment setup and de-\nFor the applications with dense rewards except the malware\nsign, followed by experiment results and analysis. We pro-\nmutation application, we measure the reward of the target\nvide additional evaluation details in Appendix C.\nagent before and after refining. In the case of the malware\nmutation application, we report the “final reward” as the\n4.1. Experiment Setup\nprobability of evading the malware detector, both before\nEnvironment Selection. We select eight representative and after refining. For the applications with sparse rewards,\nenvironments to demonstrate the effectiveness of RICE we report the performance during the refining process.\nacross two categories: simulated games (Hopper, Walker2d,\nReacher, and HalfCheetah of the MuJoCo games) and real- 4.2. Experiment Design\nworld applications (selfish mining, network defense, au-\nWe use the following experiments to evaluate the fidelity\ntonomous driving, and malware mutation) 2. We addition-\nand efficiency of the explanation method, the effectiveness\nally run the experiments in the three sparse MuJoCo games\nof the refining method and other factors that influenced the\nintroduced by Mazoure et al. (2019). The details of these\nsystem performance (e.g., alternative design choices, hyper-\napplications can be found in Appendix C.2.\nparameters).\nBaseline Explanation Methods. Since our explanation\nExperiment I. To show the equivalence of our explanation\nmethod proposes an alternative design of StateMask, the first\nmethod with StateMask, we compare the fidelity of our\nbaseline is StateMask. We compare our explanation method\nmethod with StateMask. Given a trajectory, the explanation\nwith StateMask to show the equivalence and efficiency of\nmethod first identifies and ranks top-K important time steps.\nour method. Additionally, we introduce “Random” as a\nAn accurate explanation means the important time steps\nbaseline explanation method. “Random” identifies critical\nhave significant contributions to the final reward. To validate\nsteps by randomly selecting a visited state as the critical\nthis, we let the agent fast-forward to the critical step and\nstate.\nforce the target agent to take random actions. Then we\nBaseline Refining Methods. We compare our refining follow the target agent’s policy to complete the rest of the\nmethod with three baselines. The first baseline is “PPO time steps. If the explanation is accurate, we expect a major\nchange to the final reward by randomizing the actions at\n2These are representative security applications that have a sig-\nthe important steps. We compute the fidelity score of each\nnificant impact on the security community (Anderson et al., 2018)\nexplanation method as mentioned in StateMask across 500\nand they represent RL tasks with sparse rewards, which are com-\nmon in security applications. trajectories. We set K = 10%, 20%, 30%, 40% and report\n6", "source": "pdfplumber_text", "token": 1261}, {"type": "image_analysis", "text": "\n[图片分析 (page_6_1765334991.jpg)]:\n图片:<{这是一张来自PDF文档第6页的文本页面，内容属于学术论文《RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation》的“Evaluation”章节。该页面主要包含以下结构化信息：\n\n1. **理论分析部分**：\n   - 提到Theorem 3.6的证明在Appendix B.3中，涉及最优策略与精炼后策略性能差异的上界分析。\n   - 引用Assumption 3.4和Lemma 3.5支持其精炼方法能进一步缩小子最优性差距（sub-optimality gap）。\n   - Claim 1指出：通过解释方法可构造更好的初始状态分布μ，从而收紧V^π*(ρ) − V^π′(ρ)的上界，优于随机解释。\n\n2. **实验设计（Evaluation）**：\n   - **4.1 Experiment Setup**：\n     - 环境选择：选取8个代表性环境，分为两类：\n       - 模拟游戏：Hopper、Walker2d、Reacher、HalfCheetah（MuJoCo游戏）。\n       - 真实世界应用：自私挖矿、网络防御、自动驾驶、恶意软件突变。\n     - 所有实验在三个稀疏奖励的MuJoCo游戏中运行，引用Mazoure et al. (2019)。\n     - 应用细节见Appendix C.2。\n   - **Baseline Explanation Methods**：\n     - 对比方法包括StateMask（Cheng et al., 2023）和Random（随机选择关键状态）。\n   - **Baseline Refining Methods**：\n     - 三种对比精炼方法：\n       - PPO fine-tuning（Schulman et al., 2017）：降低学习率并继续训练。\n       - StateMask（Cheng et al., 2023）：重置至关键状态后继续训练。\n       - JSRL（Jump-Start RL, Uchendu et al., 2023）：引入引导策略π_g，初始化为π_e = π_g，实现精炼。\n\n3. **评估指标（Evaluation Metrics）**：\n   - 使用滑动窗口法计算保真度分数（fidelity score），定义如下：\n     - 在轨迹中滑动窗口，选择平均重要性得分最高的窗口。\n     - 窗口宽度为l，轨迹总长度为L。\n     - 在选定窗口的关键步骤中随机化动作（masking），测量平均奖励变化d。\n     - 最大可能奖励变化为d_max。\n     - 保真度分数 = log(d/d_max) − log(l/L)，值越高表示解释越准确。\n   - 针对密集奖励任务（除恶意软件突变外），报告目标代理前后奖励变化。\n   - 恶意软件突变任务中，报告“最终奖励”即绕过检测的概率。\n   - 稀疏奖励任务中，报告精炼过程中的性能表现。\n\n4. **实验设计（4.2 Experiment Design）**：\n   - 实验I：比较所提方法与StateMask的保真度等效性。\n     - 给定轨迹，识别并排名前K个重要时间步。\n     - 快进至关键步骤，强制执行随机动作。\n     - 若解释准确，应导致最终奖励显著变化。\n     - 计算500条轨迹上的保真度分数，K取10%、20%、30%、40%。\n\n5. **其他信息**：\n   - 页面底部标注页码为“6”。\n   - 脚注②说明所选安全应用具有显著安全影响（Anderson et al., 2018），代表稀疏奖励的强化学习任务。\n\n整体特征：该页面为纯文本，无图表或图像，重点在于描述实验设计、基线方法、评估指标及理论支撑，用于验证RICE方法在强化学习中的有效性。}>\n", "is_scanned_fallback": false, "token": 878}]}, {"page_num": 7, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nthe fidelity of the selected methods under each setup. We tions and provide the full results in Figure 5 of Appendix\nrepeat each experiment 3 times with various random seeds C.3. We observe that the fidelity scores of StateMask and\nand report the mean and standard deviation. Additionally, our method are comparable. Furthermore, We evaluate\nto show the efficiency of our design, we report the training the efficiency of our explanation method compared with\ntime of the mask network using StateMask and our method StateMask. We report the cost time and the number of sam-\nwhen given a fixed number of training samples. ples when training our explanation method and StateMask\nin Table 4 of Appendix C.3. We observe an average of\nExperiment II. To show the effectiveness of the refin-\n16.8% drop in the training time compared with StateMask.\ning method, we compare the agent’s performance after re-\nThe reason is that the training algorithm of the mask net-\nfining using our method and three aforementioned base-\nwork in StateMask involves an estimation of the discounted\nline methods, i.e., PPO fine-tuning (Schulman et al., 2017),\naccumulated reward with respect to the current policy of\nStateMask’s fine-tuning from critical steps (Cheng et al.,\nthe perturbed agent and the policy of the target agent which\n2023), and Jump-Start Reinforcement Learning (Uchendu\nrequires additional computation cost. In contrast, our design\net al., 2023). For this experiment, all the refining meth-\nonly adds an additional term to the reward which is simple\nods use the same explanation generated by our explanation\nbut effective.\nmethod if needed, to ensure a fair comparison. Addition-\nally, we conduct a qualitative study to understand how our Effectiveness of Refining. We compare the agent’s per-\nrefining method influences agent behavior and performance. formance after refining using different retaining methods\nacross all applications with dense rewards in Table 1. The\nExperiment III To investigate how the quality of expla-\nperformance is measured by the final reward of the refined\nnation affects the downstream refining process, we run our\nagent. In most applications, rewards are typically assigned\nproposed refining method based on the critical steps identi-\npositive values. However, in Cage Challenge 2, the reward is\nfied by different explanation methods (Random, StateMask,\ndesigned to incorporate negative values (see Appendix C.2).\nand our method) and compare the agent’s performance after\nWe have three main observations. First, we observe that\nrefining.\nour refining method can bring the largest improvement for\nExperiment IV. To show the versatility of our method, the target agent in all applications. Second, we find that the\nwe examine the refining performance when the pre-trained PPO fine-tuning method only has marginal improvements\nagent was trained by other algorithms such as Soft Actor- for the agents due to its incapability of jumping out of local\nCritic (SAC) (Haarnoja et al., 2018). First, we obtain a optima. Third, the refining method proposed in StateMask\npre-trained SAC agent and then use Generative Adversarial (which is to start fine-tuning only from critical steps) cannot\nImitation Learning (GAIL) (Ho & Ermon, 2016) to learn always improve the agent’s performance. The reason is that\nan approximated policy network. We compare the refining this refining strategy can cause overfitting and thus harm the\nperformance using our method against baseline methods, agent’s performance. We illustrate this problem in greater\ni.e., PPO fine-tuning (Schulman et al., 2017), StateMask’s detail in a case study of Malware Mutation in Appendix D.\nfine-tuning from critical steps (Cheng et al., 2023), and It is also worth mentioning that we discover design flows of\nJump-Start Reinforcement Learning (Uchendu et al., 2023). Malware Mutation and present the details in Appendix D.\nIn addition, we also include fine-tuning the pre-trained SAC\nWe also run our experiments of varying refining methods\nagent with the SAC algorithm as a baseline.\non selected MuJoCo games with sparse rewards. Figure 2\nExperiment V. We test the impact of hyper-parameter shows the results of our method against other baselines in\nchoices for two primary hyper-parameters for refining: p SparseHopper and SparseHalfCheetah games. We observe\n(used to control the mixed initial state distribution) and λ that our refining method has significant advantages over\n(used to control the exploration bonus). For our refining other baselines with respect to final performance and refin-\nmethod, we vary p from {0, 0.25, 0.5, 0.75, 1} and vary λ ing efficiency. Through varying explanation methods, we\nfrom {0, 0.1, 0.01, 0.001}. By examining the agent’s per- confirm that the contribution should owe to our explana-\nformance with various λ values, we can further investigate tion method. We leave the refining results of the Sparse-\nthe necessity of the exploration bonus. Additionally, we Walker2d game and the hyper-parameter sensitivity results\nevaluate the choice of α for our explanation method (used to of all sparse MuJoCo games in Appendix C.4.\ncontrol the mask ratio for the mask network). Specifically, In addition to numerical results, we also provide a qualita-\nwe vary α from {0.01, 0.001, 0.0001}. tive analysis of the autonomous driving case to understand\nhow RICE influences agent behavior and performance, par-\n4.3. Experiment Results ticularly in a critical state, in Appendix C.5. We visualize\nthe agent’s behavior before and after refining the agent to\nFidelity and Efficiency of Explanation. We compare the\nshow that RICE is able to help the agent break through\nfidelity scores of our method with StateMask in all applica-\n7", "source": "pdfplumber_text", "token": 1316}, {"type": "image_analysis", "text": "\n[图片分析 (page_7_1765335032.jpg)]:\n图片:<{这是一张来自PDF文档第7页的文本页面，内容属于一篇关于强化学习（Reinforcement Learning）研究论文的一部分。该页面主要描述了实验设计与结果分析，具体包括以下几个方面：\n\n1. **实验设计**：\n   - **Experiment II**：比较使用作者提出的方法、PPO fine-tuning、StateMask从关键步骤微调以及Jump-Start Reinforcement Learning等方法在精炼（refining）后的智能体性能。\n   - **Experiment III**：探讨不同解释方法（Random, StateMask, 作者方法）生成的解释质量如何影响下游精炼过程，并比较精炼后智能体的表现。\n   - **Experiment IV**：验证所提方法的通用性，使用Soft Actor-Critic (SAC) 预训练智能体，并通过生成对抗模仿学习（GAIL）获得策略网络，再进行精炼对比。\n   - **Experiment V**：测试超参数对精炼效果的影响，涉及两个主要超参数：p（控制混合初始状态分布）和λ（控制探索奖励），并评估α（控制掩码网络掩码比例）对解释方法的影响。\n\n2. **实验结果分析**：\n   - 提到“Fidelity and Efficiency of Explanation”部分，将作者方法与StateMask在所有应用中的保真度得分进行比较。\n   - 指出在稀疏奖励任务中（如Mujoco游戏），作者方法优于其他基线方法，具有更高的最终性能和精炼效率。\n   - 强调其方法在处理关键状态时能帮助智能体突破瓶颈，尤其在SparseHopper和SparseHalfCheetah游戏中表现显著。\n\n3. **补充信息引用**：\n   - 多次提及附录中的图表和表格，例如Figure 5（Appendix C.3）、Table 4（Appendix C.3）、Figure 2、Appendix D、Appendix C.5等，用于支持实验结论。\n\n4. **关键术语与技术点**：\n   - 使用了诸如“explanation method”、“mask network”、“refining process”、“exploration bonus”、“critical steps”等术语。\n   - 提及了相关算法：PPO、StateMask、Jump-Start RL、SAC、GAIL。\n\n5. **整体特征**：\n   - 页面为学术论文格式，包含段落式文字说明，无流程图或数据图表。\n   - 文章重点在于方法的有效性和泛化能力验证，强调其在提升强化学习训练效率方面的贡献。\n\n6. **图片标号**：\n   - 本页未包含图像，仅含文本内容，因此无图片编号或图表标识。\n\n综上所述，此页面是论文中关于实验设置与初步结果分析的部分，旨在展示所提方法在多个维度上的优越性，但本身不包含可视化的流程图或数据图表。}>\n", "is_scanned_fallback": false, "token": 632}]}, {"page_num": 8, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nTable 1. Agent Refining Performance—“No Refine” indicates the target agent’s performance before refining. For the first group of\nexperiments (left), we fixed the explanation method to our method (mask network) and varied the refining methods. For the second\ngroup of experiments (right), we fixed the refining method to our method and varied the explanation methods. We report the mean value\n(standard deviations) of the final reward after refining. A higher value is better.\nFix Explanation; Vary Refine Methods Fix Refine; Vary Explanation Methods\nTask No Refine\nPPO JSRL StateMask-R Ours Random StateMask Ours\nHopper 3559.44 (19.15) 3638.75 (16.67) 3635.08 (9.82) 3652.06 (8.63) 3663.91 (20.98) 3648.98 (39.06) 3661.86 (19.95) 3663.91 (20.98)\nWalker2d 3768.79 (18.68) 3965.63 (9.46) 3963.57 (6.73) 3966.96 (3.39) 3982.79 (3.15) 3969.64(6.38) 3982.67 (5.55) 3982.79 (3.15)\nReacher -5.79 (0.73) -3.04 (0.04) -3.23 (0.26) -3.45 (0.32) -2.66 (0.03) -3.11 (0.42) -2.69 (0.28) -2.66 (0.03)\nHalfCheetah 2024.09 (28.34) 2133.31 (4.11) 2128.04 (0.91) 2085.28 (1.92) 2138.89 (3.22) 2132.01 (0.76) 2136.23 (0.49) 2138.89 (3.22)\nSelfish Mining 14.36 (0.24) 14.93 (0.45) 14.88 (0.51) 14.53 (0.33) 16.56 (0.63) 15.09 (0.28) 16.49 (0.46) 16.56 (0.63)\nCage Challenge 2 -23.64 (0.27) -23.58 (0.37) -22.97 (0.57) -26.98 (0.84) -20.02 (0.32) -25.94 (2.34) -20.07 (1.33) -20.02 (0.32)\nAuto Driving 10.30 (2.25) 13.37 (3.10) 11.26 (3.66) 7.62 (1.77) 17.03 (1.65) 11.72 (1.77) 16.28 (2.33) 17.03 (1.65)\nMalware Mutation 42.20 (6.86) 49.33 (8.59) 43.10 (7.24) 50.13 (8.14) 57.53 (8.71) 48.60 (7.60) 57.16 (8.51) 57.53 (8.71)\nSparseHopper SparseHalfCheetah SparseHopper SparseHalfCheetah\n(a) Fix Explanation; Vary Refine (b) Fix Refine; Vary Explanation\nFigure 2. Agent Refining Performance in two Sparse MuJoCo Games—For Group (a), we fix the explanation method to our method\n(mask network) if needed while varying refining methods. For Group (b), we fix the refining method to our method while varying the\nexplanation methods.\nthe bottleneck based on the identified critical states of the\nfailure.\nRefining based on Different Explanations. To examine\nhow the quality of explanation affects the downstream re-\nfining process, we present Table 1. We run our proposed\nrefining method based on the critical steps identified by\nours and Random. We have two main observations. First, Pre-train the policy(1M steps) Refine the policy(1M steps)\nusing the explanation generated by our mask network, the\nFigure 3. SAC Agent Refining Performance in Hopper Game\nrefining achieves the best outcome across all applications.\n—In the left part, we show the training curve of obtaining a pre-\nSecond, using the explanation generated by our explanation\ntrained policy through the SAC algorithm. In the right part, we\nsignificantly outperforms the random baseline. This aligns\nshow the refining curves of different methods.\nwith our theoretical analysis that our refining framework\nprovides a tighter bound for the sub-optimality.\nAppendix C.3. We have three main observations.\nRefining a Pre-trained Agent of Other Algorithms. To\nFirst, p controls the mixing ratio of critical states (identified\nshow that our framework is general to refine pre-trained\nby the explanation method) and the initial state distribution\nagents that were not trained by PPO algorithms, we do ex-\nfor refining. The performance is low when p = 0 (all\nperiments on refining a SAC agent in the Hopper game.\nstarting from the default initial distribution) or p = 1 (all\nFigure 3 demonstrates the advantage of our refining method\nstarting from the identified critical states). The performance\nagainst other baselines when refining a SAC agent. Addi-\nhas significant improvements when 0 < p < 1, i.e., using a\ntionally, we observe that fine-tuning the DRL agent with\nmixed initial state distribution. Across all applications, we\nthe SAC algorithm still suffers from the training bottleneck\nobserve that setting p to 0.25 or 0.5 is most beneficial. A\nwhile switching to the PPO algorithm provides an opportu-\nmixed initial distribution can help eliminate the problem of\nnity to break through the bottleneck. We provide the refining\ncurves when varying hyper-parameters p and λ in Appendix overfitting.\nC.3. Second, as long as λ > 0 (thereby enabling exploration),\nthere is a noticeable improvement in performance, highlight-\nImpact of Hyper-parameters. Due to space limit, we\ning the importance of exploration in refining the pre-trained\nprovide the sensitivity of hyper-parameters p, λ, and α in\nagent. The result is less sensitive to the specific value of\n8", "source": "pdfplumber_text", "token": 1656}, {"type": "table", "text": "\n[Detected Table]\n| NoRefine | FixExplanation;VaryRefineMethods |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- |\n|  | PPO | JSRL | StateMask-R | Ours | Random | StateMask |\n| 3559.44(19.15) | 3638.75(16.67) | 3635.08(9.82) | 3652.06(8.63) | 3663.91(20.98) | 3648.98(39.06) | 3661.86(19.95) |\n| 3768.79(18.68) | 3965.63(9.46) | 3963.57(6.73) | 3966.96(3.39) | 3982.79(3.15) | 3969.64(6.38) | 3982.67(5.55) |\n| -5.79(0.73) | -3.04(0.04) | -3.23(0.26) | -3.45(0.32) | -2.66(0.03) | -3.11(0.42) | -2.69(0.28) |\n| 2024.09(28.34) | 2133.31(4.11) | 2128.04(0.91) | 2085.28(1.92) | 2138.89(3.22) | 2132.01(0.76) | 2136.23(0.49) |\n| 14.36(0.24) | 14.93(0.45) | 14.88(0.51) | 14.53(0.33) | 16.56(0.63) | 15.09(0.28) | 16.49(0.46) |\n| -23.64(0.27) | -23.58(0.37) | -22.97(0.57) | -26.98(0.84) | -20.02(0.32) | -25.94(2.34) | -20.07(1.33) |\n| 10.30(2.25) | 13.37(3.10) | 11.26(3.66) | 7.62(1.77) | 17.03(1.65) | 11.72(1.77) | 16.28(2.33) |\n| 42.20(6.86) | 49.33(8.59) | 43.10(7.24) | 50.13(8.14) | 57.53(8.71) | 48.60(7.60) | 57.16(8.51) |\n\n", "source": "pdfplumber_table", "token": 843}, {"type": "table", "text": "\n[Detected Table]\n| SparseHopper |  |  | SparseHopper |  |  |\n| --- | --- | --- | --- | --- | --- |\n\n", "source": "pdfplumber_table", "token": 36}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  |  |  |  |  |\n\n", "source": "pdfplumber_table", "token": 69}, {"type": "image_analysis", "text": "\n[图片分析 (page_8_1765335052.jpg)]:\n图片:<{\n  \"type\": \"数据图表与表格组合\",\n  \"description\": \"该图片来自论文的第8页，包含一个表格（Table 1）、两个子图（Figure 2）和一个双图（Figure 3），用于展示强化学习中基于解释的策略优化方法（RICE）在不同任务上的性能表现。\",\n  \"tables\": [\n    {\n      \"table_id\": \"Table 1\",\n      \"title\": \"Agent Refining Performance\",\n      \"description\": \"比较了在固定解释方法或精炼方法的情况下，不同精炼策略对代理性能的影响。数据分为两组：第一组固定解释方法为本文提出的mask network，变化精炼方法；第二组固定精炼方法为本文方法，变化解释方法。\",\n      \"columns\": [\n        \"Task\",\n        \"No Refine\",\n        \"Fix Explanation: Vary Refine Methods\",\n        \"Fix Refine: Vary Explanation Methods\"\n      ],\n      \"data\": [\n        {\n          \"Task\": \"Hopper\",\n          \"No Refine\": \"359.44 (19.15)\",\n          \"Fix Explanation: Vary Refine Methods\": {\n            \"PPO\": \"3638.75 (16.67)\",\n            \"TSRL\": \"3653.08 (9.82)\",\n            \"StateMask-R\": \"3652.06 (8.63)\",\n            \"Ours\": \"3663.91 (20.98)\"\n          },\n          \"Fix Refine: Vary Explanation Methods\": {\n            \"Random\": \"3648.98 (39.08)\",\n            \"StateMask\": \"3661.86 (19.95)\",\n            \"Ours\": \"3663.91 (20.98)\"\n          }\n        },\n        {\n          \"Task\": \"Walker2d\",\n          \"No Refine\": \"3768.79 (18.68)\",\n          \"Fix Explanation: Vary Refine Methods\": {\n            \"PPO\": \"3965.63 (9.46)\",\n            \"TSRL\": \"3963.57 (6.75)\",\n            \"StateMask-R\": \"3966.96 (3.39)\",\n            \"Ours\": \"3982.79 (3.15)\"\n          },\n          \"Fix Refine: Vary Explanation Methods\": {\n            \"Random\": \"3969.64(6.38)\",\n            \"StateMask\": \"3982.67 (5.55)\",\n            \"Ours\": \"3982.79 (3.15)\"\n          }\n        },\n        {\n          \"Task\": \"Reacher\",\n          \"No Refine\": \"-5.79 (0.73)\",\n          \"Fix Explanation: Vary Refine Methods\": {\n            \"PPO\": \"-3.04 (0.04)\",\n            \"TSRL\": \"-3.23 (0.26)\",\n            \"StateMask-R\": \"-3.45 (0.32)\",\n            \"Ours\": \"-2.66 (0.03)\"\n          },\n          \"Fix Refine: Vary Explanation Methods\": {\n            \"Random\": \"-3.11 (0.42)\",\n            \"StateMask\": \"-2.69 (0.28)\",\n            \"Ours\": \"-2.66 (0.03)\"\n          }\n        },\n        {\n          \"Task\": \"HalfCheetah\",\n          \"No Refine\": \"2024.09 (28.34)\",\n          \"Fix Explanation: Vary Refine Methods\": {\n            \"PPO\": \"2133.31 (4.11)\",\n            \"TSRL\": \"2128.04 (0.91)\",\n            \"StateMask-R\": \"2085.28 (1.92)\",\n            \"Ours\": \"2138.89 (3.22)\"\n          },\n          \"Fix Refine: Vary Explanation Methods\": {\n            \"Random\": \"2132.01 (0.76)\",\n            \"StateMask\": \"2136.23 (0.49)\",\n            \"Ours\": \"2138.89 (3.22)\"\n          }\n        },\n        {\n          \"Task\": \"Sclfish Mining\",\n          \"No Refine\": \"14.36 (0.24)\",\n          \"Fix Explanation: Vary Refine Methods\": {\n            \"PPO\": \"14.93 (0.45)\",\n            \"TSRL\": \"14.88 (0.51)\",\n            \"StateMask-R\": \"14.53 (0.33)\",\n            \"Ours\": \"16.56 (0.63)\"\n          },\n          \"Fix Refine: Vary Explanation Methods\": {\n            \"Random\": \"15.09 (0.28)\",\n            \"StateMask\": \"16.49 (0.46)\",\n            \"Ours\": \"16.56 (0.63)\"\n          }\n        },\n        {\n          \"Task\": \"Cage Challenge 2\",\n          \"No Refine\": \"-23.64 (2.27)\",\n          \"Fix Explanation: Vary Refine Methods\": {\n            \"PPO\": \"-23.58 (0.37)\",\n            \"TSRL\": \"-22.97 (0.57)\",\n            \"StateMask-R\": \"-26.98 (0.84)\",\n            \"Ours\": \"-20.02 (0.32)\"\n          },\n          \"Fix Refine: Vary Explanation Methods\": {\n            \"Random\": \"-25.94 (2.34)\",\n            \"StateMask\": \"-20.07 (1.33)\",\n            \"Ours\": \"-20.02 (0.32)\"\n          }\n        },\n        {\n          \"Task\": \"Auto Driving\",\n          \"No Refine\": \"10.30 (2.25)\",\n          \"Fix Explanation: Vary Refine Methods\": {\n            \"PPO\": \"13.37 (3.10)\",\n            \"TSRL\": \"11.26 (3.66)\",\n            \"StateMask-R\": \"7.62 (1.77)\",\n            \"Ours\": \"17.03 (1.65)\"\n          },\n          \"Fix Refine: Vary Explanation Methods\": {\n            \"Random\": \"11.72 (1.77)\",\n            \"StateMask\": \"16.28 (2.33)\",\n            \"Ours\": \"17.03 (1.65)\"\n          }\n        },\n        {\n          \"Task\": \"Malware Mutation\",\n          \"No Refine\": \"42.20 (6.86)\",\n          \"Fix Explanation: Vary Refine Methods\": {\n            \"PPO\": \"49.33 (8.98)\",\n            \"TSRL\": \"43.10 (7.24)\",\n            \"StateMask-R\": \"50.13 (8.14)\",\n            \"Ours\": \"57.53 (8.71)\"\n          },\n          \"Fix Refine: Vary Explanation Methods\": {\n            \"Random\": \"48.60 (7.60)\",\n            \"StateMask\": \"57.16 (8.51)\",\n            \"Ours\": \"57.53 (8.71)\"\n          }\n        }\n      ],\n      \"key_insights\": [\n        \"在所有任务中，'Ours' 方法均优于其他基线方法（如 PPO、TSRL、StateMask-R、Random）。\",\n        \"当使用作者提出的方法进行解释时，无论精炼方法如何，性能都显著提升。\",\n        \"‘Ours’ 在 'Malware Mutation' 和 'Auto Driving' 等任务上表现出最大增益。\"\n      ]\n    }\n  ],\n  \"figures\": [\n    {\n      \"figure_id\": \"Figure 2\",\n      \"title\": \"Agent Refining Performance in two Sparse MuJoCo Games\",\n      \"description\": \"展示了在两个稀疏奖励的MuJoCo游戏中，固定解释方法或精炼方法时，不同方法的性能随训练步数的变化曲线。\",\n      \"subfigures\": [\n        {\n          \"label\": \"(a) Fix Explanation; Vary Refine\",\n          \"plots\": [\n            {\n              \"method\": \"PPO Finetune\",\n              \"color\": \"green\",\n              \"trend\": \"上升趋势，但波动较大\"\n            },\n            {\n              \"method\": \"StateMask-R\",\n              \"color\": \"blue\",\n              \"trend\": \"稳步上升，后期趋于平缓\"\n            },\n            {\n              \"method\": \"Ours\",\n              \"color\": \"red\",\n              \"trend\": \"快速上升并保持最高水平\"\n            },\n            {\n              \"method\": \"SRIL\",\n              \"color\": \"orange\",\n              \"trend\": \"缓慢上升，低于Ours\"\n            }\n          ],\n          \"x_axis\": \"Training Steps\",\n          \"y_axis\": \"Reward\",\n          \"observation\": \"Ours 方法在两种任务（SparseHopper, SparseHalfCheetah）中均达到最高且最稳定的奖励。\"\n        },\n        {\n          \"label\": \"(b) Fix Refine; Vary Explanation\",\n          \"plots\": [\n            {\n              \"method\": \"Ours\",\n              \"color\": \"red\",\n              \"trend\": \"快速收敛至高奖励\"\n            },\n            {\n              \"method\": \"Random\",\n              \"color\": \"blue\",\n              \"trend\": \"初始较低，后期缓慢增长\"\n            }\n          ],\n          \"x_axis\": \"Training Steps\",\n          \"y_axis\": \"Reward\",\n          \"observation\": \"使用作者提出的解释方法（Ours）明显优于随机解释（Random），尤其是在早期阶段。\"\n        }\n      ]\n    },\n    {\n      \"figure_id\": \"Figure 3\",\n      \"title\": \"SAC Agent Refining Performance in Hopper Game\",\n      \"description\": \"展示在Hopper游戏中，SAC预训练代理的精炼过程及其与其他方法的对比。\",\n      \"subfigures\": [\n        {\n          \"label\": \"Left Plot\",\n          \"title\": \"Pre-train the policy (1M steps)\",\n          \"plots\": [\n            {\n              \"method\": \"SAC\",\n              \"color\": \"purple\",\n              \"trend\": \"从低奖励开始，逐步上升至约3000\"\n            }\n          ],\n          \"x_axis\": \"Training Steps\",\n          \"y_axis\": \"Reward\",\n          \"observation\": \"SAC算法在1M步内成功训练出一个基础策略。\"\n        },\n        {\n          \"label\": \"Right Plot\",\n          \"title\": \"Refine the policy (1M steps)\",\n          \"plots\": [\n            {\n              \"method\": \"PPO Finetune\",\n              \"color\": \"blue\",\n              \"trend\": \"初期快速上升，随后下降\"\n            },\n            {\n              \"method\": \"StateMask-R\",\n              \"color\": \"green\",\n              \"trend\": \"平稳上升，但最终奖励较低\"\n            },\n            {\n              \"method\": \"Ours\",\n              \"color\": \"red\",\n              \"trend\": \"持续稳定上升，达到最高奖励\"\n            },\n            {\n              \"method\": \"SAC Finetune\",\n              \"color\": \"orange\",\n              \"trend\": \"小幅上升后停滞\"\n            },\n            {\n              \"method\": \"SRIL\",\n              \"color\": \"magenta\",\n              \"trend\": \"波动较大，未达理想水平\"\n            }\n          ],\n          \"x_axis\": \"Refinement Steps\",\n          \"y_axis\": \"Reward\",\n          \"observation\": \"Ours 方法在精炼阶段表现最佳，显示出更强的泛化能力和稳定性。\"\n        }\n      ],\n      \"additional_info\": \"文中提到通过调整超参数 p（混合比例）可以避免过拟合，p=0.25~0.5 最优；λ>0 启用探索有助于提升性能。\"\n    }\n  ],\n  \"overall_insights\": [\n    \"本文提出的 RICE 框架在多个任务上显著优于现有方法，尤其在精炼阶段表现出更强的鲁棒性和性能提升。\",\n    \"解释的质量对下游精炼过程有显著影响，作者的方法生成的解释能有效指导策略改进。\",\n    \"该框架不仅适用于PPO训练的代理，也适用于其他算法（如SAC）的预训练代理。\",\n    \"超参数 p 和 λ 对性能有重要影响，推荐使用混合初始状态分布和适度探索。\"\n  ]\n}>\n", "is_scanned_fallback": false, "token": 2804}]}, {"page_num": 9, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nλ. In general, a λ value of 0.01 yields good performance 6. Conclusion\nacross all four applications.\nIn this paper, we present RICE to break through bottle-\nThird, recall that the hyper-parameter α is to control the necks of reinforcement learning training with explanation.\nbonus of blinding the target agent when training the mask We propose an alternative design of StateMask to provide\nnetwork. We vary α from {0.01, 0.001, 0.0001} and find high-fidelity explanations for DRL agents’ behaviors, by\nthat our explanation method is not that sensitive to α. identifying critical time steps that contribute the most to\nthe agent’s success/failure. We encourage the agent to ex-\n5. Discussion plore starting from a mixture of default initial states and the\nidentified critical states. Compared with existing refining\nApplicability. RICE is suitable for DRL applica- strategies, we empirically show that our method brings the\ntions that are trained within controllable environment largest improvement after refining with theoretical guaran-\n(e.g., simulators), in order to generate explanations. In fact, tees.\nmost of today’s DRL applications rely on some form of\nsimulator for their training. For example, for safety-critical\nAcknowledgements\napplications such as autonomous driving, DRL agents are\nusually designed, trained, and tested in a simulated environ- This project was supported in part by Northwestern Univer-\nment first before moving them to real-world testing. Simu- sity TGS Fellowship and NSF Grants 2225234, 2225225,\nlation platforms broadly include Carla (Dosovitskiy et al., 2229876, 1955719, and 2055233.\n2017) and MetaDrive (Li et al., 2022) which have been used\nto facilitate the training of DRL agents (Zhang et al., 2021;\nImpact Statement\nWang et al., 2023; Peng et al., 2022). Therefore, RICE\nshould be applicable to such DRL systems (especially dur- This paper presents work whose goal is to advance the field\ning their development phase) for refining a pre-trained DRL of reinforcement learning with explanation. There are many\nagent. potential social impacts of our work. Our approach provides\na feasible solution to break through the training bottlenecks\nWarm Start vs Cold Start. As is mentioned in Section 3,\nof reinforcement learning with explanation, which is an\nour method requires a “warm start” setting, i.e., the agent\nautomatic process and saves manual effort.\nhas good coverage of the state distribution of the optimal\npolicy. Even if the agent has good coverage of the state However, it is also worth noting the potential negative soci-\ndistribution, it does not necessarily mean that the agent has etal impacts of our work. Some of the real-world applica-\nalready learned a good policy due to the potential of choos- tions we select such as malware mutation can create attack\ning wrong actions (Uchendu et al., 2023). Therefore, the examples that may bring additional ethical concerns. In the\ntraining bottleneck can still exist under a good coverage of realm of security research, the ultimate goal of these tasks\nthe state distribution. In contrast, Our method does not work is to generate stronger testing cases to enhance the defense,\nwell in a “cold start” setting, i.e., when the state coverage and it is standard practice. Take malware mutation as an\nof the pre-trained policy is extremely poor. In that case, example, the produced samples can be used to proactively\nstep-level explanation methods cannot give useful help and improve the robustness and effectiveness of malware de-\nour method is actually equivalent to the RND method 3. tection systems (e.g., through adversarial training), thereby\nbenefiting cybersecurity defense (Yang et al., 2017).\nCritical State Filtering. Though RICE identifies critical\nstates based on their necessity for achieving good outcomes,\nReferences\nit does not fully consider their importance for further agent\nlearning. For instance, a state might be deemed critical,\nGitHub - bfilar/malware rl: Malware Bypass Research us-\nyet the trained agent could have already converged to the ing Reinforcement Learning — github.com. https:\noptimal action for that state. In such cases, resetting the //github.com/bfilar/malware_rl, a.\nenvironment to this state doesn’t significantly benefit the\nlearning process. Future work could explore additional\nfiltering of critical states using metrics such as policy con- GitHub - cage-challenge/cage-challenge-2: TTCP CAGE\nvergence or temporal difference (TD) errors, which may Challenge 2 — github.com. https://github.com/\nhelp concentrate efforts and accelerate refinement. cage-challenge/cage-challenge-2, b.\n3We provide an example of Mountain Car game in Appendix E\nto illustrate this limitation. GitHub - john-cardiff/-cyborg-cage-2 — github.com.\nhttps://github.com/john-cardiff/\n-cyborg-cage-2, c.\n9", "source": "pdfplumber_text", "token": 1128}, {"type": "image_analysis", "text": "\n[图片分析 (page_9_1765335129.jpg)]:\n图片:<{这是一张来自学术论文第9页的文本内容，包含“Discussion”、“Conclusion”、“Acknowledgements”、“Impact Statement”和“References”等部分。主要内容如下：\n\n- **Discussion** 部分讨论了 RICE 方法的适用性，指出其适用于在可控环境中训练的深度强化学习（DRL）应用，例如模拟器（如 Carla 和 MetaDrive）。还探讨了“Warm Start vs Cold Start”的区别，强调 RICE 在“warm start”设置下有效，但在“cold start”时表现不佳。此外，提到“Critical State Filtering”存在局限性，即虽然能识别关键状态，但未充分考虑这些状态对后续学习的重要性。\n\n- **Conclusion** 总结了 RICE 方法通过引入解释机制来突破强化学习训练瓶颈，提出了一种名为 StateMask 的替代设计，用于生成高保真度的 DRL 智能体行为解释。该方法通过识别对成功或失败贡献最大的关键时间步，鼓励智能体从默认初始状态和已识别的关键状态混合中进行探索，实验证明其在理论保证的基础上显著优于现有精炼策略。\n\n- **Acknowledgements** 提到该项目得到了西北大学 TGS 奖学金和 NSF 资助（编号：2225234, 2225225, 2229876, 1955719, 2055233）的支持。\n\n- **Impact Statement** 描述了该研究的社会影响，包括推动可解释强化学习的发展、自动化训练过程以节省人力，并指出潜在的负面社会影响，例如恶意软件突变可能带来的伦理问题。同时，强调其在网络安全领域的积极应用，如增强恶意软件检测系统的鲁棒性和有效性。\n\n- **References** 列出了三个 GitHub 项目链接：\n  1. `https://github.com/bfilar/malware_rl` —— 使用强化学习绕过恶意软件的研究。\n  2. `https://github.com/cage-challenge/cage-challenge-2` —— TTCP CAGE Challenge 2。\n  3. `https://github.com/john-cardiff/-cyborg-cage-2` —— cyborg-cage-2 项目。\n\n页面底部标有页码“9”。}>\n", "is_scanned_fallback": false, "token": 517}]}, {"page_num": 10, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nGitHub - roibarzur/pto-selfish-mining: Code repository Cheng, Z., Wu, X., Yu, J., Sun, W., Guo, W., and Xing,\nfor technical papers about selfish mining analysis. — X. Statemask: Explaining deep reinforcement learning\ngithub.com. https://github.com/roibarzur/ through state mask. In Proc. of NeurIPS, 2023.\npto-selfish-mining, d.\nDosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and\nMountain car continuous. https://mgoulao. Koltun, V. CARLA: An open urban driving simulator. In\ngithub.io/gym-docs/environments/ Proc. of CoRL, pp. 1–16, 2017.\nclassic_control/mountain_car_\ncontinuous/. Accessed: 2024-05-24. drive Contributors, D. DI-drive: OpenDILab deci-\nsion intelligence platform for autonomous driving\nAgarwal, A., Henaff, M., Kakade, S., and Sun, W. Pc-\nsimulation. https://github.com/opendilab/\npg: Policy cover directed exploration for provable policy\nDI-drive, 2021.\ngradient learning. Proc. of NeurIPS, 2020.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O.,\nAgarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G.\nand Clune, J. Go-explore: a new approach for hard-\nOn the theory of policy gradient methods: Optimality,\nexploration problems. arXiv preprint arXiv:1901.10995,\napproximation, and distribution shift. Journal of Machine\n2019.\nLearning Research, 2021.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C.,\nClune, J. First return, then explore. Nature, 2021.\nand Bellemare, M. Reincarnating reinforcement learning:\nReusing prior computation to accelerate progress. In Proc.\nErez, T., Tassa, Y., and Todorov, E. Infinite-horizon\nof NeurIPS, 2022.\nmodel predictive control for periodic tasks with contacts.\nAnderson, H. S., Kharkar, A., Filar, B., Evans, D., and Roth, Robotics: Science and systems VII, pp. 73, 2012.\nP. Learning to evade static pe machine learning mal-\nware models via reinforcement learning. arXiv preprint Eyal, I. and Sirer, E. G. Majority is not enough: Bitcoin\narXiv:1801.08917, 2018. mining is vulnerable. Communications of the ACM, 61\n(7):95–102, 2018.\nBar-Zur, R., Abu-Hanna, A., Eyal, I., and Tamar, A. Werl-\nman: To tackle whale (transactions), go deep (rl). In Proc. Eysenbach, B., Salakhutdinov, R., and Levine, S. The infor-\nof IEEE S&P, 2023. mation geometry of unsupervised reinforcement learning.\nIn Proc. of ICLR, 2021.\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T.,\nSaxton, D., and Munos, R. Unifying count-based ex- Fu, J., Luo, K., and Levine, S. Learning robust rewards with\nploration and intrinsic motivation. In Proc. of NeurIPS, adverserial inverse reinforcement learning. In Proc. of\n2016. ICLR, 2018.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Explo-\nGuan, L., Verma, M., Guo, S. S., Zhang, R., and Kambham-\nration by random network distillation. In Proc. of ICLR,\npati, S. Widening the pipeline in human-guided reinforce-\n2018.\nment learning with explanation and context-aware data\naugmentation. In Proc.of NeurIPS, 2021.\nCAGE. Ttcp cage challenge 2. In Proc. of AAAI-22 Work-\nshop on Artificial Intelligence for Cyber Security (AICS),\nGuo, W., Wu, X., Khan, U., and Xing, X. Edge: Explaining\n2022.\ndeep reinforcement learning policies. In Proc. of NeurIPS,\nCai, X.-Q., Ding, Y.-X., Chen, Z., Jiang, Y., Sugiyama, 2021.\nM., and Zhou, Z.-H. Seeing differently, acting similarly:\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-\nHeterogeneously observable imitation learning. In Proc.\ncritic: Off-policy maximum entropy deep reinforcement\nof ICLR, 2022.\nlearning with a stochastic actor. In Proc. of ICML, pp.\nCai, X.-Q., Zhang, Y.-J., Chiang, C.-K., and Sugiyama, 1861–1870, 2018.\nM. Imitation learning from vague feedback. In Proc. of\nNeurIPS, 2023. Ho, J. and Ermon, S. Generative adversarial imitation learn-\ning. In Proc. of NeurIPS, 2016.\nChang, J. D., Brantley, K., Ramamurthy, R., Misra, D., and\nSun, W. Learning to generate better than your llm. arXiv Kakade, S. and Langford, J. Approximately optimal approx-\npreprint arXiv:2306.11816, 2023. imate reinforcement learning. In Proc. of ICML, 2002.\n10", "source": "pdfplumber_text", "token": 1410}, {"type": "image_analysis", "text": "\n[图片分析 (page_10_1765335148.jpg)]:\n图片:<该图片为PDF文档的第10页，内容为一篇学术论文的参考文献列表。页面顶部标题为“RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation”，表明这是关于强化学习中训练瓶颈突破的研究论文。页面主体部分列出了一系列相关研究工作的引用，包括作者姓名、论文标题、会议或期刊名称（如NeurIPS、ICLR、ICML等）、年份以及部分链接信息（如GitHub仓库地址）。这些参考文献涵盖了强化学习、策略梯度方法、探索机制、模仿学习、解释性AI等多个方向，体现了该论文在理论与应用层面的广泛背景支持。此外，文中还包含一些技术资源链接，例如Mountain Car环境的GitHub地址和DI-drive自动驾驶仿真平台的链接。整体来看，此页无图表、流程图或数据可视化内容，仅包含结构化的文本型参考文献信息。>\n", "is_scanned_fallback": false, "token": 215}]}, {"page_num": 11, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nLi, Q., Peng, Z., Feng, L., Zhang, Q., Xue, Z., and Zhou, Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nB. Metadrive: Composing diverse driving scenarios for Klimov, O. Proximal policy optimization algorithms.\ngeneralizable reinforcement learning. IEEE Transactions arXiv preprint arXiv:1707.06347, 2017.\non Pattern Analysis and Machine Intelligence, 2022.\nSundararajan, M., Taly, A., and Yan, Q. Axiomatic attribu-\nLi, Q., Zhai, Y., Ma, Y., and Levine, S. Understanding the tion for deep networks. In Proc. of ICML, 2017.\ncomplexity gains of single-task rl with a curriculum. In\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics\nProc. of ICML, pp. 20412–20451, 2023.\nengine for model-based control. In Proc. of IROS, 2012.\nMazoure, B., Doan, T., Durand, A., Hjelm, R. D., and\nUchendu, I., Xiao, T., Lu, Y., Zhu, B., Yan, M., Simon, J.,\nPineau, J. Leveraging exploration in off-policy algorithms\nBennice, M., Fu, C., Ma, C., Jiao, J., et al. Jump-start\nvia normalizing flows. In Proc. of CoRL, 2019.\nreinforcement learning. In Proc. of ICML, 2023.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nVan Waveren, S., Pek, C., Tumova, J., and Leite, I. Correct\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint me if i’m wrong: Using non-experts to repair reinforce-\narXiv:1312.5602, 2013. ment learning policies. In Proc. of HRI, 2022.\nOh, J., Chockalingam, V., Lee, H., et al. Control of memory, Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,\nactive perception, and action in minecraft. In Proc. of Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,\nICML, 2016. T., Georgiev, P., et al. Grandmaster level in starcraft ii\nusing multi-agent reinforcement learning. Nature, 2019.\nOh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learn-\ning. In Proc. of ICML, pp. 3878–3887, 2018. Vyas, S., Hannay, J., Bolton, A., and Burnap, P. P. Au-\ntomated cyber defence: A review. arXiv preprint\nOstrovski, G., Bellemare, M. G., Oord, A., and Munos, R. arXiv:2303.04926, 2023.\nCount-based exploration with neural density models. In\nProc. of ICML, 2017. Wang, X., Zhang, J., Hou, D., and Cheng, Y. Autonomous\ndriving based on approximate safe action. IEEE Transac-\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., tions on Intelligent Transportation Systems, 2023.\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance Weng, J., Chen, H., Yan, D., You, K., Duburcq, A., Zhang,\ndeep learning library. In Proc. of NeurIPS, 2019. M., Su, Y., Su, H., and Zhu, J. Tianshou: A highly\nmodularized deep reinforcement learning library. Journal\nPeng, Z., Li, Q., Liu, C., and Zhou, B. Safe driving via of Machine Learning Research, 2022.\nexpert guided policy optimization. In Proc. of CoRL,\n2022. Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. Policy\nfinetuning: Bridging sample-efficient offline and online\nPuri, N., Verma, S., Gupta, P., Kayastha, D., Deshmukh, S., reinforcement learning. In Proc. of NeurIPS, 2021.\nKrishnamurthy, B., and Singh, S. Explain your move:\nUnderstanding agent actions using specific and relevant Yang, W., Kong, D., Xie, T., and Gunter, C. A. Malware\nfeature attribution. In Proc. of ICLR, 2019. detection in adversarial settings: Exploiting feature evolu-\ntions and confusions in android apps. In Proc. of ACSAC,\nRaff, E., Barker, J., Sylvester, J., Brandon, R., Catanzaro, 2017.\nB., and Nicholas, C. Malware detection by eating a whole\nexe. arXiv preprint arXiv:1710.09435, 2017. Yu, J., Guo, W., Qin, Q., Wang, G., Wang, T., and Xing, X.\nAirs: Explanation for deep reinforcement learning based\nRaffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, security applications. In Proc. of USENIX Security, 2023.\nM., and Dormann, N. Stable-baselines3: Reliable rein-\nforcement learning implementations. Journal of Machine Zhang, Z., Liniger, A., Dai, D., Yu, F., and Van Gool, L.\nLearning Research, 2021. End-to-end urban driving by imitating a reinforcement\nlearning coach. In Proc. of ICCV, 2021.\nRashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell,\nS. Bridging offline reinforcement learning and imitation\nlearning: A tale of pessimism. In Proc. of NeurIPS, 2021.\nRen, Z., Dong, K., Zhou, Y., Liu, Q., and Peng, J. Explo-\nration via hindsight goal generation. In Proc. of NeurIPS,\n2019.\n11", "source": "pdfplumber_text", "token": 1566}, {"type": "image_analysis", "text": "\n[图片分析 (page_11_1765335155.jpg)]:\n图片:<该图片为PDF文档的第11页，内容为参考文献列表，包含多个与强化学习、深度学习、安全应用等相关的学术论文引用。每条引用包括作者姓名、论文标题、会议或期刊名称以及年份。例如：Li, Q. 等人的“Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning”发表于IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022；Mnih, V. 等人的“Playing atari with deep reinforcement learning”发表于arXiv preprint arXiv:1312.5602, 2013。页面底部标有页码“11”。>\n", "is_scanned_fallback": false, "token": 176}]}, {"page_num": 12, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nA. Proof of Theorem 3.3\nBased on the Performance Difference Lemma (Kakade & Langford, 2002), we have\n1\nη(π¯) − η(π) = E E Aπ (s, a) , (4)\n1 − γ s∼dπ ρ ¯ a∼π¯(·|s)\nwhere π is the policy of the target agent, π¯ is the perturbed policy, ρ is the initial distribution, and γ is the discount rate.\nNote that the perturbed policy π¯ is a mixture of the target agent’s policy π and a random policy πr (i.e., π¯(·|s) = π˜(ae =\n0|s)π(·|s) + π˜(ae = 1|s)πr(·|s)). Denote the probability of the mask network outputting 0 at state s as π˜(ae = 0|s) =\nξ(s) and the probability of the mask network outputting 1 at state s as π˜(ae = 1|s) = 1 − ξ(s) Given the fact that\nAπ (s, π(·|s)) = E Aπ(s, a) = 0, we have\na∼π(s)\n1\nη(π¯) − η(π) = E E Aπ (s, a)\n1 − γ s∼dπ ρ ¯ a∼π¯(·|s)\n1 (cid:88)\n= E π¯(a|s)Aπ (s, a)\n1 − γ\ns∼dπ\nρ\n¯\na\n1 (cid:88) 1 (cid:88)\n= E ξ(s)π(a|s)Aπ (s, a) + E (1 − ξ(s))πr(a|s)Aπ (s, a) (5)\n1 − γ\ns∼dπ\nρ\n¯\n1 − γ\ns∼dπ\nρ\n¯\na a\n1 1\n= E ξ(s)E Aπ (s, a) + E (1 − ξ(s))E Aπ (s, a)\n1 − γ s∼dπ ρ ¯ a∼π(·|s) 1 − γ s∼dπ ρ ¯ a∼πr(·|s)\n1\n= E (1 − ξ(s))E Aπ (s, a) ≤ 0.\n1 − γ\ns∼dπ\nρ\n¯ a∼πr(·|s)\nTherefore, we show that η(π¯) is upper bounded by η(π) given Assumption 3.1.\nB. Theoretical Guarantee\nB.1. Validation of Assumption 3.4 in a 2-state MDP\nIn a 2-state MDP, we have two different states, namely, s and s . The state distribution of any policy π follows\nA B\ndπ(s ) + dπ(s ) = 1. As such, the set of feasible state marginal distribution can be described by a line [(0, 1), (1, 0)]\nρ A ρ B\nin R2. Let’s denote vector s = [s , s ]. The expected total reward of a policy π can be represented as η(π) =<\nA B\ndπ(s), R(s) > (Eysenbach et al., 2021), where R(s) = [R(s ), R(s )]. Figure 4 shows the area of achievable state\nρ A B\ndistribution via the initial state distribution ρ (highlighted in orange).\nIt should be noted that not all the points in the line [(0, 1), (1, 0)] corresponded to a valid Markovian policy. However, for\nany convex combination of valid state occupancy measures, there exists a Markovian policy that has this state occupancy\nmeasure. As such, the policy search occurs within a convex polytope, essentially a segment (i.e., , marked in orange) along\nthis line. In Figure 4, we visualize R(s) as vectors starting at the origin. Since V πˆ(ρ) ≥ V π(ρ), We mark dπˆ(s) closer to\nρ\nR(s) (i.e., the inner product between dπˆ(s) and R(s) and is larger than dπ(s) and R(s)). The following theorem explains\nρ ρ\nhow we determine the location of the location of dπ∗(s) in Figure 4.\nρ\nTheorem B.1 (Fact 1 (Eysenbach et al., 2021)). For every state-dependent reward function, among the set of policies that\nmaximize that reward function is one that lies at a vertex of the state marginal polytope.\nAccording to Theorem B.1, dπ∗(s) located at either vertex in the orange segment. Since π∗ is the optimal policy, it lies\nρ\nat the vertex that has the larger inner product within R(s). Once the position of dπ∗(s) is determined, we can easily find\nρ\n(cid:13) (cid:13) (cid:13) (cid:13)\n(cid:13) dπ∗ (s) (cid:13) (cid:13) dπ∗ (s) (cid:13)\n(cid:13) ρ (cid:13) ≤ (cid:13) ρ (cid:13) based on Figure 4.\n(cid:13) dπ ρ ˆ(s) (cid:13) ∞ (cid:13) dπ ρ (s) (cid:13) ∞\nB.2. Proof of Lemma 3.5\nProof. Since our explanation method provides the importance of each state, we could view the sampling based on the state’s\nimportance as a reweighting of the state occupancy measure. Mathematically, it can be expressed as dπˆ(s) = dπ(s)w(s),\nρ ρ\n12", "source": "pdfplumber_text", "token": 1354}, {"type": "image_analysis", "text": "\n[图片分析 (page_12_1765335160.jpg)]:\n图片:<{这是一张来自学术论文PDF第12页的文本内容，主要包含数学推导、定理证明和理论分析，不包含图像或图表。内容涉及强化学习中的策略性能差异分析，具体包括：\n\n- **A. Theorem 3.3 的证明**：基于Performance Difference Lemma（Kakade & Langford, 2002），推导了目标策略π与扰动策略¯π之间的性能差η(¯π)−η(π)，并利用状态分布dπρ、动作价值函数Aπ(s,a)以及掩码网络输出概率ξ(s)进行展开，最终得出η(¯π) ≤ η(π)，在假设Assumption 3.1成立的前提下。\n\n- **B. 理论保证**：\n  - **B.1. 在2状态MDP中验证Assumption 3.4**：讨论了一个两状态马尔可夫决策过程（s_A 和 s_B），状态边际分布位于线段[(0,1), (1,0)]上。引入向量s=[s_A,s_B]表示状态分布，总期望奖励为η(π)=<dπρ(s),R(s)>，其中R(s)=[R(s_A),R(s_B)]。文中提到Figure 4展示了可达状态分布区域（橙色高亮），但该图未在本页显示。\n  - 引用Theorem B.1（Eysenbach et al., 2021）说明最优策略π*对应的dπ∗ρ(s)位于橙色线段的顶点，且其位置可通过与R(s)的内积最大化确定。\n  - 提出dπ∗ρ(s)的位置决定后，可计算||dπ∗ρ(s)/dπρ(s)||∞的值。\n\n- **B.2. Lemma 3.5 的证明**：提出解释方法赋予每个状态重要性权重w(s)，采样过程可视为对状态占用测度dπρ(s)的重加权，即d¯πρ(s)=dπρ(s)w(s)。\n\n关键信息提取：\n- 文档标题：RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n- 图片标号：Figure 4（提及但未显示）\n- 公式编号：(4)、(5)\n- 定理/引理引用：Theorem 3.3、Assumption 3.1、Theorem B.1、Lemma 3.5\n- 参考文献：Kakade & Langford (2002)，Eysenbach et al. (2021)\n\n整体特征：该页面以数学推导为主，结构清晰，逻辑严密，属于理论性较强的学术论文内容，重点在于建立强化学习策略优化的理论基础。}>\n", "is_scanned_fallback": false, "token": 632}]}, {"page_num": 13, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n𝑑𝑑(𝑠𝑠𝐵𝐵)\n1\n𝜋𝜋\n𝑑𝑑𝜌𝜌 (𝒔𝒔)\n𝜋𝜋�\n𝑑𝑑𝜌𝜌 (𝒔𝒔)∗\n𝜋𝜋\n𝑑𝑑𝜌𝜌 (𝒔𝒔)\n𝑅𝑅(𝒔𝒔)\n0 1 𝑑𝑑(𝑠𝑠𝐴𝐴)\nFigure 4. Visualization of state occupancy measures with respect to different policies and the reward function in a 2-state MDP.\nwhere πˆ is the equivalent policy of reweighting the original policy π and w(s) is the weight provided by the mask network.\nAlthough the mask network takes the current input state as input, it implicitly considers the current action as well, as detailed\nby StateMask (Cheng et al., 2023). Consequently, a more accurate formulation is dπˆ(s, a) = dπ(s, a)w(s, a), where w(s, a)\nρ ρ\nrepresent the weight assigned by mask network.\nRecall that our proposed explanation method is to randomize actions at non-critical steps, which essentially considers the\nvalue of Q = Qπ(s, a) − E [Qπ(s, a′)]. In fact, a larger Q indicates current time step is more critical to the\ndiff a′∈A diff\nagent’s final reward. Our mask network approximates the value of Q via the deep neural network to determine the\ndiff\nimportance of each step, which implies w(s, a) ∝ Q ∝ Qπ(s, a).\ndiff\nNext, we aim to prove that our MaskNet-based sampling approach is equivalent to sampling from a better policy πˆ.\nFirst, the equivalent policy πˆ after reweighting can be expressed as\ndπˆ(s, a) dπ(s, a)w(s, a) dπ(s)\nπˆ(a|s) = ρ = ρ = w(s, a)π(a|s) ρ . (6)\ndπˆ(s) dπˆ(s) dπˆ(s)\nρ ρ ρ\nFurther , we would like to show that if w(s, a) = f (Qπ(s, a)) where f (·) is a monotonic increasing function, πˆ is uniformly\nas good as,or better than π, i.e., V πˆ(s) ≥ V π(s).\nProposition B.2. Suppose two policies πˆ and π satisfy g (πˆ(a|s)) = g(π(a|s)) + h (s, Qπ(s, a)) where g(·) is a monotoni-\ncally increasing function, and h(s, ·) is monotonically increasing for any fixed s . Then we have V πˆ(s) ≥ V π(s), ∀s ∈ S.\nProof. For a given s, we partition the action set A into two subsets A and A .\n1 2\nA ≜ {a ∈ A|πˆ(a|s) ⩾ π(a|s)} .\n1\nA ≜ {a ∈ A|πˆ(a|s) < π(a|s)} .\n2\nThus, ∀a ∈ A , ∀a ∈ A , we have\n1 1 2 2\nh (s, Qπ(s, a )) = g (πˆ(a |s)) − g(π(a |s))\n1 1 1\n≥ 0\n(7)\n≥ g (πˆ(a |s)) − g(π(a |s))\n2 2\n= h (s, Qπ(s, a )) .\n2\nLet h (s, Qπ(s, a)) = Qπ(s, a). We can get Qπ(s, a ) ≥ Qπ(s, a ) which means we can always find q(s) ∈ R such that\n1 2\n13", "source": "pdfplumber_text", "token": 830}, {"type": "image_analysis", "text": "\n[图片分析 (page_13_1765335178.jpg)]:\n图片:<{这是一张来自论文第13页的图表，标题为“Figure 4. Visualization of state occupancy measures with respect to different policies and the reward function in a 2-state MDP.”（图4：在二状态马尔可夫决策过程（MDP）中，不同策略与奖励函数下的状态占用度量的可视化）。\n\n该图是一个二维坐标系中的几何图形，用于展示在两个状态的MDP中，不同策略下状态占用测度（state occupancy measures）的变化情况。具体分析如下：\n\n- 坐标轴：\n  - 横轴表示状态 $ s_A $ 的占用度量 $ d(s_A) $，范围从0到1。\n  - 纵轴表示状态 $ s_B $ 的占用度量 $ d(s_B) $，范围也从0到1。\n  - 两者满足 $ d(s_A) + d(s_B) = 1 $，因此所有点位于连接点 (0,1) 和 (1,0) 的对角线上。\n\n- 图形元素：\n  - 对角线代表所有可能的状态分布（即概率分布），因为两个状态的概率和为1。\n  - 从原点 (0,0) 出发有多条射线，分别指向对角线上的不同点，这些射线代表不同的策略下的状态占用分布。\n  - 其中两条射线被特别标注：\n    - 一条指向点 $ d^\\pi_p(s) $，表示原始策略 $ \\pi $ 下的状态占用度量。\n    - 另一条指向点 $ d^{\\tilde\\pi}_p(s) $，表示经过重加权后的策略 $ \\tilde\\pi $ 下的状态占用度量。\n    - 这两个点均位于对角线上，且 $ d^{\\tilde\\pi}_p(s) $ 更靠近 (1,0)，表明在新策略下状态 $ s_A $ 的占用更高。\n\n- 辅助线：\n  - 有一条蓝色斜线标记为 $ R(s) $，表示奖励函数在状态空间中的投影或偏好方向。这条线从原点出发，穿过对角线，其倾斜程度暗示了奖励函数对不同状态组合的偏好。\n\n- 关键信息：\n  - 图像展示了通过掩码网络（mask network）调整策略后，状态占用分布如何变化，从而影响整体性能。\n  - $ d^\\pi_p(s) $ 和 $ d^{\\tilde\\pi}_p(s) $ 的相对位置表明重加权策略倾向于增加某些状态的访问频率，以优化长期回报。\n\n- 图像编号：Figure 4\n- 文本描述：该图是用于说明RICE方法中策略重加权如何改变状态占用分布，进而提升强化学习训练效率的示意图。\n\n总结：此图为一个二维状态占用分布图，展示了在二状态MDP中不同策略（特别是原始策略 $ \\pi $ 和重加权策略 $ \\tilde\\pi $）下的状态占用度量差异，并通过奖励函数 $ R(s) $ 的方向性提示了策略优化的方向。它不包含具体数值数据，而是定性地表达了策略改进的过程。}>\n", "is_scanned_fallback": false, "token": 705}]}, {"page_num": 14, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nQπ(s, a ) ≥ q(s) ≥ Qπ(s, a ), ∀a ∈ A , ∀a ∈ A . Thus,\n1 2 1 1 2 2\n(cid:88) (cid:88)\nπˆ(a|s)Qπ(s, a) − π(a|s)Qπ(s, a)\na∈A a∈A\n(cid:88) (cid:88)\n= (πˆ (a |s) − π(a |s)) Qπ (s, a ) + (πˆ (a |s) − π(a |s)) Qπ(s, a )\n1 1 1 2 2 2\na1∈A1 a2∈A2\n(cid:88) (cid:88)\n≥ (πˆ (a |s) − π (a |s)) q(s) + (πˆ (a |s) − π (a |s)) q(s) (8)\n1 1 2 2\na1∈A1 a2∈A2\n(cid:88)\n= q(s) (π′(a|s) − π(a|s))\na∈A\n= 0.\nLet V (s) = V π(s). And we denote the value function of following πˆ for l steps then following π as V (s) =\n0 l\nE (cid:2) E (r + γV (s′)) (cid:3) if l ≥ 1.\na∼πˆ(.|s) s′,r|s,a l−1\nFirst, we observe that\nV (s) = E (cid:2) E (r + γV π(s′))]\n1 a∼πˆ(.|s) s′,r|s,a\n(cid:88)\n= πˆ(a|s)Qπ(s, a)\na∈A\n(9)\n(cid:88)\n⩾ π(a|s)Qπ(s, a)\na∈A\n= V (s).\n0\nBy induction, we assume V (s) ≥ V (s). Given that\nl l−1\nV (s) = E (cid:2) E (r + V (s′)) (cid:3) ,\nl+1 a∼πˆ s′,r|s,a l\nV (s) = E (cid:2) E (r + V (s′)) (cid:3) ,\nl a∼πˆ s′,r|s,a l−1\nwe have V (s) ≥ V (s).\nl+1 l\nTherefore, we can conclude that V (s) ≥ V (s), ∀l ≥ 0. We have V (s) ≥ V (s) which is V πˆ(s) ≥ V π(s).\nl+1 l ∞ 0\nBased on the Proposition B.2, if we choose g as a logarithmic function and h = log(w(s, a)) + log(dπ(s)) − log(dπˆ(s)),\nρ ρ\nwe can easily verify that our MaskNet-based sampling approach is equivalent to sampling from a better policy πˆ.\nB.3. Proof of Theorem 3.6\nProof. Given the fact that the refined policy π′ is converged, (i.e., the local one-step improvement is small\n(cid:104) (cid:105)\nE max Aπ′(s, a) < ϵ), we have\ns∼dπ′ a\nµ\nϵ >\n(cid:88) dπ′\n(s)\n(cid:104)\nmax\nAπ′\n(s, a)\n(cid:105)\nµ\na\ns∈S\n≥ min\n(cid:32) dπ\nµ\n′(s) (cid:33)\n(cid:88) dπ∗ (s) max Aπ′ (s, a)\ns dπ∗(s) ρ a (10)\nρ s\n≥ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) −1 (cid:88) dπ∗ (s)π∗(a|s)Aπ′ (s, a).\n(cid:13) dπ′ (cid:13) ρ\n(cid:13) µ (cid:13) ∞ s,a\nBased on the Performance Difference Lemma (Kakade & Langford, 2002), for two policies π∗, π′ and a state distribution ρ,\nthe performance difference is bounded by\n1 (cid:104) (cid:105)\nV\nπ∗\n(ρ) − V\nπ′\n(ρ) = E E\nAπ′\n(s, a) . (11)\n1 − γ\ns∼dπ\nρ\n∗ a∼π∗(.|s)\n14", "source": "pdfplumber_text", "token": 1099}, {"type": "image_analysis", "text": "\n[图片分析 (page_14_1765335203.jpg)]:\n图片:<{这是一张来自学术论文的页面（PDF Page 14），内容主要为强化学习理论的数学推导和定理证明。页面中包含多个公式（编号从(8)到(11)），涉及策略优化、价值函数、性能差异等概念。\n\n关键内容分析如下：\n\n1. 公式(8)：展示了在策略π和改进策略ˆπ之间的Q值差异，通过分解动作集合A₁和A₂，利用Q函数和状态分布q(s)进行不等式推导，最终得出差值为0，表明某种策略改进的合理性。\n\n2. 公式(9)：定义了V₁(s)作为遵循策略ˆπ一步后的期望回报，并通过比较与原始策略π的期望回报，证明了V₁(s) ≥ V₀(s)，即改进策略至少不劣于原策略。\n\n3. 归纳推理部分：通过归纳法证明对于任意l ≥ 0，有Vₗ₊₁(s) ≥ Vₗ(s)，从而推出极限价值函数V∞(s) ≥ V₀(s)，进一步说明改进策略优于原策略。\n\n4. 基于Proposition B.2的讨论：指出当g为对数函数且h定义为特定形式时，MaskNet采样方法等价于从更优策略ˆπ中采样。\n\n5. B.3节“Proof of Theorem 3.6”：\n   - 给出条件：精炼策略π′收敛，即局部单步改进小于ε。\n   - 推导出关于优势函数Aπ′的不等式（公式(10)），涉及状态分布dπ′_p和dπ*_p，以及最大优势值。\n   - 引用Performance Difference Lemma（Kakade & Langford, 2002）给出两个策略π*和π′在状态分布ρ下的性能差异上界（公式(11)）。\n\n整体特征：\n- 文本以数学符号和逻辑推理为主，属于强化学习领域的理论分析。\n- 包含关键术语如Q函数、价值函数V、策略π、优势函数A、状态分布d、性能差异等。\n- 使用标准的数学排版格式，公式清晰编号。\n- 图片中标注页码为14，标题为“RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation”。\n\n无图表或流程图结构，仅为纯文本与公式的组合，用于支持理论论证。}>\n", "is_scanned_fallback": false, "token": 555}]}, {"page_num": 15, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nThen we have\n(cid:13) (cid:13) dπ∗ (cid:13) (cid:13) −1 (cid:16) (cid:17)\nϵ > (1 − γ) (cid:13) ρ (cid:13) V π (ρ) − V π′ (ρ) . (12)\n(cid:13) dπ′ (cid:13)\n(cid:13) µ (cid:13)\n∞\nTherefore, we have\nV π∗ (ρ) − V π′ (ρ) ≤ ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) . (13)\n1 − γ (cid:13) dπ′ (cid:13)\n(cid:13) µ (cid:13)\n∞\nDue to dπ′(s) ≥ (1 − γ)µ(s), we further obtain\nµ\nV π∗ (ρ) − V π′ (ρ) ≤ ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) . (14)\n(1 − γ)2 (cid:13) µ (cid:13)\n(cid:13) (cid:13)\n∞\nSince µ(s) = βdπˆ(s) + (1 − β)ρ(s) ≥ βdπˆ(s), we have\nρ ρ\nV π∗ (ρ) − V π′ (ρ) ≤ ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) . (15)\n(1 − γ)2 (cid:13) βdπˆ (cid:13)\n(cid:13) ρ (cid:13)\n∞\nIn our case, β is a constant (i.e., a hyper-parameter), thus we could derive that\nV π∗ (ρ) − V π′ (ρ) ≤ O( ϵ (cid:13) (cid:13) (cid:13) dπ ρ ∗ (cid:13) (cid:13) (cid:13) ), (16)\n(1 − γ)2 (cid:13) dπˆ (cid:13)\n(cid:13) ρ (cid:13)\n∞\nwhich completes the proof.\nB.4. Analysis of Claim 1\nRecall that Lemma 3.5 indicates that our MaskNet-based sampling approach is equivalent to sampling states from a better\npolicy πˆ compared with a random explanation sampling from the policy π, i.e., η(πˆ) ≥ η(π). Let us denote the new initial\n(cid:13) (cid:13) (cid:13) (cid:13)\n(cid:13) dπ∗ (cid:13) (cid:13) dπ∗ (cid:13)\ndistribution using our MaskNet-based sampling approach as µ. By Assumption 3.4, we have (cid:13) ρ (cid:13) ≤ (cid:13) ρ (cid:13) . Using\n(cid:13) dπ ρ ˆ (cid:13) ∞ (cid:13) dπ ρ (cid:13) ∞\nour explanation method introduces a smaller distribution mismatch coefficient than using a random explanation method.\nTherefore, we claim that using our explanation method, we are able to form a better initial distribution µ and tighten the\nupper bound in Theorem 3.6, i.e., enhancing the agent’s performance after refining.\nC. Details of Evaluation\nC.1. Implementation Details\nImplementation of Our Method. We implement the proposed method using PyTorch (Paszke et al., 2019). We implement\nour method in four selected MuJoCo games based on Stable-Baselines3 (Raffin et al., 2021). We train the agents on a server\nwith 8 NVIDIA A100 GPUs for all the learning algorithms. For all our experiments, if not otherwise stated, we use a set of\ndefault hyper-parameters for p, λ, and α (listed in Appendix C.3).\nWe implement the environment reset function similar to Ecoffet et al. (2019) to restore the environment to selected critical\nstates. This method is feasible in our case, as we operate within simulator-based environments. However, in the real world,\nit may not be always possible to return to a certain state with the same sequences of actions due to the stochastic nature\nof state transition. It’s important to note that our framework is designed to be versatile and is indeed compatible with a\ngoal/state-conditioned policy approach such as Ecoffet et al. (2021). Given a trajectory with an identified most important\nstate, we can select the most important state as the final goal and select the en-route intermediate states as sub-goals. Then\nwe can train an agent to reach the final goal by augmenting each state with the next goal and giving a goal-conditioned\nreward once the next goal is reached until all goals are achieved.\nImplementation of Baseline Methods. Regarding baseline approaches, we use the code released by the authors or\nimplement our own version if the authors don’t release the code. Specifically, as for StateMask, we use their released open-\nsourced code from https://github.com/nuwuxian/RL-state_mask. Regarding Jump-Start Reinforcement\nLearning, we use the implementation from https://github.com/steventango/jumpstart-rl.\n15", "source": "pdfplumber_text", "token": 1301}, {"type": "image_analysis", "text": "\n[图片分析 (page_15_1765335217.jpg)]:\n图片:<{这是一张来自学术论文第15页的文本内容，主要包含数学推导、定理证明以及实验实现细节。具体内容如下：\n\n1. **数学推导部分**：\n   - 从公式(12)到(16)，展示了关于强化学习中策略价值函数差异（$V^{\\pi^*}(\\rho) - V^{\\pi'}(\\rho)$）的上界分析。\n   - 推导过程中引入了状态分布 $d_\\mu^{\\pi^*}$ 和 $d_\\mu^{\\pi'}$ 的无穷范数比值，并利用 $\\epsilon > (1-\\gamma)\\left\\|\\frac{d_\\mu^{\\pi^*}}{d_\\mu^{\\pi'}}\\right\\|_\\infty^{-1}\\left(V^{\\pi^*}(\\rho)-V^{\\pi'}(\\rho)\\right)$ 等不等式逐步收紧误差上界。\n   - 最终得出结论：$V^{\\pi^*}(\\rho) - V^{\\pi'}(\\rho) \\leq \\mathcal{O}\\left(\\frac{\\epsilon}{(1-\\gamma)^2}\\left\\|\\frac{d_\\mu^{\\pi^*}}{d_\\mu^\\pi}\\right\\|_\\infty\\right)$，完成证明。\n\n2. **理论分析（B.4 Analysis of Claim 1）**：\n   - 引用引理3.5和假设3.4，说明基于MaskNet的状态采样方法能够生成优于随机解释采样的初始策略 $\\hat{\\pi}$，即 $\\eta(\\hat{\\pi}) \\geq \\eta(\\pi)$。\n   - 指出该方法能减小分布失配系数（distribution mismatch coefficient），从而形成更优的初始分布 $\\mu$，并紧缩定理3.6中的上界，提升智能体性能。\n\n3. **评估细节（C. Details of Evaluation）**：\n   - **C.1 实现细节**：\n     - 使用 PyTorch 实现所提出的方法。\n     - 在四个选定的 MuloCo 游戏中基于 Stable-Baselines3 进行训练。\n     - 训练服务器配备 8 块 NVIDIA A100 GPU。\n     - 默认超参数设置见附录 C.3。\n     - 提出环境重置方法（类似 Ecoffet et al., 2019），用于恢复关键状态。\n     - 方法适用于模拟器环境，但在真实世界中可能受限于状态转移的随机性。\n     - 框架设计为通用型，支持目标/状态条件策略（goal/state-conditioned policy），可将轨迹中最重要状态作为子目标进行分步训练。\n\n   - **基线方法实现**：\n     - 对于未公开代码的基线方法，自行实现；对于已开源代码（如 StateMask），使用其 GitHub 仓库：https://github.com/nuwuxian/RL-state_mask。\n     - Jump-Start RL 使用 https://github.com/steven-tango/jumpstart-rl 的实现。\n\n4. **其他信息**：\n   - 页面编号为 15。\n   - 图片中无图表或流程图，仅含文字与公式。\n   - 关键术语包括：MaskNet、distribution mismatch、value function bound、PyTorch、Stable-Baselines3、NVIDIA A100 GPU、Ecoffet et al. (2019)、StateMask、Jump-Start RL。\n\n综上所述，该页面是论文的技术分析与实验实现部分，重点在于理论严谨性和实际可复现性，适合用于算法理解和代码复现参考。}>\n", "is_scanned_fallback": false, "token": 814}]}, {"page_num": 16, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nC.2. Extra Introduction to Applications\nHopper. Hopper game is a captivating two-dimensional challenge featuring a one-legged figure comprising a torso, thigh,\nleg, and a single supporting foot (Erez et al., 2012). The objective is to propel the Hopper forward through strategic hops by\napplying torques to the three hinges connecting its body parts. Observations include positional values followed by velocities\nof each body part, and the action space involves applying torques within a three-dimensional action space. Under the dense\nreward setting, the reward system combines healthy reward, forward reward, and control cost. Under the sparse reward\nsetting (Mazoure et al., 2019), the reward informs the x position of the hopper only if x > 0.6 in our experiments. The\nepisode concludes if the Hopper becomes unhealthy. We use “Hopper-v3” in our experiments.\nWalker2d. Walker2d is a dynamic two-dimensional challenge featuring a two-legged figure with a torso, thighs, legs, and\nfeet. The goal is to coordinate both sets of lower limbs to move in the forward direction by applying torques to the six hinges\nconnecting these body parts. The action space involves six dimensions, allowing exert torques at the hinge joints for precise\ncontrol. Observations encompass positional values and velocities of body parts, with the former preceding the latter. Under\nthe dense reward setting, the reward system combines a healthy reward bonus, forward reward, and control cost. Under\nthe sparse reward setting (Mazoure et al., 2019), the reward informs the x position of the hopper only if x > 0.6 in our\nexperiments. The episode concludes if the walker is deemed unhealthy. We use “Walker2d-v3” in our experiments and\nnormalize the observation when training the DRL agent.\nReacher. Reacher is an engaging two-jointed robot arm game where the objective is to skillfully maneuver the robot’s\nend effector, known as the fingertip, towards a randomly spawned target. The action space involves applying torques at\nthe hinge joints. Observations include the cosine and sine of the angles of the two arms, the coordinates of the target,\nangular velocities of the arms, and the vector between the target and the fingertip. It is worth noting that there is no sparse\nreward implementation of Reacher-v2 in Mazoure et al. (2019). The reward system comprises two components: “reward\ndistance” indicating the proximity of the fingertip to the target, and “reward control” penalizing excessive actions with a\nnegative squared Euclidean norm. The total reward is the sum of these components, and an episode concludes either after 50\ntimesteps with a new random target or if any state space value becomes non-finite. We use “Reacher-v2” in our experiments.\nHalfCheetah. HalfCheetah is an exhilarating 2D robot game where players control a 9-link cheetah with 8 joints, aiming to\npropel it forward with applied torques for maximum speed. The action space contains six dimensions, that enable strategic\nmovement. Observations include positional values and velocities of body parts. Under the dense reward setting, the\nreward system balances positive “forward reward” for forward motion with “control cost” penalties for excessive actions.\nUnder the sparse reward setting (Mazoure et al., 2019), the reward informs the x position of the hopper only if x > 5\nin our experiments. Episodes conclude after 1000 timesteps, offering a finite yet thrilling gameplay experience. We use\n“HalfCheetah-v3” in our experiments and normalize the observation when training the DRL agent.\nSelfish Mining. Selfish mining is a security vulnerability in blockchain protocols, identified by Eyal & Sirer (2018).\nWhen a miner holds a certain amount of computing power, they can withhold their freshly minted blocks from the public\nblockchain, thereby initiating a fork that is subsequently mined ahead of the official public blockchain. With this advantage,\nthe miner can introduce this fork into the network, overwriting the original blockchain and obtaining more revenue.\nTo find the optimal selfish mining strategies, Bar-Zur et al. (2023) proposed a deep reinforcement learning model to generate\na mining policy. The policy takes the current chain state as the input and chooses from the three pre-determined actions,\ni.e., adopting, revealing, and mining. With this policy network, the miner can obtain more mining rewards compared to\nusing heuristics-based strategies.\nWe train a PPO agent in the blockchain model developed by Bar-Zur et al. (git, d). The network architecture of the PPO\nagent is a 4-layer Multi-Layer Perceptron (MLP) with a hidden size of 128, 128, 128, and 128 in each layer. We adopt a\nsimilar network structure for training our mask network. The whale transaction has a fee of 10 with the occurring probability\nof 0.01 while other normal transactions have a fee of 1. The agent will receive a positive reward if his block is accepted and\nwill be penalized if his action is determined to be unsuccessful, e.g., revealing a private chain.\nIn our selfish mining task (Bar-Zur et al., 2023), three distinct actions are defined as follows:\nAdopt l: The miner chooses to adopt the first l blocks in the public chain while disregarding their private chain. Following\nthis, the miner will continue their mining efforts, commencing from the last adopted block.\nReveal l: This action becomes legal when the miner’s private chain attains a length of at least l. The consequence of this\n16", "source": "pdfplumber_text", "token": 1239}, {"type": "image_analysis", "text": "\n[图片分析 (page_16_1765335238.jpg)]:\n图片:<{这是一张不含具体数据的图片，内容为PDF文档的第16页，标题为“RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation”。该页面属于“C.2. Extra Introduction to Applications”章节，主要介绍多个强化学习（Reinforcement Learning, RL）应用场景和实验设置。\n\n具体内容包括以下五个部分：\n\n1. **Hopper**：描述了一个单腿机器人（Hopper）在二维环境中的运动任务。目标是通过施加力矩使机器人向前跳跃。观测包括各身体部位的位置和速度，动作空间为三维力矩。奖励设置分为dense（包含健康奖励、前进奖励、控制成本）和sparse（仅在x > 0.6时提供位置信息）。实验使用“Hopper-v3”。\n\n2. **Walker2d**：一个双足机器人任务，由躯干、大腿、小腿和脚组成，共六个关节。目标是协调两条腿向前移动。动作空间为六维，观测包括位置和速度。奖励结构与Hopper类似，但在sparse设置下仅当x > 0.6时提供位置信息。实验使用“Walker2d-v3”，并归一化观测值。\n\n3. **Reacher**：一个两关节机械臂任务，目标是将指尖移动到随机生成的目标点。动作空间为两个铰链关节的力矩。观测包括角度、目标坐标、角速度及指尖与目标之间的向量。奖励由“reward distance”（距离奖励）和“reward control”（控制惩罚）组成。实验使用“Reacher-v2”，每50步或目标消失/动作空间变为非有限时结束。\n\n4. **HalfCheetah**：一个9连杆猎豹模型，有8个关节，目标是通过施加力矩实现最大速度前进。动作空间为六维。观测包括位置和速度。dense奖励结合前进奖励与控制成本；sparse奖励仅在x > 5时提供位置信息。实验使用“HalfCheetah-v3”，每1000步结束，且观测被归一化。\n\n5. **Selfish Mining**：描述区块链中的一种安全漏洞——自私挖矿。当矿工拥有一定算力时，可隐藏区块发起分叉，从而获得额外收益。Bar-Zur et al. (2023) 使用深度强化学习（DRL）训练PPO代理来生成最优自私挖矿策略。代理输入当前链状态，选择三种预定义动作之一：Adopt l（采用前l个公共链区块）、Reveal l（当私链长度≥l时公开私链）、Mining（继续挖矿）。网络结构为4层MLP，每层隐藏单元数为128。交易费用设定为0.01（特殊）和1（普通），成功出块得正奖励，失败则受罚。\n\n该页面无图表或流程图，也无可视化数据，仅为文字描述。关键信息包括实验任务名称、奖励机制、动作空间、观测空间、实验版本（如v3）、以及在自私挖矿任务中使用的PPO模型架构与训练细节。}>\n", "is_scanned_fallback": false, "token": 696}]}, {"page_num": 17, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\naction may result in either the creation of an active fork in the public chain or the overriding of the public chain.\nMine: This action simply involves continuing with the mining process. Once executed, a new block is mined and subsequently\nadded to either the private chain of the rational miner or to the public chain, contingent on which entity successfully mined\nthe block.\nCAGE Challenge 2. To inspire new methods for automating cyber defense, the Technical Cooperation Program (TTCP)\nlaunched the Autonomous Cyber Defence Challenge (CAGE Challenge) to produce AI-based blue teams for instantaneous\nresponse against cyber attacks (CAGE, 2022). The goal is to create a DRL blue agent to protect a network against a red\nagent. The action space of the blue agent includes monitoring, analyzing, decoyApache, decoyFemitter, decoyHarakaSMPT,\ndecoySmss, decoySSHD, decoySvchost, decoyTomcat, removing, and restoring. Note that the blue agent can receive a\nnegative reward when the red agent gets admin access to the system (and continues to receive negative rewards as the red\nagent maintains the admin access). We use CAGE challenge 2 for our evaluation.\nWe choose the champion scheme proposed by Cardiff University (git, c) in CAGE challenge 2 (git, b). The target agent is a\nPPO-based blue agent to defend a network against the red agent “B-line”. The trail has three different lengths, i.e., 30, 50,\nand 100. The final reward is the sum of the average rewards of these three different lengths.\nThe action set of the blue agent is defined as follows.\nMonitor: The blue agent automatically collects the information about flagged malicious activity on the system and reports\nnetwork connections and associated processes that are identified as malicious.\nAnalyze: The blue agent analyzes the information on files associated with recent alerts including signature and entropy.\nDecoyApache, DecoyFemitter, DecoyHarakaSMPT, DecoySmss, DecoySSHD, DecoySvchost, DecoyTomcat: The blue agent\nsets up the corresponding decoy service on a specified host. An alert will be raised if the red agent accesses the decoy\nservice.\nRemove: The blue agent attempts to remove red from a host by destroying malicious processes, files, and services.\nRestore: The blue agent restores a system to a known good state. Since it significantly impacts the system’s availability, a\nreward penalty of -1 will be added when executing this action.\nAutonomous Driving. Deep reinforcement learning has been applied in autonomous driving to enhance driving safety.\nOne representative driving simulator is MetaDrive (Li et al., 2022). A DRL agent is trained to guide a vehicle safely and\nefficiently to travel to its destination. MetaDrive converts the Birds Eye View (BEV) of the road conditions and the sensor\ninformation such as the vehicle’s steering, direction, velocity, and relative distance to traffic lanes into a vector representation\nof the current state. The policy network takes this state vector as input and yields driving actions, including accelerating,\nbraking, and steering commands. MetaDrive employs a set of reward functions to shape the learning process. For instance,\npenalties are assigned when the agent collides with other vehicles or drives out of the road boundary. To promote smooth and\nefficient driving, MetaDrive also incorporates rewards to encourage forward motion and the maintenance of an appropriate\nspeed.\nWe select the “Macro-v1” environment powered by the MetaDrive simulator (Li et al., 2022). The goal of the agent is to\nlearn a deep policy to successfully cross the car flow and reach the destination. We train the target agent and our mask\nnetwork by the PPO algorithm following the implementation of DI-drive (drive Contributors, 2021). The environment\nreceives normalized action to control the target agent a = [a , a ] ∈ [−1, 1]2. The action vector a will then be converted to\n1 2\nthe steering (degree), acceleration (hp), and brake signal (hp).\nMalware Mutation. DRL has been used to assess the robustness of ML-based malware detectors. For example, Anderson\net al. (2018) propose a DRL-based approach to attack malware detectors for portable executable (PE) files. We use the\n“Malconv” gym environment Raff et al. (2017) implemented in (git, a) for our experiments. We train a DRL agent based on\nTianshou framework (Weng et al., 2022). The input of the DRL agent is a feature vector of the target malware and outputs\nthe corresponding action to guide the malware manipulation. We present the action set of the MalConv gym environment in\nTable 2 for ease of comprehension in the case study section. A big reward of 10 is provided when evading detection.\nThe reward mechanism of the “Malconv” environment is as follows. Initially, the malware detection model will provide\na score sc of the current malware. If sc is lower than some threshold, the malware has already evaded the detection.\n0 0\nOtherwise, the DRL agent will take some mutation actions to bypass the detection. At step t, after executing the agent’s\n17", "source": "pdfplumber_text", "token": 1140}, {"type": "image_analysis", "text": "\n[图片分析 (page_17_1765335258.jpg)]:\n图片:<{这是一张来自PDF文档第17页的纯文本内容截图，不包含任何图像、图表或流程图。其内容主要描述了在强化学习（Reinforcement Learning, RL）领域中几个具体应用场景的研究与实验设置，包括：\n\n1. **CAGE Challenge 2**：介绍了一个由Technical Cooperation Program (TTCP)发起的自主网络防御挑战（CAGE Challenge），目标是训练AI驱动的“蓝队”代理（blue agent）以对抗红队攻击。该任务中蓝队代理需执行一系列动作来保护网络系统，包括：\n   - Monitor：监控并报告恶意活动。\n   - Analyze：分析关联文件（如签名和熵）。\n   - Decoy服务部署：设置诱饵服务（如DecoyApache、DecoyFemitter等）。\n   - Remove：移除恶意进程、文件和服务。\n   - Restore：将系统恢复到已知良好状态（执行此操作会受到-1的惩罚奖励）。\n\n   蓝队代理使用PPO算法进行训练，目标是在不同长度（30、50、100）的轨迹上最大化平均奖励之和。\n\n2. **Autonomous Driving**：介绍了MetaDrive仿真环境中的深度强化学习应用，用于自动驾驶场景。该环境将鸟瞰视角（BEV）的道路信息和车辆传感器数据转化为向量状态输入，通过策略网络输出驾驶动作（加速、刹车、转向）。奖励机制包括避免碰撞、保持车道内行驶、鼓励前进和维持合适速度等。实验采用“Macro-v1”环境，目标是让智能体穿越车流到达目的地，动作空间为a = [a₁, a₂] ∈ [-1, 1]²，映射为转向角、加速度和刹车信号。\n\n3. **Malware Mutation**：基于DRL评估机器学习检测器对恶意软件的鲁棒性。使用“Malconv”环境（Raft et al., 2017），输入为恶意软件特征向量，输出为引导恶意软件变异的动作。当成功逃避检测时给予+10的高奖励。初始阶段，若检测模型给出的分数sc₀低于阈值，则视为已逃逸；否则，DRL代理采取变异动作以绕过检测。\n\n文中还提到了相关参考文献（如Li et al., 2022；Anderson et al., 2018；Weng et al., 2022）以及使用的工具框架（Tianshou、drive Contributors），并指出表2中列出了MalConv环境中对应的动作。\n\n关键信息：\n- 页面编号：17\n- 图片类型：纯文本页面，无图像或图表\n- 涉及技术领域：强化学习、网络安全、自动驾驶、恶意软件检测\n- 核心任务：描述三个DRL应用场景的环境设定、动作空间、奖励机制和实验配置}>\n", "is_scanned_fallback": false, "token": 639}]}, {"page_num": 18, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nTable 2. Action set of the MalConv gym environment.\nAction index Action meaning\n0 “modify machine type”\n1 “pad overlay”\n2 “append benign data overlay”\n3 “append benign binary overlay”\n4 “add bytes to section cave”\n5 “add section strings”\n6 “add section benign data”\n7 “add strings to overlay”\n8 “add imports”\n9 “rename section”\n10 “remove debug”\n11 “modify optional header”\n12 “modify timestamp”\n13 “break optional header checksum”\n14 “upx unpack”\n15 “upx pack”\nTable 3. Hyper-parameter choices in Experiment I-V. “Selfish” represents Selfish Mining. “Cage” represents Cage Challenge 2. “Auto”\nrepresents Autonomous Driving. “Malware” represents Malware Mutation.\nHyper-parameter Hopper Walker2d Reacher HalfCheetah Selfish Cage Auto Malware\np 0.25 0.25 0.50 0.50 0.25 0.50 0.25 0.50\nλ 0.001 0.01 0.001 0.01 0.001 0.01 0.01 0.01\nα 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001\naction, the malware detection model will provide a new score sc . If sc is lower than some threshold, the mutation is\nt t\nsuccessful and a big reward of 10 will be given. Otherwise, the reward will be sc − sc . The maximum allowed number of\n0 t\nsteps is 10.\nC.3. Additional Experiment Results\nHyper-parameter Choices in Experiment I-V. Table 3 summarizes our hyper-parameter choices in Experiment I-V. For all\napplications, we choose the coefficient of the intrinsic reward for training the mask network α as 0.01. The hyper-parameters\np and λ for our refining method vary by application.\nFidelity Scores in Experiment I. Figure 5 shows the fidelity score comparison across all explanation methods. We have\nthree key observations. First, We observe that our explanation method has similar fidelity scores with StateMask across all\napplications, empirically indicating the equivalence of our explanation method with StateMask. Second, we observe that our\nexplanation method and StateMask have higher fidelity scores than random explanation across all applications, indicating\nthat the mask network provides more faithful explanations for the target agents.\nEfficiency Comparison in Experiment II. Table 4 reports the efficiency evaluation results when training a mask network\nusing StateMask and our method. We observe that it takes 16.8% less time on average to train a mask network using our\nmethod than using StateMask, which shows the advantage of our method with respect to efficiency.\nComparison with Self-Imitation Learning. We compare RICE against the self-imitation learning (SIL) approach (Oh\net al., 2018) across four MuJoCo games. We present the results presented in Table 5. These experiment results demonstrate\nthat RICE consistently outperforms the self-imitation learning method. While self-imitation learning has the advantage\nof encouraging the agent to imitate past successful experiences by prioritizing them in the replay buffer, it cannot address\nscenarios where the agent (and its past experience) has errors or sub-optimal actions. In contrast, RICE constructs a mixed\ninitial distribution based on the identified critical states (using explanation methods) and encourages the agent to explore the\nnew initial states. This helps the agent escape from local minima and break through the training bottlenecks.\nImpact of Other Explanation Methods. We investigate the impact of other explanation methods (i.e., AIRS (Yu et al.,\n2023) and Integrated Gradients (Sundararajan et al., 2017)) on four Mujoco games. we fix the refining method and use\n18", "source": "pdfplumber_text", "token": 899}, {"type": "table", "text": "\n[Detected Table]\n| Hopper | Walker2d | Reacher | HalfCheetah | Selfish | Cage | Auto |\n| --- | --- | --- | --- | --- | --- | --- |\n| 0.25 | 0.25 | 0.50 | 0.50 | 0.25 | 0.50 | 0.25 |\n| 0.001 | 0.01 | 0.001 | 0.01 | 0.001 | 0.01 | 0.01 |\n| 0.0001 | 0.0001 | 0.0001 | 0.0001 | 0.0001 | 0.0001 | 0.0001 |\n\n", "source": "pdfplumber_table", "token": 190}, {"type": "image_analysis", "text": "\n[图片分析 (page_18_1765335275.jpg)]:\n图片:<{\n  这是一张来自PDF文档第18页的页面截图，主要内容包括两个表格（Table 2 和 Table 3）以及一段文字描述。\n\n  Table 2: \"Action set of the MalConv gym environment\"\n  - 描述了MalConv环境中的动作集合，共16个动作，每个动作有对应的索引和含义。\n  - 动作索引从0到15，对应的具体操作包括：\n    - 0: \"modify_machine_type\"\n    - 1: \"pad_overlay\"\n    - 2: \"append_benign_data_overlay\"\n    - 3: \"append_benign_binary_overlay\"\n    - 4: \"add_bytes_to_section_cave\"\n    - 5: \"add_section_strings\"\n    - 6: \"add_section_benign_data\"\n    - 7: \"add_strings_to_overlay\"\n    - 8: \"add_imports\"\n    - 9: \"rename_section\"\n    - 10: \"remove_debug\"\n    - 11: \"modify_optional_header\"\n    - 12: \"modify_timestamp\"\n    - 13: \"break_optional_header_checksum\"\n    - 14: \"upx_unpack\"\n    - 15: \"upx_pack\"\n\n  Table 3: \"Hyper-parameter choices in Experiment I–V\"\n  - 列出了在实验I-V中使用的超参数设置，涵盖不同任务场景：Hopper, Walker2d, Reacher, HalfCheetah, Selfish, Cage, Auto, Malware。\n  - 超参数包括：\n    - p: 取值范围为0.25或0.50\n    - λ: 取值范围为0.001或0.01\n    - α: 固定为0.0001\n  - 每个任务场景下具体的参数配置如下：\n    - Hopper: p=0.25, λ=0.001, α=0.0001\n    - Walker2d: p=0.25, λ=0.01, α=0.0001\n    - Reacher: p=0.50, λ=0.001, α=0.0001\n    - HalfCheetah: p=0.50, λ=0.01, α=0.0001\n    - Selfish: p=0.25, λ=0.001, α=0.0001\n    - Cage: p=0.50, λ=0.01, α=0.0001\n    - Auto: p=0.25, λ=0.01, α=0.0001\n    - Malware: p=0.50, λ=0.01, α=0.0001\n\n  文字内容分析：\n  - 提及了实验中关于奖励机制的设计：当恶意软件检测模型给出的新分数 $ sc_t $ 低于某个阈值时，突变成功并获得10分奖励；否则奖励为 $ sc_0 - sc_t $。最大允许步骤数为10。\n  - C.3节讨论了额外实验结果，包括：\n    - 超参数选择（Table 3）\n    - 实验I中的保真度得分比较（Figure 5未显示）\n    - 实验II中的效率对比（Table 4未显示）\n    - 与自我模仿学习（SIL）方法的比较（Table 5未显示）\n    - 其他解释方法的影响（如AIRS、Integrated Gradients）\n\n  关键信息：\n  - 图片来源于论文《RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation》\n  - 页面编号为18\n  - 包含的数据主要集中在超参数配置和动作空间定义上，用于支持强化学习实验设计\n}>\n", "is_scanned_fallback": false, "token": 887}]}, {"page_num": 19, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n3\n2\n1\n0\nTop10 Top20 Top30 Top40\nHopper\nytilediF\n3 1.0 2.0 Random\nStateMask\n0.8\n2 1.5 OURS\n0.6\n1.0\n0.4\n1\n0.2 0.5\n0 0.0 0.0\nTop10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40\nWalker2d Reacher HalfCheetah\n2.5 1.5 1.0 Random\n1.5 StateMask\n2.0 1.0 0.5 OURS\n1.0\n1.5\n1.0 0.5 0.5 0.0\n0.5 0.0 0.0 0.5\n0.0\nTop10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40\nSelfish Mining Cage Challenge 2 Autonomous Driving Malware Mutation\nFigure 5. Fidelity scores for explanation generated by baseline methods and our proposed explanation method. Note that a higher score\nimplies higher fidelity.\nTable 4. Efficiency comparison when training the mask network. We report the number of seconds when training the mask using a fixed\nnumber of samples. “Selfish” represents Selfish Mining. “Cage” represents Cage Challenge 2. “Auto” represents Autonomous Driving.\n“Malware” represents Malware Mutation.\nApplications Hopper Walker2d Reacher HalfCheetah Selfish Cage Auto Malware\nNum. of samples 3 × 105 3 × 105 3 × 105 3 × 105 1.5 × 106 1 × 107 2443260 32349\nStateMask 15393 2240 8571 1579 9520 79382 109802 50775\nOurs 12426 1899 7033 1317 8360 65400 88761 41340\ndifferent explanation methods to identify critical steps for refinement. The results are reported in Table 6. We observe that\nusing the explanation generated by our mask network, the refining achieves the best outcome across all four applications.\nUsing other explanation methods (Integrated Gradients and AIRS), our framework still achieves better results than the\nrandom baseline, suggesting that our framework can work with different explanation method choices.\nSensitivty of p and λ in Hopper game with an imitated PPO agent. We report the sensitivity of hyper-parameters p and\nλ in Hopper game with an imitated PPO agent in Figure 6. We observe that in general, a mixture probability of p = 0.25 or\np = 0.5 is a better choice. An RND bonus can facilitate the agent with faster refinement.\nSensitivity of Hyper-parameters p and λ. We provide the sensitivity results of p in all applications in Figure 7. We\nobserve that generally a mixture probability of p = 0.25 or p = 0.5 is a good choice. Additionally, recall that we need to\nuse the hyper-parameter λ to balance the scale of the “true” environment reward and the exploration bonus. We test the\nsensitivity of λ from the space {0.1, 0.01, 0.001}. Figure 8 reports the agent’s performance after refining under different\nsettings of λ. We observe that our retaining method is insensitive to the choice of λ. The agent’s performance does not vary\na lot with different settings of λ. But λ = 0.01 gives the best performance in all applications except selfish mining.\nSensitivity of α. Recall that under certain assumptions, we are able to simplify the design of StateMask. We propose\nan intrinsic reward mechanism to encourage the mask network to blind more states without sacrificing performance. The\nhyper-parameter α is then introduced to balance the performance of the perturbed agent and the need for encouraging\nblinding. We test the sensitivity of α from the space {0.01, 0.001, 0.0001} and report the fidelity scores under different\nsettings of α in Figure 9. We observe that though the value of α varies, the fidelity score does not change much.\nC.4. Evaluation Results of MuJoCo Games with Sparse Rewards\nResults of SparseWalker2d. First, we compare our refining method with other baseline methods (i.e., PPO fine-tuning,\nStateMask-R, and JSRL) in the SparseWalker2d game. Figure 10 shows that our refining method is able to help the DRL\nagent break through the bottleneck with the highest efficiency compared with other baseline refining methods. Additionally,\nby replacing our explanation method with a random explanation, we observe that the refining performance is getting worse.\n19", "source": "pdfplumber_text", "token": 1157}, {"type": "table", "text": "\n[Detected Table]\n| 3 1.0 2.0 Random 3 StateMask 0.8 ytilediF 2 1.5 OURS 2 0.6 1.0 0.4 1 1 0.2 0.5 0 0 0.0 0.0 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Hopper Walker2d Reacher HalfCheetah 2.5 1.5 1.0 Random 1.5 StateMask 2.0 1.0 0.5 OURS 1.0 1.5 1.0 0.5 0.5 0.0 0.5 0.0 0.0 0.5 0.0 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Selfish Mining Cage Challenge 2 Autonomous Driving Malware Mutation | Random |\n| --- | --- |\n|  | StateMask OURS |\n\n", "source": "pdfplumber_table", "token": 288}, {"type": "table", "text": "\n[Detected Table]\n| Random StateMask OURS |\n| --- |\n|  |\n\n", "source": "pdfplumber_table", "token": 19}, {"type": "table", "text": "\n[Detected Table]\n| Hopper | Walker2d | Reacher | HalfCheetah | Selfish | Cage | Auto |\n| --- | --- | --- | --- | --- | --- | --- |\n| 3×105 | 3×105 | 3×105 | 3×105 | 1.5×106 | 1×107 | 2443260 |\n| 15393 | 2240 | 8571 | 1579 | 9520 | 79382 | 109802 |\n| 12426 | 1899 | 7033 | 1317 | 8360 | 65400 | 88761 |\n\n", "source": "pdfplumber_table", "token": 191}, {"type": "image_analysis", "text": "\n[图片分析 (page_19_1765335298.jpg)]:\n图片:<{这是一张包含多个柱状图的图表，标题为“Figure 5. Fidelity scores for explanation generated by baseline methods and our proposed explanation method.”。该图展示了在不同任务（Hopper, Walker2d, Reacher, HalfCheetah, Selfish Mining, Cage Challenge 2, Autonomous Driving, Malware Mutation）中，三种方法（Random、StateMask、OURS）生成解释的保真度（Fidelity）得分。每个子图分别对应一个任务，横轴表示Top10、Top20、Top30、Top40的样本集合，纵轴表示保真度分数，数值越高表示保真度越高。\n\n关键数据点包括：\n- 在大多数任务中，OURS 方法的保真度得分显著高于 Random 和 StateMask 方法。\n- 特别是在 Hopper、Walker2d、Reacher 和 HalfCheetah 等任务中，OURS 方法在 Top10 到 Top40 的所有区间内均表现出更高的保真度。\n- 在 Selfish Mining 和 Cage Challenge 2 中，OURS 方法同样优于其他两种方法。\n- 在 Autonomous Driving 和 Malware Mutation 任务中，OURS 方法在 Top10 和 Top20 区间表现优异，但在 Top30 和 Top40 区间略有下降，但仍优于 Random 方法。\n\n整体特征：\n- 所有子图均采用相同的颜色编码：蓝色代表 Random，橙色代表 StateMask，绿色代表 OURS。\n- 每个柱状图上方带有误差棒，表示数据的不确定性或标准差。\n- 图表下方附有说明：“Note that a higher score implies higher fidelity.”，即分数越高，保真度越高。\n\n此外，图片中还包含 Table 4，对比了训练掩码网络时不同方法的效率（以秒为单位），具体数据如下：\n| Applications | Hopper | Walker2d | Reacher | HalfCheetah | Selfish | Cage | Auto | Malware |\n|--------------|--------|----------|---------|-------------|---------|------|------|---------|\n| Num. of samples | 3×10⁴ | 3×10⁴ | 3×10⁴ | 3×10⁴ | 1.5×10⁶ | 1×10⁶ | 2443260 | 32349 |\n| StateMask    | 15393 | 2240 | 8571 | 1579 | 9520 | 79382 | 109802 | 50775 |\n| Ours         | 12426 | 1899 | 7033 | 1317 | 8360 | 65400 | 88761 | 41340 |\n\n从表格可以看出，OURS 方法在所有任务中训练时间均短于 StateMask 方法，表明其具有更高的效率。}>\n", "is_scanned_fallback": false, "token": 677}]}, {"page_num": 20, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n4000.0\n3500.0\n3000.0\n2500.0\n2000.0\n1500.0\n1000.0\n500.0\n0.0\n0.0 200000.0 400000.0 600000.0 800000.0 1000000.0\n= 0.0\ndraweR\n4000.0\n3500.0\n3000.0\n2500.0\n2000.0\n1500.0\n1000.0\n500.0\n0.0\n0.0 200000.0 400000.0 600000.0 800000.0 1000000.0\n= 0.1\ndraweR\n4000.0\n3500.0\n3000.0\n2500.0\n2000.0\n1500.0\n1000.0\n500.0\n0.0\n0.0 200000.0 400000.0 600000.0 800000.0 1000000.0\n= 0.01\ndraweR\n4000.0\n3500.0\n3000.0\n2500.0\n2000.0\n1500.0\n1000.0\n500.0\n0.0\n0.0 200000.0 400000.0 600000.0 800000.0 1000000.0\n= 0.001\ndraweR\np=0.0\np=0.25\np=0.5\np=0.75\np=1.0\nFigure 6. Sensitivity results of hyper-parameters p and λ in Hopper game with an imitated PPO agent. We vary the hyper-parameter λ\nfrom {0, 0.1, 0.01, 0.001} and record the performance of the agent after refining. A smaller choice of λ means a smaller reward bonus\nfor exploration.\n3665\n3660\n3655\n3650\n3645\n0.00 0.25 0.50 0.75 1.00\nHopper\necnamrofreP\n3980\n3.0\n3975\n3.5\n3970\n4.0\n3965\n0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00\nWalker2d Reacher HalfCheetah\n16.5\n16.0\n15.5\n15.0\n14.5\n0.00 0.25 0.50 0.75 1.00\nSelfish Mining\necnamrofreP\n=0 =0.1 =0.01 =0.001\n20 57.5\n16\n22 55.0\n14\n24 12 52.5\n26 10 50.0\n28 8\n47.5\n0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00\nCage Challenge 2 Safe Driving Malware Mutation\nFigure 7. Sensitivity results of hyper-parameter p in all applications. We vary the hyper-parameter p from {0, 0.25, 0.5, 0.75, 1} under\ndifferent λ, and record the performance of the agent after refining. When p = 0, refining starts from the default initial states of the\nenvironment. When p = 1, refining starts exclusively from critical states. We show that the “mixed” initial state distribution helps to\nachieve a better performance.\n20", "source": "pdfplumber_text", "token": 1113}, {"type": "image_analysis", "text": "\n[图片分析 (page_20_1765335319.jpg)]:\n图片:<{\n  \"type\": \"数据图表\",\n  \"figures\": [\n    {\n      \"id\": \"Figure 6\",\n      \"title\": \"Sensitivity results of hyper-parameters p and λ in Hopper game with an imitated PPO agent\",\n      \"description\": \"展示在Hopper游戏中，使用模仿PPO智能体时，超参数p和λ的敏感性结果。通过改变λ值（{0, 0.1, 0.01, 0.001}），记录智能体在精炼后的性能表现。较小的λ意味着较小的探索奖励加成。\",\n      \"subplots\": [\n        {\n          \"lambda\": 0.0,\n          \"x_label\": \"Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": \"多条彩色曲线表示不同p值（p=0.0, 0.25, 0.5, 0.75, 1.0）下的奖励变化趋势，曲线有显著波动且存在重叠区域，整体呈上升趋势。\",\n          \"observation\": \"当λ=0.0时，所有p值下奖励增长较慢，后期出现剧烈波动。\"\n        },\n        {\n          \"lambda\": 0.1,\n          \"x_label\": \"Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": \"与λ=0.0类似，但整体奖励水平略高，部分p值（如p=0.5, 1.0）在后期表现出更强的收敛性。\",\n          \"observation\": \"λ=0.1时，探索奖励增加，导致早期波动更大，但后期稳定性提升。\"\n        },\n        {\n          \"lambda\": 0.01,\n          \"x_label\": \"Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": \"奖励增长更平滑，p=0.5和p=1.0表现突出，尤其在后期接近4000奖励。\",\n          \"observation\": \"小λ值（0.01）带来更稳定的训练过程，适合精细调整。\"\n        },\n        {\n          \"lambda\": 0.001,\n          \"x_label\": \"Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": \"所有p值下奖励增长缓慢，p=0.0和p=0.25几乎停滞，而p=1.0在后期略有上升。\",\n          \"observation\": \"极小的λ值抑制了探索，导致学习效率下降。\"\n        }\n      ],\n      \"legend\": \"p=0.0 (蓝色), p=0.25 (橙色), p=0.5 (绿色), p=0.75 (红色), p=1.0 (紫色)\"\n    },\n    {\n      \"id\": \"Figure 7\",\n      \"title\": \"Sensitivity results of hyper-parameter p in all applications\",\n      \"description\": \"展示在多个应用场景中，超参数p对智能体性能的影响。固定λ，改变p从{0, 0.25, 0.5, 0.75, 1}，记录精炼后性能。p=0时从默认初始状态开始精炼；p=1时仅从关键状态开始精炼。\",\n      \"subplots\": [\n        {\n          \"task\": \"Hopper\",\n          \"x_label\": \"p\",\n          \"y_label\": \"Performance\",\n          \"data\": \"p=0.5时性能最高（约3665），p=0和p=1时较低，p=0.75时略降。\",\n          \"observation\": \"混合初始状态分布（p=0.5）表现最佳。\"\n        },\n        {\n          \"task\": \"Walker2d\",\n          \"x_label\": \"p\",\n          \"y_label\": \"Performance\",\n          \"data\": \"p=0.25时性能最优（约3980），p=0.5次之，p=1时最低。\",\n          \"observation\": \"轻度偏向关键状态（p=0.25）有助于提升性能。\"\n        },\n        {\n          \"task\": \"Reacher\",\n          \"x_label\": \"p\",\n          \"y_label\": \"Performance\",\n          \"data\": \"p=0.5时性能最高（约-3.0），p=0和p=1时较差。\",\n          \"observation\": \"中等比例的关键状态初始化效果最好。\"\n        },\n        {\n          \"task\": \"HalfCheetah\",\n          \"x_label\": \"p\",\n          \"y_label\": \"Performance\",\n          \"data\": \"p=0.5时性能最高（约57.5），p=0和p=1时显著降低。\",\n          \"observation\": \"混合策略在复杂任务中优势明显。\"\n        },\n        {\n          \"task\": \"Selfish Mining\",\n          \"x_label\": \"p\",\n          \"y_label\": \"Performance\",\n          \"data\": \"p=0.25时性能最高（约16.5），p=0.75和p=1时下降。\",\n          \"observation\": \"适度关注关键状态更有效。\"\n        },\n        {\n          \"task\": \"Cage Challenge 2\",\n          \"x_label\": \"p\",\n          \"y_label\": \"Performance\",\n          \"data\": \"p=0.25时性能最优（约-20），p=0和p=1时较差。\",\n          \"observation\": \"避免完全依赖默认或关键状态，平衡策略更优。\"\n        },\n        {\n          \"task\": \"Safe Driving\",\n          \"x_label\": \"p\",\n          \"y_label\": \"Performance\",\n          \"data\": \"p=0.5时性能最高（约16），p=0和p=1时较低。\",\n          \"observation\": \"混合策略在安全驾驶任务中表现稳定。\"\n        },\n        {\n          \"task\": \"Malware Mutation\",\n          \"x_label\": \"p\",\n          \"y_label\": \"Performance\",\n          \"data\": \"p=0.5时性能最高（约57.5），p=0和p=1时显著下降。\",\n          \"observation\": \"混合初始状态在恶意软件变异任务中表现最佳。\"\n        }\n      ],\n      \"legend\": \"λ=0 (蓝色), λ=0.1 (橙色), λ=0.01 (绿色), λ=0.001 (红色)\"\n    }\n  ],\n  \"overall_analysis\": \"两组图表均用于分析强化学习中超参数p和λ对智能体性能的影响。Figure 6聚焦于Hopper任务中λ的变化对奖励的影响，显示较小λ值可提高稳定性但可能牺牲探索能力。Figure 7扩展到多个任务，表明p=0.5（即混合初始状态分布）通常能获得最佳性能，说明‘混合’策略在多种场景下具有普适性。这些结果支持RICE方法中引入解释机制以突破训练瓶颈的有效性。\",\n  \"key_data_points\": [\n    \"λ=0.01时，p=0.5在Hopper任务中达到最高奖励（约4000）\",\n    \"在Hopper任务中，p=0.5时性能峰值为3665\",\n    \"在HalfCheetah任务中，p=0.5时性能达到57.5\",\n    \"在Malware Mutation任务中，p=0.5时性能最高（57.5）\"\n  ],\n  \"page_number\": 20\n}>\n", "is_scanned_fallback": false, "token": 1629}]}, {"page_num": 21, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n3660\n3655\n3650\n10 3 10 2 10 1\nHopper\necnamrofreP\n2140\n3980\n3.0 2130\n3975\n3.5 2120\n3970\n2110\n4.0\n3965\n10 3 10 2 10 1 10 3 10 2 10 1 10 3 10 2 10 1\nWalker2d Reacher HalfCheetah\n16.5\n16.0\n15.5\n15.0 10 3 10 2 10 1\nSelfish Mining\necnamrofreP\np=0 p=0.25 p=0.5 p=0.75 p=1\n58\n20\n16 56\n22\n14 54\n24\n12 52\n26\n10 50\n28 48\n10 3 10 2 10 1 10 3 10 2 10 1 10 3 10 2 10 1\nCage Chllenge 2 Safe Driving Malware Mutation\nFigure 8. Sensitivity results of hyper-parameter λ. We vary the hyper-parameter λ from {0.1, 0.01, 0.001} and record the performance of\nthe agent after refining. A smaller choice of λ means a smaller reward bonus for exploration.\n3\n2\n1\n0\nTop10 Top20 Top30 Top40\nHopper\nytilediF\n3 1.0 2.0 =0.01\n=0.001\n0.8\n2 1.5 =0.0001\n0.6\n1.0\n0.4\n1\n0.2 0.5\n0 0.0 0.0\nTop10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40\nWalker2d Reacher HalfCheetah\n2.5 1.5 1.0 =0.01\n1.5 =0.001\n2.0 =0.0001\n1.0 0.5\n1.5 1.0\n1.0\n0.5 0.5 0.0\n0.5\n0.0 0.0 0.0\nTop10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40\nSelfish Mining Cage Challenge 2 Autonomous Driving Malware Mutation\nFigure 9. Sensitivity results of hyper-parameter α. We vary the hyper-parameter α from {0.01, 0.001, 0.0001} and record the fidelity\nscores of the mask network trained under different settings of α. A higher fidelity score means a higher fidelity.\n21", "source": "pdfplumber_text", "token": 759}, {"type": "table", "text": "\n[Detected Table]\n| 3 1.0 2.0 =0.01 3 =0.001 0.8 ytilediF 2 1.5 =0.0001 2 0.6 1.0 0.4 1 1 0.2 0.5 0 0 0.0 0.0 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Hopper Walker2d Reacher HalfCheetah 2.5 1.5 1.0 =0.01 1.5 =0.001 2.0 =0.0001 1.0 0.5 1.5 1.0 1.0 0.5 0.5 0.0 0.5 0.0 0.0 0.0 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Top10 Top20 Top30 Top40 Selfish Mining Cage Challenge 2 Autonomous Driving Malware Mutation |  | =0.01 |\n| --- | --- | --- |\n|  |  | =0.001 =0.0001 |\n\n", "source": "pdfplumber_table", "token": 329}, {"type": "image_analysis", "text": "\n[图片分析 (page_21_1765335361.jpg)]:\n图片:<{\n  \"type\": \"数据图表\",\n  \"description\": \"该图片包含两个主要的图表（Figure 8 和 Figure 9），分别展示了在强化学习中不同超参数对模型性能和掩码网络保真度的影响。\",\n  \"figure_8\": {\n    \"title\": \"Sensitivity results of hyper-parameter λ\",\n    \"purpose\": \"研究超参数 λ 对代理性能的影响，λ 的取值为 {0.1, 0.01, 0.001}，较小的 λ 意味着更小的探索奖励加成。\",\n    \"data_type\": \"折线图\",\n    \"x_axis\": \"λ 值（以对数尺度表示）\",\n    \"y_axis\": \"Performance（性能）\",\n    \"tasks\": [\n      \"Hopper\",\n      \"Walker2d\",\n      \"Reacher\",\n      \"HalfCheetah\",\n      \"Selfish Mining\",\n      \"Cage Challenge 2\",\n      \"Safe Driving\",\n      \"Malware Mutation\"\n    ],\n    \"legend\": {\n      \"p=0\": \"蓝色\",\n      \"p=0.25\": \"橙色\",\n      \"p=0.5\": \"绿色\",\n      \"p=0.75\": \"红色\",\n      \"p=1\": \"紫色\"\n    },\n    \"key_observations\": [\n      \"在大多数任务中，随着 λ 的减小（即探索奖励减少），性能表现呈现波动趋势。\",\n      \"某些任务如 Hopper 和 Walker2d 在 p=0.25 或 p=0.5 时表现出较高性能。\",\n      \"Reacher 和 HalfCheetah 的性能在 λ 较小时下降明显，说明这些任务对探索奖励敏感。\",\n      \"Selfish Mining 和 Malware Mutation 表现出不同的趋势，可能与任务复杂性有关。\"\n    ]\n  },\n  \"figure_9\": {\n    \"title\": \"Sensitivity results of hyper-parameter α\",\n    \"purpose\": \"研究超参数 α 对掩码网络保真度的影响，α 的取值为 {0.01, 0.001, 0.0001}，更高的保真度分数意味着更高的保真度。\",\n    \"data_type\": \"柱状图\",\n    \"x_axis\": \"Top-k 分类（top10, top20, top30, top40）\",\n    \"y_axis\": \"Fidelity（保真度）\",\n    \"tasks\": [\n      \"Hopper\",\n      \"Walker2d\",\n      \"Reacher\",\n      \"HalfCheetah\",\n      \"Selfish Mining\",\n      \"Cage Challenge 2\",\n      \"Autonomous Driving\",\n      \"Malware Mutation\"\n    ],\n    \"legend\": {\n      \"α=0.01\": \"蓝色\",\n      \"α=0.001\": \"橙色\",\n      \"α=0.0001\": \"绿色\"\n    },\n    \"key_observations\": [\n      \"在大多数任务中，α=0.0001（绿色）通常获得最高的保真度，表明更小的 α 值有助于提高掩码网络的保真度。\",\n      \"Hopper、Walker2d 和 HalfCheetah 在所有 α 设置下均表现出较高的保真度。\",\n      \"Reacher 和 Cage Challenge 2 的保真度随 α 减小而下降，尤其是在 top40 上表现明显。\",\n      \"Autonomous Driving 和 Malware Mutation 的保真度较低，且在不同 α 下差异不大，说明这些任务可能对 α 不敏感或存在其他限制因素。\"\n    ]\n  },\n  \"overall_insights\": [\n    \"λ 和 α 是影响模型训练效果的关键超参数，其选择需根据具体任务调整。\",\n    \"探索奖励（λ）对任务性能有显著影响，尤其在高动态环境（如 Reacher）中表现明显。\",\n    \"掩码网络的保真度受 α 影响较大，较小的 α 值通常能提升保真度，但并非所有任务都适用。\",\n    \"任务间的差异性表明需要针对特定场景进行超参数调优。\"\n  ],\n  \"image_label\": \"Figure 8 and Figure 9\"\n}>\n", "is_scanned_fallback": false, "token": 925}]}, {"page_num": 22, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nTable 5. Performance comparison between Self-Imitation Learning (SIL) and RICE on four MuJoCo tasks.\nMethod Hopper Walker2d Reacher HalfCheetah\nSIL 3646.46 (23.12) 3967.66 (1.53) -2.87 (0.09) 2069.80 (3.44)\nOurs 3663.91 (20.98) 3982.79 (3.15) -2.66 (0.03) 2138.89 (3.22)\nTable 6. Performance comparison when using different explanation methods across four MuJoCo tasks.\nTask Random Explanation Integrated Gradients AIRS Ours\nHopper 3648.98 (39.06) 3653.24 (14.23) 3654.49 (8.12) 3663.91 (20.98)\nWalker2d 3969.64 (6.38) 3972.15 (4.77) 3976.35 (2.40) 3982.79 (3.15)\nReacher -3.11 (0.42) -2.99 (0.31) -2.89 (0.19) -2.66 (0.03)\nHalfCheetah 2132.01 (0.76) 2132.81 (0.83) 2133.98 (2.52) 2138.89 (3.22)\nSensitivity of p and λ. We report the sensitivity of hyper-parameters p and λ in the three MuJoCo games with sparse\nrewards in Figure 11, Figure 12, and Figure 13. We have the following observations: First, generally, a mixed probability p\nwithin the range of 0.25 and 0.5 would be a good choice. Second, the refining benefits from the exploration bonus in the\nsparse MuJoCo games. Third, PPO fine-tuning cannot guarantee that the refined agent can achieve a good performance.\nEspecially in SparseWalker2d game, we observe that ppo fine-tuning cannot break through the training bottleneck of the\nDRL agent.\nC.5. Qualitative Analysis\nWe do a qualitative analysis of the autonomous driving case to understand how RICE impacts agent behavior and performance.\nWe visualize the agent’s behavior before and after refining the agent. Figure 14(a) shows a trajectory wherein the target\nagent (depicted by the green car) fails to reach its destination due to a collision with a pink car on the road. Given the\nundesired outcome, we use our method to identify the critical steps that contribute to the final (undesired) outcome. The\nimportant steps are highlighted in red color. Our method identifies the important step as the one when the green car switches\nacross two lanes into the lane of the pink car. The critical state is reasonable because this early step allows the green car to\nswitch lanes to avoid the collision. Based on the provided explanation, we apply our refining method to improve the target\nagent. The trajectory after refining is shown in Figure 14(b). It shows that after refining, the refined agent (the green car)\nsuccessfully identifies an alternative path to reach the destination while avoiding collision.\nD. Case Study: Malware Mutation\nD.1. Design Intuitions\nFirst, we use malware mutation as a case study to confirm our design intuitions before the proposed refining method. Recall\nthat the refining method contains three important ideas. First, we integrate the explanation result (identified critical step)\ninto the refining process. Second, we design a mixed initial state distribution to guide the refining of the target agent. Third,\nwe encourage the agent to perform exploration for diverse states during the refining phase. In the following, we create\nmultiple baselines by gradually adding these ideas to a naive baseline to show the contribution of each idea. We also provide\nevidence to support our stance against overfitting. Table 7 summarizes the results.\nTo start, the original agent is trained for 100 epochs until convergence. We test the target agent for 500 runs, resulting in\nan average evasion probability of 33.8%. To extract behavioral patterns, we perform a frequency analysis on the mutation\nactions taken by the agent across all 500 runs. As shown in the first row of Table 7, there is a clear preference for A\n4\n(i.e., “add bytes to section cave”). A complete list of the possible actions (16 in total) is shown in Table 2 (Appendix).\nContinue Learning w/o Explanation. The most common refining method is to lower the learning rate and continue\ntraining. We continue to train this target agent using the PPO algorithm for an additional 30 epochs and evaluate its\nperformance over 500 runs. This yields an average evasion probability of 38.8% (second row in Table 7). It is worth noting\nthat A (i.e., “add bytes to section cave”) remains the most frequently selected action.\n4\nLeverage Explanation Results for Refining. Subsequently, we assess the refining outcome by incorporating our\nexplanation result into the refining process. Specifically, we initiate the refining exclusively from the critical steps identified\nby the explanation method. For this setting, we do not perform exploration.\n22", "source": "pdfplumber_text", "token": 1236}, {"type": "table", "text": "\n[Detected Table]\n| Hopper | Walker2d | Reacher |\n| --- | --- | --- |\n| 3646.46(23.12) | 3967.66(1.53) | -2.87(0.09) |\n| 3663.91(20.98) | 3982.79(3.15) | -2.66(0.03) |\n\n", "source": "pdfplumber_table", "token": 112}, {"type": "table", "text": "\n[Detected Table]\n| RandomExplanation | IntegratedGradients | AIRS |\n| --- | --- | --- |\n| 3648.98(39.06) | 3653.24(14.23) | 3654.49(8.12) |\n| 3969.64(6.38) | 3972.15(4.77) | 3976.35(2.40) |\n| -3.11(0.42) | -2.99(0.31) | -2.89(0.19) |\n| 2132.01(0.76) | 2132.81(0.83) | 2133.98(2.52) |\n\n", "source": "pdfplumber_table", "token": 201}, {"type": "image_analysis", "text": "\n[图片分析 (page_22_1765335386.jpg)]:\n图片:<{\n  这是一张来自论文第22页的文本内容，主要包含两个表格（Table 5 和 Table 6）以及相关的文字描述。以下是对该页面内容的详细分析：\n\n1. **Table 5: Performance comparison between Self-Imitation Learning (SIL) and RICE on four MuJoCo tasks**\n   - 比较了 SIL 方法与本文提出的 RICE 方法在四个 MuJoCo 任务上的性能表现。\n   - 表格列出了每个方法在 Hopper、Walker2d、Reacher 和 HalfCheetah 四个任务上的平均奖励值及其标准差。\n   - 关键数据点：\n     - Hopper: SIL = 3646.46 (23.12), Ours = 3663.91 (20.98)\n     - Walker2d: SIL = 3967.66 (1.53), Ours = 3982.79 (3.15)\n     - Reacher: SIL = -2.87 (0.09), Ours = -2.66 (0.03)\n     - HalfCheetah: SIL = 2069.80 (3.44), Ours = 2138.89 (3.22)\n   - 所有任务中，“Ours”（即 RICE 方法）均优于 SIL。\n\n2. **Table 6: Performance comparison when using different explanation methods across four MuJoCo tasks**\n   - 对比了四种不同的解释方法（Random Explanation, Integrated Gradients, AIRS, Ours）在相同四个任务中的表现。\n   - 数据显示“Ours”方法在所有任务上都取得了最佳或接近最佳的结果。\n   - 关键数据点：\n     - Hopper: Random = 3648.98 (39.06), Integrated Gradients = 3653.24 (14.23), AIRS = 3654.49 (8.12), Ours = 3663.91 (20.98)\n     - Walker2d: Random = 3969.64 (6.38), Integrated Gradients = 3972.15 (4.77), AIRS = 3976.35 (2.40), Ours = 3982.79 (3.15)\n     - Reacher: Random = -3.11 (0.42), Integrated Gradients = -2.99 (0.31), AIRS = -2.89 (0.19), Ours = -2.66 (0.03)\n     - HalfCheetah: Random = 2132.01 (0.76), Integrated Gradients = 2132.81 (0.83), AIRS = 2133.98 (3.52), Ours = 2138.89 (3.22)\n\n3. **文字内容分析：**\n   - **Sensitivity of p and λ**: 提到在三个稀疏奖励的 MuJoCo 游戏中报告了超参数 p 和 λ 的敏感性，并引用了 Figure 11、Figure 12 和 Figure 13。\n   - **C.5. Qualitative Analysis**: 描述了一个自动驾驶案例的定性分析，使用 Figure 14(a) 和 Figure 14(b) 展示了目标智能体（绿色车）在碰撞前后的轨迹变化。通过识别关键步骤（红色高亮），改进后的智能体成功避开障碍物并找到替代路径。\n   - **D. Case Study: Malware Mutation**:\n     - D.1. Design Intuitions 部分介绍了设计直觉，包括将解释结果整合进精炼过程、混合初始状态分布引导精炼、鼓励探索等。\n     - 实验设置：原始代理训练 100 轮后得到平均逃避概率为 33.8%；继续训练 30 轮后提升至 38.8%。\n     - 利用解释结果进行精炼时，仅从关键步骤出发，不进行额外探索。\n\n4. **整体特征总结：**\n   - 本页重点在于展示 RICE 方法相对于其他方法的优势，特别是在强化学习中的性能提升和对代理行为的解释能力。\n   - 强调了 RICE 在多个任务上的稳定性和优越性，尤其是在处理稀疏奖励场景下的有效性。\n   - 使用定量数据（表格）和定性分析（案例研究）相结合的方式验证方法的有效性。\n\n5. **关键信息提取：**\n   - 图片标号：无直接图像，但提及 Figure 11, Figure 12, Figure 13, Figure 14(a), Figure 14(b)。\n   - 表格编号：Table 5, Table 6, Table 7（虽未完全显示，但被提及）。\n   - 方法名称：RICE, SIL, Random Explanation, Integrated Gradients, AIRS。\n   - 任务名称：Hopper, Walker2d, Reacher, HalfCheetah。\n   - 关键指标：平均奖励值、标准差、逃避概率。\n\n综上所述，此页内容以数据驱动为主，展示了 RICE 方法在多种任务上的优越性能，并结合案例研究说明其实际应用价值。\n}>\n", "is_scanned_fallback": false, "token": 1218}]}, {"page_num": 23, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nFigure 10. Agent Refining Performance in the SparseWalker2d Games. For the left figure, we fix the explanation method to our method\n(mask network) if needed while varying refining methods. For the right figure, we fix the refining method to our method while varying the\nexplanation methods.\nTable 7. Malware Mutation Case Study—We evaluate the evasion probability of the agent under different settings and count the\ncorresponding action frequencies.\nRefine Setting Test Setting Action Frequency Evasion\nOriginal agent w/o refinement From default initial S {A : 4,914, A : 5} 33.8%\n4 9\nContinue training From default initial S {A : 2,590, A : 55, A : 99, A : 95} 38.8%\n4 7 1 9\nFrom critical states {A : 2,546, A : 138, A : 32, A : 8} 50.8%\nRefine from critical states 12 5 4 9\nFrom default initial S {A : 4,728, A : 62} 36.2%\n12 5\nRefine from mixed initial state dist. From default initial S {A : 1,563, A : 1,135, A : 332, A : 12} 58.4%\n4 12 5 6\nRefine from mixed initial state dist. + exploration From default initial S {A : 2,448, A : 165, A : 138, A : 6} 68.2%\n5 7 12 4\nDuring the test phase, we explore two testing settings. First, we artificially reset the test environment to start from these\ncritical steps. We find that evasion probability surges to 50.8%. A (i.e., “modify timestamp”) becomes as the most\n12\nfrequently chosen action. This indicates the refined agent learns a policy when encountering the critical state again. However,\nfor more realistic testing, we need to set the test environment to the default initial state (i.e., the correct testing condition).\nUnder this setting, we find the evasion probability diminishes to 36.2%. This stark contrast in results shows evidence of\noverfitting. The refined agent excels at solving the problem when starting from critical steps but falters when encountering\nthe task from default initial states.\nImpact of Mixed Initial State Distribution. Given the above result, we further build a baseline by refining from the\nproposed mixed initial state distribution (i.e., blending the default initial state distribution with the critical states). For this\nsetting, we also do not perform exploration. Through 500 runs of testing, we observe a notable improvement, with the\naverage evasion probability reaching 58.4% (from the previous baseline’s 36.2%). Furthermore, the action frequency pattern\nhas also undergone a shift. It combines the preferred actions from the two previous refining strategies, highlighting the\nfrequent selection of both A and A .\n4 12\nImpact of Exploration. Finally, we explore the impact of exploration. This baseline represents the complete version\nof our proposed system by adding the exploration step and using the mixed initial distribution. We notice that the\naverage evasion probability across 500 runs has a major increase, reaching 68.2%. The most frequent action now is A\n5\n(i.e., “add section strings”). A and A are still among the top actions but their frequencies are lowered. This shows the\n4 12\nbenefits of exploring previously unseen states and diverse mutation paths. In return, the refined agent is able to get out of the\nlocal minima to identify more optimal policies.\nD.2. Discovery of Design Flaws\nAdditionally, our explanation results have led to the discovery of design flaws in the malware mutation application (Raff\net al., 2017). We will further explain how we use RICE to identify these problems.\nQuestions and Intuitions. When using RICE to explain the malware mutation process, we observe a scenario where the\nagent constantly chooses the same action “upx pack” in multiple consecutive steps. According to the agent, these actions\n23", "source": "pdfplumber_text", "token": 948}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  |\n\n", "source": "pdfplumber_table", "token": 45}, {"type": "table", "text": "\n[Detected Table]\n| TestSetting | ActionFrequency |\n| --- | --- |\n| FromdefaultinitialS | {A :4,914,A :5} 4 9 |\n| FromdefaultinitialS | {A :2,590,A :55,A :99,A :95} 4 7 1 9 |\n| Fromcriticalstates | {A :2,546,A :138,A :32,A :8} 12 5 4 9 |\n| FromdefaultinitialS | {A :4,728,A :62} 12 5 |\n| FromdefaultinitialS | {A :1,563,A :1,135,A :332,A :12} 4 12 5 6 |\n| FromdefaultinitialS | {A :2,448,A :165,A :138,A :6} 5 7 12 4 |\n\n", "source": "pdfplumber_table", "token": 217}, {"type": "image_analysis", "text": "\n[图片分析 (page_23_1765335422.jpg)]:\n图片:<{\n  \"type\": \"数据图表\",\n  \"description\": \"该图片包含两个折线图和一个表格，用于展示强化学习中代理在SpareWalker2d游戏中的性能表现以及恶意软件变异案例研究的结果。\",\n  \"charts\": [\n    {\n      \"title\": \"Agent Refining Performance in the SparseWalker2d Games (Left Figure)\",\n      \"x_axis\": \"Num. of Training Steps\",\n      \"y_axis\": \"Reward\",\n      \"lines\": [\n        {\n          \"label\": \"PPO finetune\",\n          \"color\": \"blue\",\n          \"trend\": \"低奖励，稳定增长\"\n        },\n        {\n          \"label\": \"StateMask-R\",\n          \"color\": \"green\",\n          \"trend\": \"略高于PPO finetune，波动较小\"\n        },\n        {\n          \"label\": \"JRL\",\n          \"color\": \"yellow\",\n          \"trend\": \"中等水平，有明显波动\"\n        },\n        {\n          \"label\": \"Ours\",\n          \"color\": \"red\",\n          \"trend\": \"初始阶段接近其他方法，后期显著提升，最终达到最高奖励\"\n        }\n      ],\n      \"key_features\": \"我们的方法（Ours）在训练后期表现出明显的性能优势，特别是在500,000步附近出现大幅奖励跃升。\"\n    },\n    {\n      \"title\": \"Agent Refining Performance in the SparseWalker2d Games (Right Figure)\",\n      \"x_axis\": \"Num. of Training Steps\",\n      \"y_axis\": \"Reward\",\n      \"lines\": [\n        {\n          \"label\": \"Ours\",\n          \"color\": \"red\",\n          \"trend\": \"整体奖励较高，且在后期有显著提升\"\n        },\n        {\n          \"label\": \"Random\",\n          \"color\": \"black\",\n          \"trend\": \"奖励极低，几乎不变\"\n        }\n      ],\n      \"key_features\": \"与随机方法相比，我们的方法（Ours）在所有训练步骤中都表现出更高的奖励，并在后期实现显著增长。\"\n    }\n  ],\n  \"table\": {\n    \"title\": \"Table 7. Malware Mutation Case Study\",\n    \"columns\": [\n      \"Refine Setting\",\n      \"Test Setting\",\n      \"Action Frequency\",\n      \"Evasion\"\n    ],\n    \"data\": [\n      {\n        \"Refine Setting\": \"Original agent w/o refinement\",\n        \"Test Setting\": \"From default initial S\",\n        \"Action Frequency\": \"{A₁: 4,914, A₉: 5}\",\n        \"Evasion\": \"33.8%\"\n      },\n      {\n        \"Refine Setting\": \"Continue training\",\n        \"Test Setting\": \"From default initial S\",\n        \"Action Frequency\": \"{A₁: 2,590, A₇: 55, A₁: 99, A₉: 95}\",\n        \"Evasion\": \"38.8%\"\n      },\n      {\n        \"Refine Setting\": \"Continue training\",\n        \"Test Setting\": \"From critical states\",\n        \"Action Frequency\": \"{A₁₂: 2,546, A₅: 138, A₄: 32, A₆: 8}\",\n        \"Evasion\": \"50.8%\"\n      },\n      {\n        \"Refine Setting\": \"Refine from critical states\",\n        \"Test Setting\": \"From default initial S\",\n        \"Action Frequency\": \"{A₁₂: 4,728, A₅: 62}\",\n        \"Evasion\": \"36.2%\"\n      },\n      {\n        \"Refine Setting\": \"Refine from mixed initial state dist.\",\n        \"Test Setting\": \"From default initial S\",\n        \"Action Frequency\": \"{A₁: 1,563, A₁₂: 1,135, A₅: 332, A₆: 12}\",\n        \"Evasion\": \"58.4%\"\n      },\n      {\n        \"Refine Setting\": \"Refine from mixed initial state dist. + exploration\",\n        \"Test Setting\": \"From default initial S\",\n        \"Action Frequency\": \"{A₅: 2,448, A₇: 165, A₁₂: 138, A₄: 6}\",\n        \"Evasion\": \"68.2%\"\n      }\n    ],\n    \"summary\": \"随着精炼策略的改进，特别是引入混合初始状态分布和探索后，逃避概率显著增加，从33.8%上升到68.2%，表明更优的策略被发现。\"\n  },\n  \"figure_label\": \"Figure 10\",\n  \"table_label\": \"Table 7\",\n  \"page_number\": 23,\n  \"contextual_insights\": [\n    \"左侧图表显示了不同精炼方法对奖励的影响，其中'Ours'方法在后期表现出最强性能。\",\n    \"右侧图表对比了我们提出的方法与随机方法，证明其优越性。\",\n    \"表格展示了不同精炼设置下的逃避概率和动作频率，揭示了探索和混合状态分布的重要性。\",\n    \"文本分析指出，过度拟合问题在特定测试条件下显现，而探索机制有助于避免局部最优。\"\n  ]\n}>\n", "is_scanned_fallback": false, "token": 1152}]}, {"page_num": 24, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.0\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.1\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.01\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.001\ndraweR\np=0.0 p=0.25 p=0.5 p=0.75 p=1.0\nFigure 11. Sensitivity results of hyper-parameter λ in SparseHopper game. We vary the hyper-parameter λ from {0, 0.1, 0.01, 0.001} and\nrecord the performance of the agent after refining. A smaller choice of λ means a smaller reward bonus for exploration.\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.0\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.1\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.01\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 100000.0 200000.0 300000.0 400000.0 500000.0\n= 0.001\ndraweR\np=0.0 p=0.25 p=0.5 p=0.75 p=1.0\nFigure 12. Sensitivity results of hyper-parameter λ in SparseWalker2d game. We vary the hyper-parameter λ from {0, 0.1, 0.01, 0.001}\nand record the performance of the agent after refining. A smaller choice of λ means a smaller reward bonus for exploration.\n24", "source": "pdfplumber_text", "token": 973}, {"type": "image_analysis", "text": "\n[图片分析 (page_24_1765335455.jpg)]:\n图片:<{\n  \"type\": \"数据图表\",\n  \"description\": \"该图片包含两个主要的折线图组（Figure 11 和 Figure 12），分别展示了在 SparseHopper 和 SparseWalker2d 游戏中，超参数 λ 对智能体性能的影响。每个图组由四个子图组成，对应不同的 λ 值：0.0、0.1、0.01、0.001。每条曲线代表不同 p 值（p=0.0, 0.25, 0.5, 0.75, 1.0）下的奖励随训练步数的变化情况。\",\n  \"figures\": [\n    {\n      \"id\": \"Figure 11\",\n      \"title\": \"Sensitivity results of hyper-parameter λ in SparseHopper game\",\n      \"description\": \"展示在 SparseHopper 游戏中，λ 参数对智能体奖励的影响。λ 值越小，探索奖励加成越小。每个子图显示了不同 λ 下，p 值变化时的奖励趋势。\",\n      \"subplots\": [\n        {\n          \"lambda\": 0.0,\n          \"x_label\": \"Training Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": {\n            \"p=0.0\": \"从低奖励开始，逐渐上升，最终趋于稳定在约 800 左右\",\n            \"p=0.25\": \"类似趋势，但略高于 p=0.0\",\n            \"p=0.5\": \"表现更优，最终接近 900\",\n            \"p=0.75\": \"波动较大，但整体表现良好\",\n            \"p=1.0\": \"初期增长缓慢，后期迅速提升至接近 900\"\n          }\n        },\n        {\n          \"lambda\": 0.1,\n          \"x_label\": \"Training Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": {\n            \"p=0.0\": \"初期稳定，中期有小幅上升，后期略有波动\",\n            \"p=0.25\": \"表现优于 p=0.0，中期显著提升\",\n            \"p=0.5\": \"持续增长，最终达到约 850\",\n            \"p=0.75\": \"波动较大，但总体趋势向上\",\n            \"p=1.0\": \"表现最好，最终接近 900\"\n          }\n        },\n        {\n          \"lambda\": 0.01,\n          \"x_label\": \"Training Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": {\n            \"p=0.0\": \"初始阶段较低，后期有所提升\",\n            \"p=0.25\": \"表现优于 p=0.0，但仍有波动\",\n            \"p=0.5\": \"明显优于其他 p 值，最终接近 800\",\n            \"p=0.75\": \"波动大，但整体上升\",\n            \"p=1.0\": \"表现最佳，最终接近 900\"\n          }\n        },\n        {\n          \"lambda\": 0.001,\n          \"x_label\": \"Training Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": {\n            \"p=0.0\": \"几乎无增长，保持在较低水平\",\n            \"p=0.25\": \"略有提升，但仍低于其他 p 值\",\n            \"p=0.5\": \"表现尚可，最终接近 600\",\n            \"p=0.75\": \"波动较大，但有明显提升\",\n            \"p=1.0\": \"表现最优，最终接近 900\"\n          }\n        }\n      ],\n      \"key_insight\": \"较小的 λ 值（如 0.001）导致探索奖励减少，影响早期学习；较大的 λ 值（如 0.1）有助于平衡探索与利用，提升整体性能。\"\n    },\n    {\n      \"id\": \"Figure 12\",\n      \"title\": \"Sensitivity results of hyper-parameter λ in SparseWalker2d game\",\n      \"description\": \"展示在 SparseWalker2d 游戏中，λ 参数对智能体奖励的影响。与 Figure 11 类似，但任务难度更高，表现差异更明显。\",\n      \"subplots\": [\n        {\n          \"lambda\": 0.0,\n          \"x_label\": \"Training Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": {\n            \"p=0.0\": \"几乎没有增长，保持在极低水平\",\n            \"p=0.25\": \"略有提升，但仍然很低\",\n            \"p=0.5\": \"出现短暂高峰，随后回落\",\n            \"p=0.75\": \"波动剧烈，有短暂高点\",\n            \"p=1.0\": \"后期突然爆发，达到约 1000\"\n          }\n        },\n        {\n          \"lambda\": 0.1,\n          \"x_label\": \"Training Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": {\n            \"p=0.0\": \"几乎无变化，保持在 0\",\n            \"p=0.25\": \"轻微增长，仍处于低位\",\n            \"p=0.5\": \"中期有提升，但不稳定\",\n            \"p=0.75\": \"波动大，但有上升趋势\",\n            \"p=1.0\": \"后期出现高峰，达到约 1000\"\n          }\n        },\n        {\n          \"lambda\": 0.01,\n          \"x_label\": \"Training Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": {\n            \"p=0.0\": \"基本无增长\",\n            \"p=0.25\": \"略有提升\",\n            \"p=0.5\": \"中期出现显著增长，后期稳定在较高水平\",\n            \"p=0.75\": \"波动大，但整体上升\",\n            \"p=1.0\": \"后期爆发，达到约 1000\"\n          }\n        },\n        {\n          \"lambda\": 0.001,\n          \"x_label\": \"Training Steps\",\n          \"y_label\": \"Reward\",\n          \"data\": {\n            \"p=0.0\": \"几乎无增长\",\n            \"p=0.25\": \"微弱提升\",\n            \"p=0.5\": \"中期有短暂高峰，随后下降\",\n            \"p=0.75\": \"波动剧烈，但有上升趋势\",\n            \"p=1.0\": \"后期突然爆发，达到约 1000\"\n          }\n        }\n      ],\n      \"key_insight\": \"SparseWalker2d 更难学习，λ 的选择对性能影响更大。p=1.0 在大多数情况下表现出色，尤其是在后期。\"\n    }\n  ],\n  \"overall_insight\": \"λ 控制探索奖励的大小，较小的 λ 导致探索不足，而较大的 λ 可能导致过度探索。p=1.0 在两种游戏中都表现出较好的鲁棒性和最终性能。建议在实际应用中尝试 λ ∈ {0.01, 0.1} 并结合 p=1.0 以获得最佳效果。\"\n}>\n", "is_scanned_fallback": false, "token": 1587}]}, {"page_num": 25, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 50000.0 100000.0 150000.0 200000.0 250000.0\n= 0.0\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 50000.0 100000.0 150000.0 200000.0 250000.0\n= 0.1\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 50000.0 100000.0 150000.0 200000.0 250000.0\n= 0.01\ndraweR\n1000.0\n800.0\n600.0\n400.0\n200.0\n0.0\n0.0 50000.0 100000.0 150000.0 200000.0 250000.0\n= 0.001\ndraweR\np=0.0 p=0.25 p=0.5 p=0.75 p=1.0\nFigure 13. Sensitivity results of hyper-parameter λ in SparseHalfCheetah game. We vary the hyper-parameter λ from {0, 0.1, 0.01, 0.001}\nand record the performance of the agent after refining. A smaller choice of λ means a smaller reward bonus for exploration.\nreceive a similar reward. However, RICE (our mask network) returns different “explanations” for these steps (i.e., they have\nhighly different importance scores). According to RICE, only the first action holds a high importance score, while the other\nconsecutive actions barely have an impact on the final reward (i.e., they appear redundant). This raises the question: why\ndoes the agent assign a similar reward to these consecutive steps in the first place?\nAnother interesting observation is from refining experiments. We find that PPO-based refining cannot yield substantial\nimprovements. While we have expected that these methods do not perform as well as ours (given our exploration step),\nthe difference is still bigger than we initially expected. This motivates us to further examine the reward function design to\nexplore whether it has inadvertently discouraged the DRL agent from finding good evasion paths.\nProblems of Reward Design. Driven by the intuitions above, we examined the reward design and identified two problems.\nFirstly, the reward mechanism is inherently non-Markovian which deviates from the expectation of a typical reinforcement\nlearning (RL) framework. In typical RL settings, rewards are contingent on the current state s and the next state s′. However,\nthe current design computes the reward based on the initial state s and the subsequent state s′. Consequently, this may\n0\nassign an identical reward for the same action (e.g., “upx pack”) in consecutive steps. This non-Markovian nature of the\nreward mechanism can mislead the DRL agent and hurt its performance.\nSecond, we find that the intermediate rewards exhibit unusually high sparsity, i.e., many intermediate rewards tend to have\na value close to zero, which poses a significant challenge for the PPO algorithm to learn a good policy based on such\nintermediate rewards. Agents refined with these methods can be easily trapped in local minima.\nFixing the Problematic Reward Design. Based on these insights, we fix the bugs in the reward design with two simple\nsteps: (1) We make the reward function Markovian, which depends only on the current state and the next state. (2) We\nperform scaling on the intermediate reward with a coefficient of 3. After that, we re-run an experiment to evaluate the\ncorrectness of our modifications. We train a DRL agent for 100 epochs with the same parameters under the new reward\ndesign and test its performance over 3 trials of 500 runs. The experiment shows that the evasion probability of the agent\nunder the new reward design jumps from 42.2% (using the old reward function, see Table 1) to 72.0%, which further\nconfirms our intuitions. This case study illustrates how developers can use RICE to debug their system and improve their\ndesigns.\n25", "source": "pdfplumber_text", "token": 1087}, {"type": "image_analysis", "text": "\n[图片分析 (page_25_1765335503.jpg)]:\n图片:<{这是一张包含四个子图的折线图，用于展示在SparseHalfCheetah游戏中超参数λ对智能体性能的影响。每个子图对应不同的λ值（0.0, 0.1, 0.01, 0.001），横轴表示训练步数（从0到250000），纵轴表示奖励（Reward）。每条曲线代表不同p值（p=0.0, p=0.25, p=0.5, p=0.75, p=1.0）下的奖励变化趋势，颜色编码如下：蓝色(p=0.0)、橙色(p=0.25)、绿色(p=0.5)、红色(p=0.75)、紫色(p=1.0)。\n\n整体特征：\n- 所有子图中，随着训练步数增加，奖励呈现上升趋势，但存在波动。\n- λ值越小（如0.001），奖励曲线更平滑，且早期收敛更快；λ值越大（如0.0），奖励增长较慢，且波动较大。\n- 不同p值对应的曲线表现出差异性，尤其在λ=0.0和λ=0.1时，p=0.0和p=1.0之间的差距较为明显。\n- 随着λ减小，各p值间的奖励差异逐渐缩小，说明较小的λ有助于缓解探索奖励的偏差。\n\n关键数据点：\n- 在λ=0.0时，p=0.0的奖励最终稳定在约800左右，而p=1.0则略低于600。\n- 在λ=0.001时，所有p值的奖励趋于接近，最终均达到约800以上。\n- 每个子图中的阴影区域表示奖励的标准差或置信区间，反映了实验结果的稳定性。\n\n图像标题与描述：\n- 图像编号为Figure 13，标题为“Sensitivity results of hyper-parameter λ in SparseHalfCheetah game”。\n- 实验目的是研究λ的变化如何影响智能体在精炼后的表现，其中λ控制探索奖励的大小，较小的λ意味着较小的奖励激励。\n\n结论：\n该图表揭示了λ对奖励结构的敏感性，表明较小的λ值能促进更稳定的性能提升，并减少不同p值之间的性能差异，从而支持更有效的探索策略。}>\n", "is_scanned_fallback": false, "token": 546}]}, {"page_num": 26, "content": [{"type": "text", "text": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation\nTarget agent Collision\nhigh\nlow\n(a) Original trajectory with importance score\n(b) Trajectory after refining with RICE Avoid collision\nFigure 14. (a): In the original trajectory, the target agent (the green car) eventually collides with the pink car, which is an undesired\noutcome. Each time step is marked with a different color: “yellow” indicates the least important step and “red” represents the most\nimportant step. (b): We highlight the critical states identified by our explanation method and the corresponding outcome after refining.\nUsing our explanation method, the target agent (the green car) successfully avoids collision.\n100\n50\n0\n0 1 2 3 4\nTraining Step (1e6)\ndraweR\nOurs\nRND\nFigure 15. Refining performance with our method and RND method in MountainCarContinuous-v0 game. The state coverage of the\npre-trained policy is limited to a small range around the initial point.\nE. Limitation\nWe use the continuous “Mountain Car” environment (mou) as a negative control task to illustrate a scenario where RICE\ndoes not work well. In this “extreme” case, Assumption 3.2 is completely broken since the state coverage of the pre-trained\nagent is limited to a small range around the initial point. In this experiment, we train a target agent using Proximal Policy\nOptimization (PPO) for 1 million steps. The results show that the policy performance remained poor, with the agent\nfrequently getting trapped at the starting point of the environment. In such cases where the original policy fails to learn\nan effective strategy, the role of explanations becomes highly limited. Since RICE relies on the identified critical states to\nenhance the policy, if the policy itself is extremely weak (i.e., not satisfying Assumption 3.2), then the explanations will not\nbe meaningful, which further huts the refinement. In the case of the Mountain Car experiment, RICE essentially reduces to\nbeing equivalent to Random Network Distillation (RND) due to the lack of meaningful explanation. We show the result\nwhen refining the pre-trained agent using our method and RND in Figure 15.\n26", "source": "pdfplumber_text", "token": 488}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n", "source": "pdfplumber_table", "token": 24}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n", "source": "pdfplumber_table", "token": 24}, {"type": "table", "text": "\n[Detected Table]\n|  |  |  |  |\n| --- | --- | --- | --- |\n|  |  |  |  |\n|  |  |  | Ours RND |\n|  |  |  |  |\n\n", "source": "pdfplumber_table", "token": 54}, {"type": "image_analysis", "text": "\n[图片分析 (page_26_1765335518.jpg)]:\n图片:<{\n  \"type\": \"复合图像\",\n  \"包含内容\": [\n    {\n      \"图号\": \"Figure 14\",\n      \"类型\": \"流程示意图\",\n      \"描述\": \"展示了在强化学习任务中，目标智能体（绿色车辆）的轨迹在原始状态和经过RICE方法优化后的对比。\",\n      \"子图分析\": [\n        {\n          \"子图\": \"(a) Original trajectory with importance score\",\n          \"内容\": \"显示原始轨迹，其中每个时间步用不同颜色标记重要性分数：黄色表示最不重要的步骤，红色表示最重要的步骤。目标智能体最终与粉色车辆发生碰撞，这是一个不良结果。\",\n          \"关键点\": [\n            \"目标智能体（绿色车）\",\n            \"碰撞事件（与粉色车）\",\n            \"重要性评分（颜色编码：黄→低，红→高）\"\n          ]\n        },\n        {\n          \"子图\": \"(b) Trajectory after refining with RICE\",\n          \"内容\": \"展示使用RICE方法优化后的轨迹，通过识别关键状态并进行修正，目标智能体成功避免了碰撞。\",\n          \"关键点\": [\n            \"避免碰撞的结果\",\n            \"关键状态被高亮显示（由解释方法识别）\",\n            \"路径调整以规避危险\"\n          ]\n        }\n      ],\n      \"执行流程\": \"原始轨迹 -> 发现关键状态（通过解释方法）-> 调整策略 -> 避免碰撞\",\n      \"依赖关系\": \"RICE方法依赖于对关键状态的准确识别来改进策略，若初始策略本身性能差，则解释效果受限。\"\n    },\n    {\n      \"图号\": \"Figure 15\",\n      \"类型\": \"折线图\",\n      \"描述\": \"比较RICE方法与随机网络蒸馏（RND）方法在MountainCarContinuous-v0环境中的训练表现。\",\n      \"坐标轴\": {\n        \"X轴\": \"Training Step (1e⁶)\",\n        \"Y轴\": \"Reward\"\n      },\n      \"数据曲线\": [\n        {\n          \"标签\": \"Ours (RICE)\",\n          \"趋势\": \"奖励值迅速上升至接近100后趋于稳定，波动较小。\",\n          \"特征\": \"快速收敛且保持较高奖励水平\"\n        },\n        {\n          \"标签\": \"RND\",\n          \"趋势\": \"奖励值同样上升但存在较大波动，整体略低于RICE。\",\n          \"特征\": \"收敛较慢，稳定性稍差\"\n        }\n      ],\n      \"关键数据点\": {\n        \"初始奖励\": \"约0\",\n        \"最大奖励\": \"接近100\",\n        \"训练步数范围\": \"0 到 4 × 10⁶\"\n      },\n      \"整体特征\": \"RICE方法在该任务中表现出更优的性能和稳定性，尤其是在早期训练阶段奖励增长更快。\"\n    }\n  ],\n  \"文本说明\": {\n    \"节标题\": \"E. Limitation\",\n    \"内容摘要\": \"使用连续型'Mountain Car'环境作为负控制实验，说明当预训练策略覆盖范围极小、无法有效探索时，RICE方法的效果会显著下降。此时由于策略本身薄弱，导致关键状态识别失效，解释能力丧失，RICE退化为类似于RND的方法。实验结果显示，在这种极端情况下，两种方法的性能相近，表明RICE依赖于已有策略的质量。\"\n  },\n  \"总结\": \"Figure 14 是一个可视化解释流程图，展示了RICE如何通过识别关键状态改善决策过程；Figure 15 是性能对比折线图，证明RICE在一般情况下优于RND，但在策略质量差时效果受限。两者共同支持论文关于RICE有效性的论述及其局限性分析。\"\n}>\n", "is_scanned_fallback": false, "token": 819}]}]