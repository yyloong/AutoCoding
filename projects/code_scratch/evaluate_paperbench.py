#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼šåœ¨ MS-Agent ä¸­è¯„æµ‹ PaperBench

ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluate_paperbench.py --split debug --type code-dev
    python evaluate_paperbench.py --split full --type complete
"""

import os
import sys
import json
import argparse
import asyncio
import subprocess
from pathlib import Path
from datetime import datetime

# è®¾ç½® Windows æ”¯æŒ UTF-8 è¾“å‡º
if os.name == 'nt':
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


class PaperBenchEvaluationRunner:
    """PaperBench è¯„æµ‹æ‰§è¡Œå™¨"""
    
    def __init__(self, paperbench_dir: str, split: str = "debug", eval_type: str = "code-dev"):
        self.paperbench_dir = Path(paperbench_dir)
        self.split = split
        self.eval_type = eval_type
        self.results_dir = Path("paperbench_results") / datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"âœ“ è¯„æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"  - PaperBench æ•°æ®ç›®å½•: {self.paperbench_dir}")
        print(f"  - è¯„æµ‹ç±»å‹: {eval_type}")
        print(f"  - è®ºæ–‡åˆ†å‰²: {split}")
        print(f"  - ç»“æœä¿å­˜ç›®å½•: {self.results_dir}")
    
    def get_papers(self) -> list:
        """è·å–è¦è¯„æµ‹çš„è®ºæ–‡åˆ—è¡¨"""
        papers_dir = self.paperbench_dir / "papers"
        
        if not papers_dir.exists():
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è®ºæ–‡ç›®å½• {papers_dir}")
            print(f"   è¯·è®¾ç½® PAPERBENCH_DATA_DIR ç¯å¢ƒå˜é‡æˆ–æ£€æŸ¥è·¯å¾„")
            return []
        
        all_papers = sorted([d.name for d in papers_dir.iterdir() if d.is_dir()])
        
        if self.split == "debug":
            papers = all_papers[:3]  # è°ƒè¯•æ—¶åªç”¨å‰ 3 ç¯‡
        elif self.split == "mini":
            papers = all_papers[:10]  # å¿«é€Ÿè¯„æµ‹ 10 ç¯‡
        else:
            papers = all_papers  # å…¨éƒ¨ 20 ç¯‡
        
        print(f"\nğŸ“„ è·å–è®ºæ–‡åˆ—è¡¨:")
        print(f"  - æ€»è®ºæ–‡æ•°: {len(all_papers)}")
        print(f"  - æœ¬æ¬¡è¯„æµ‹: {len(papers)} ç¯‡")
        print(f"  - è®ºæ–‡åˆ—è¡¨: {papers[:5]}..." if len(papers) > 5 else f"  - è®ºæ–‡åˆ—è¡¨: {papers}")
        
        return papers
    
    def validate_environment(self) -> bool:
        """æ£€éªŒç¯å¢ƒæ˜¯å¦é…ç½®æ­£ç¡®"""
        print("\nğŸ” æ£€éªŒç¯å¢ƒé…ç½®...")
        
        checks = []
        
        # 1. æ£€æŸ¥ PaperBench æ•°æ®ç›®å½•
        if not self.paperbench_dir.exists():
            checks.append(("PaperBench æ•°æ®ç›®å½•", False, f"æ‰¾ä¸åˆ° {self.paperbench_dir}"))
        else:
            checks.append(("PaperBench æ•°æ®ç›®å½•", True, str(self.paperbench_dir)))
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰ papers ç›®å½•
        papers_dir = self.paperbench_dir / "papers"
        if not papers_dir.exists():
            checks.append(("è®ºæ–‡ç›®å½•", False, "æ‰¾ä¸åˆ° papers å­ç›®å½•"))
        else:
            paper_count = len(list(papers_dir.iterdir()))
            checks.append(("è®ºæ–‡ç›®å½•", True, f"åŒ…å« {paper_count} ç¯‡è®ºæ–‡"))
        
        # 3. æ£€æŸ¥ API Key
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            checks.append(("OpenAI API Key", True, "âœ“ å·²è®¾ç½®"))
        else:
            checks.append(("OpenAI API Key", False, "æœªè®¾ç½®"))
        
        # 4. æ£€æŸ¥ MS-Agent é¡¹ç›®
        if Path("projects/code_scratch").exists():
            checks.append(("MS-Agent Code Scratch", True, "âœ“ é¡¹ç›®å­˜åœ¨"))
        else:
            checks.append(("MS-Agent Code Scratch", False, "æ‰¾ä¸åˆ°é¡¹ç›®"))
        
        # æ‰“å°æ£€éªŒç»“æœ
        for item, status, detail in checks:
            icon = "âœ“" if status else "âœ—"
            color_prefix = "\033[92m" if status else "\033[91m"
            color_suffix = "\033[0m"
            print(f"  {color_prefix}{icon} {item}{color_suffix}: {detail}")
        
        all_ok = all(status for _, status, _ in checks)
        return all_ok
    
    async def run_evaluation(self):
        """è¿è¡Œè¯„æµ‹"""
        print(f"\nğŸš€ å¼€å§‹è¯„æµ‹...")
        
        papers = self.get_papers()
        if not papers:
            print("âŒ æ²¡æœ‰è®ºæ–‡å¯ä»¥è¯„æµ‹")
            return False
        
        results = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "split": self.split,
                "eval_type": self.eval_type,
                "total_papers": len(papers),
            },
            "papers": []
        }
        
        for i, paper_id in enumerate(papers, 1):
            print(f"\n[{i}/{len(papers)}] è¯„æµ‹è®ºæ–‡: {paper_id}")
            
            result = await self._evaluate_paper(paper_id)
            results["papers"].append(result)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            self._save_results(results, f"results_temp.json")
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        results["summary"] = self._calculate_summary(results["papers"])
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_results(results, f"results_final.json")
        
        return True
    
    async def _evaluate_paper(self, paper_id: str) -> dict:
        """è¯„æµ‹å•ç¯‡è®ºæ–‡ - è°ƒç”¨ MS-Agent ç”Ÿæˆä»£ç å¹¶éªŒè¯"""
        paper_dir = self.paperbench_dir / "papers" / paper_id
        
        result = {
            "paper_id": paper_id,
            "status": "pending",
            "score": 0,
            "details": {}
        }
        
        try:
            # æ£€æŸ¥è®ºæ–‡æ–‡ä»¶
            paper_file = paper_dir / "paper.md"
            if not paper_file.exists():
                paper_file = paper_dir / "paper.pdf"
            
            if not paper_file.exists():
                result["status"] = "failed"
                result["error"] = "æ‰¾ä¸åˆ°è®ºæ–‡æ–‡ä»¶"
                return result
            
            # è¯»å–è®ºæ–‡å†…å®¹ä¸è¯„ä¼°æ ‡å‡†
            with open(paper_file, "r", encoding="utf-8") as f:
                paper_content = f.read()
            
            rubric_file = paper_dir / "rubric.json"
            rubric = {}
            if rubric_file.exists():
                with open(rubric_file) as f:
                    rubric = json.load(f)
            result["details"]["rubric"] = rubric
            
            # æ„å»º promptï¼šæ ¹æ®è®ºæ–‡å†…å®¹è®© MS-Agent ç”Ÿæˆé¡¹ç›®ä»£ç 
            prompt = f"""
æ ¹æ®ä»¥ä¸‹è®ºæ–‡æè¿°ï¼Œå®ç°è¯¥è®ºæ–‡ä¸­æå‡ºçš„ç³»ç»Ÿ/æ–¹æ³•ã€‚ç”Ÿæˆå®Œæ•´çš„é¡¹ç›®ä»£ç ï¼ŒåŒ…æ‹¬å‰åç«¯å®ç°ã€‚

è®ºæ–‡æ‘˜è¦:
{paper_content[:2000]}

è¦æ±‚:
1. ç”Ÿæˆå‰ç«¯ä»£ç ï¼ˆReact + Viteï¼‰å’Œåç«¯ä»£ç ï¼ˆNode.jsï¼‰
2. ç¡®ä¿ä»£ç èƒ½å¤Ÿç¼–è¯‘å’Œè¿è¡Œ
3. å®ç°è®ºæ–‡çš„æ ¸å¿ƒåŠŸèƒ½æ¼”ç¤º
"""
            
            # è°ƒç”¨ MS-Agent CLI
            print(f"    â†’ è°ƒç”¨ MS-Agent ç”Ÿæˆä»£ç ...")
            cmd = [
                "python", "ms_agent/cli/cli.py", "run",
                "--config", "projects/code_scratch",
                "--query", prompt,
                "--trust_remote_code", "true"
            ]
            
            # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æ”¯æŒ UTF-8 è¾“å‡ºï¼ˆè§£å†³ Windows GBK ç¼–ç é—®é¢˜ï¼‰
            env = os.environ.copy()
            env["PYTHONIOENCODING"] = "utf-8"
            
            try:
                # æ”¹ç”¨ subprocessï¼ˆè€Œé asyncio.create_subprocess_execï¼‰ä»¥é¿å…å¤æ‚çš„æµè¯»å–
                # ä½¿ç”¨é˜»å¡æ¨¡å¼å¹¶åœ¨é»˜è®¤çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œè¶…æ—¶ç”± asyncio.wait_for ç®¡ç†
                loop = asyncio.get_event_loop()
                result_proc = await asyncio.wait_for(
                    loop.run_in_executor(
                        None,
                        lambda: subprocess.run(
                            cmd,
                            capture_output=True,
                            text=True,
                            encoding="utf-8",
                            errors="replace",
                            env=env,
                            timeout=300  # 5 åˆ†é’Ÿè¶…æ—¶
                        )
                    ),
                    timeout=310  # asyncio è¶…æ—¶ç¨é•¿ï¼Œä»¥ä¾¿ subprocess çš„ timeout å…ˆè§¦å‘
                )
                stdout_text = result_proc.stdout or ""
                stderr_text = result_proc.stderr or ""
                returncode = result_proc.returncode
                
            except asyncio.TimeoutError:
                result["status"] = "timeout"
                result["error"] = "MS-Agent æ‰§è¡Œè¶…æ—¶ï¼ˆè¶…è¿‡ 300 ç§’ï¼‰"
                return result
            except subprocess.TimeoutExpired as e:
                result["status"] = "timeout"
                result["error"] = f"MS-Agent æ‰§è¡Œè¶…æ—¶"
                return result
            except Exception as e:
                result["status"] = "agent_execution_error"
                result["error"] = str(e)
                return result
            
            if returncode != 0:
                result["status"] = "agent_failed"
                result["error"] = f"MS-Agent æ‰§è¡Œå¤±è´¥: {stderr_text[-500:]}"
                result["details"]["stderr"] = stderr_text[-1000:]
                return result
            
            # æ£€æŸ¥ç”Ÿæˆçš„ä»£ç 
            output_dir = Path("output")
            if not output_dir.exists():
                result["status"] = "code_generation_failed"
                result["error"] = "MS-Agent æœªç”Ÿæˆ output ç›®å½•"
                return result
            
            # ç»Ÿè®¡ç”Ÿæˆçš„æ–‡ä»¶
            generated_files = list(output_dir.rglob("*"))
            result["details"]["generated_files_count"] = len([f for f in generated_files if f.is_file()])
            result["details"]["has_frontend"] = (output_dir / "frontend").exists()
            result["details"]["has_backend"] = (output_dir / "backend").exists()
            
            # å°è¯•ç¼–è¯‘å‰ç«¯
            compilation_passed = False
            try:
                print(f"    â†’ éªŒè¯å‰ç«¯ç¼–è¯‘...")
                proc = await asyncio.create_subprocess_exec(
                    "npm", "run", "build",
                    cwd="output/frontend",
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    limit=10*1024*1024
                )
                
                stdout, stderr = await asyncio.wait_for(
                    proc.communicate(),
                    timeout=120
                )
                
                if proc.returncode == 0:
                    compilation_passed = True
                    result["details"]["frontend_build"] = "passed"
                else:
                    result["details"]["frontend_build"] = "failed"
                    result["details"]["frontend_build_error"] = stderr.decode("utf-8", errors="replace")[-500:]
            except asyncio.TimeoutError:
                result["details"]["frontend_build"] = "timeout"
            except Exception as e:
                result["details"]["frontend_build"] = f"error: {str(e)}"
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            score = 0.0
            
            # ä»£ç ç”ŸæˆæˆåŠŸ (0.3)
            if result["details"]["generated_files_count"] > 5:
                score += 0.3
            
            # ç”Ÿæˆäº†å‰åç«¯ (0.3)
            if result["details"]["has_frontend"] and result["details"]["has_backend"]:
                score += 0.3
            
            # å‰ç«¯ç¼–è¯‘é€šè¿‡ (0.4)
            if compilation_passed:
                score += 0.4
            
            result["status"] = "completed"
            result["score"] = min(score, 1.0)
            result["code_generated"] = result["details"]["generated_files_count"] > 0
            result["compilation_passed"] = compilation_passed
            
        except asyncio.TimeoutError:
            result["status"] = "timeout"
            result["error"] = "è¯„æµ‹è¶…æ—¶ï¼ˆè¶…è¿‡ 5 åˆ†é’Ÿï¼‰"
        except Exception as e:
            result["status"] = "error"
            result["error"] = str(e)
            import traceback
            result["details"]["traceback"] = traceback.format_exc()
        
        return result
    
    def _calculate_summary(self, results: list) -> dict:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        if not results:
            return {}
        
        completed = [r for r in results if r["status"] == "completed"]
        failed = [r for r in results if r["status"] in ["failed", "error"]]
        
        avg_score = sum(r.get("score", 0) for r in completed) / len(completed) if completed else 0
        
        return {
            "total": len(results),
            "completed": len(completed),
            "failed": len(failed),
            "success_rate": len(completed) / len(results) if results else 0,
            "average_score": avg_score,
        }
    
    def _save_results(self, results: dict, filename: str):
        """ä¿å­˜ç»“æœ"""
        output_file = self.results_dir / filename
        with open(output_file, "w") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"  â†’ ç»“æœå·²ä¿å­˜: {output_file}")
    
    def print_summary(self, results: dict):
        """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
        summary = results.get("summary", {})
        
        print(f"\n{'='*60}")
        print("ğŸ“Š è¯„æµ‹æ€»ç»“")
        print(f"{'='*60}")
        print(f"æ€»è®ºæ–‡æ•°:        {summary.get('total', 0)}")
        print(f"å®Œæˆ:            {summary.get('completed', 0)}")
        print(f"å¤±è´¥:            {summary.get('failed', 0)}")
        print(f"æˆåŠŸç‡:          {summary.get('success_rate', 0):.1%}")
        print(f"å¹³å‡åˆ†æ•°:        {summary.get('average_score', 0):.2f}")
        print(f"{'='*60}")
        
        # å¯¹æ ‡åŸºçº¿
        baseline = {
            "Claude 3.5 Sonnet (Code-Dev)": 0.21,
            "Claude 3.5 Sonnet (å®Œæ•´)": 0.161,
            "GPT-4o": 0.041,
        }
        
        avg_score = summary.get('average_score', 0)
        print(f"\nğŸ“ˆ åŸºçº¿å¯¹æ ‡:")
        for model, baseline_score in baseline.items():
            diff = avg_score - baseline_score
            status = "âœ“ è¶…è¶Š" if diff > 0 else "âœ— ä½äº"
            print(f"  {status} {model}: {baseline_score:.1%} (å·®å¼‚: {diff:+.1%})")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="PaperBench è¯„æµ‹å·¥å…· - è¯„æµ‹ MS-Agent è®ºæ–‡å¤ç°èƒ½åŠ›"
    )
    
    parser.add_argument(
        "--split",
        choices=["debug", "mini", "full"],
        default="debug",
        help="è®ºæ–‡åˆ†å‰²: debug(3ç¯‡), mini(10ç¯‡), full(20ç¯‡)"
    )
    
    parser.add_argument(
        "--type",
        dest="eval_type",
        choices=["code-dev", "complete"],
        default="code-dev",
        help="è¯„æµ‹ç±»å‹: code-dev(ä»…ä»£ç ), complete(åŒ…æ‹¬æ‰§è¡Œ)"
    )
    
    parser.add_argument(
        "--paperbench-dir",
        default=os.getenv("PAPERBENCH_DATA_DIR"),
        help="PaperBench æ•°æ®ç›®å½•ï¼ˆé»˜è®¤ä» PAPERBENCH_DATA_DIR ç¯å¢ƒå˜é‡è¯»å–ï¼‰"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å¿…è¦å‚æ•°
    if not args.paperbench_dir:
        print("âŒ é”™è¯¯: è¯·è®¾ç½® PAPERBENCH_DATA_DIR ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨ --paperbench-dir å‚æ•°")
        sys.exit(1)
    
    # åˆ›å»ºè¯„æµ‹å™¨
    runner = PaperBenchEvaluationRunner(
        args.paperbench_dir,
        split=args.split,
        eval_type=args.eval_type
    )
    
    # éªŒè¯ç¯å¢ƒ
    if not runner.validate_environment():
        print("\nâŒ ç¯å¢ƒæ£€éªŒå¤±è´¥ï¼Œè¯·æŒ‰ä¸Šè¿°æç¤ºä¿®å¤")
        sys.exit(1)
    
    # è¿è¡Œè¯„æµ‹
    print(f"\nâœ“ ç¯å¢ƒæ£€éªŒé€šè¿‡ï¼Œå‡†å¤‡å¼€å§‹è¯„æµ‹...")
    success = await runner.run_evaluation()
    
    if success:
        # åŠ è½½å¹¶æ‰“å°æœ€ç»ˆç»“æœ
        results_file = runner.results_dir / "results_final.json"
        with open(results_file) as f:
            results = json.load(f)
        
        runner.print_summary(results)
        
        print(f"\nâœ“ è¯„æµ‹å®Œæˆï¼")
        print(f"  è¯¦ç»†ç»“æœä¿å­˜åœ¨: {runner.results_dir}/")
        print(f"  æŸ¥çœ‹ç»“æœ: cat {results_file}")
    else:
        print(f"\nâŒ è¯„æµ‹å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
